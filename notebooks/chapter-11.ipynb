{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup and Imports\n\nRun this cell first to install required packages and import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision transformers numpy matplotlib seaborn pandas pillow requests\n",
        "!pip install clip-by-openai sentence-transformers datasets\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 11: Practical Implementation Guide\n\n**Interactive Jupyter Notebook Version**\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 11: Practical Implementation Guide\n\n---\n\n**Previous**: [Chapter 10: Seminal Models and Architectures](chapter-10.md) | **Next**: [Chapter 12: Advanced Topics and Future Directions](chapter-12.md) | **Home**: [Table of Contents](index.md)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 11: Practical Implementation Guide (FULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Collect and preprocess multimodal datasets\n- Build production-ready training pipelines\n- Handle edge cases and failures\n- Deploy models efficiently\n- Monitor and maintain systems\n- Implement best practices for MLOps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.1 Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Multimodal Datasets\n\n**Data sources:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWeb-scale data:\n  LAION (5.8B images + captions)\n  Conceptual Captions (3.3M pairs)\n  Wikipedia + images\n  News articles + images\n  Social media posts + images/video\n\nCurated datasets:\n  COCO (image captioning)\n  Flickr30K (image-text)\n  Visual Genome (regions + descriptions)\n  ActivityNet (video + captions)\n\nSynthetic/Generated:\n  Text descriptions from writers\n  AI-generated descriptions\n  Rule-based generation\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data quality considerations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nIssue 1: Image-text mismatch\n  Problem: Caption doesn't describe image\n  Solution: Filter with CLIP-based similarity\n\nIssue 2: Duplicate or near-duplicate pairs\n  Problem: Same image with different captions\n  Solution: Hash-based deduplication\n\nIssue 3: Offensive or sensitive content\n  Problem: Dataset contains harmful content\n  Solution: Content moderation filters\n\nIssue 4: Biases in distribution\n  Problem: Skewed toward certain domains\n  Solution: Stratified sampling, data augmentation\n\nIssue 5: Missing or corrupted files\n  Problem: Broken image links, corrupted videos\n  Solution: Validation pipeline\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing Pipeline\n\n**Step 1: Image preprocessing**\n\n```python\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch\n\nclass ImagePreprocessor:\n    def __init__(self, input_size=224):\n        self.input_size = input_size\n\n        # Training transforms (with augmentation)\n        self.train_transforms = transforms.Compose([\n            transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.RandomRotation(15),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n        # Validation transforms (no augmentation)\n        self.val_transforms = transforms.Compose([\n            transforms.Resize((input_size, input_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    def preprocess_image(self, image_path, is_train=True):\n        \"\"\"Load and preprocess image\"\"\"\n        try:\n            # Load image\n            image = Image.open(image_path).convert('RGB')\n\n            # Apply transforms\n            if is_train:\n                image = self.train_transforms(image)\n            else:\n                image = self.val_transforms(image)\n\n            return image\n\n        except Exception as e:\n            print(f\"Error processing {image_path}: {e}\")\n            return None\n\n    def preprocess_batch(self, image_paths, is_train=True):\n        \"\"\"Preprocess batch of images\"\"\"\n        images = []\n        valid_paths = []\n\n        for path in image_paths:\n            img = self.preprocess_image(path, is_train)\n            if img is not None:\n                images.append(img)\n                valid_paths.append(path)\n\n        if images:\n            images = torch.stack(images)\n            return images, valid_paths\n        else:\n            return None, []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example usage\npreprocessor = ImagePreprocessor(input_size=224)\nimage_batch, valid_paths = preprocessor.preprocess_batch(\n    image_paths=['img1.jpg', 'img2.jpg', 'img3.jpg'],\n    is_train=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n\n**Step 2: Text preprocessing**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\nfrom transformers import AutoTokenizer\n\nclass TextPreprocessor:\n    def __init__(self, model_name='bert-base-uncased', max_length=77):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.max_length = max_length\n\n    def clean_text(self, text):\n        \"\"\"Clean text\"\"\"\n        # Remove extra whitespace\n        text = ' '.join(text.split())\n\n        # Remove special characters (keep basic punctuation)\n        import re\n        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\']', '', text)\n\n        # Lowercase\n        text = text.lower()\n\n        return text\n\n    def tokenize(self, text):\n        \"\"\"Tokenize single text\"\"\"\n        cleaned = self.clean_text(text)\n\n        tokens = self.tokenizer(\n            cleaned,\n            return_tensors='pt',\n            padding='max_length',\n            max_length=self.max_length,\n            truncation=True,\n            add_special_tokens=True\n        )\n\n        return tokens\n\n    def tokenize_batch(self, texts):\n        \"\"\"Tokenize batch of texts\"\"\"\n        cleaned = [self.clean_text(text) for text in texts]\n\n        tokens = self.tokenizer(\n            cleaned,\n            return_tensors='pt',\n            padding='max_length',\n            max_length=self.max_length,\n            truncation=True,\n            batch_first=True\n        )\n\n        return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example\ntext_proc = TextPreprocessor()\ntokens = text_proc.tokenize_batch([\n    \"A red cat on a wooden chair\",\n    \"Two dogs playing in the park\"\n])\nprint(tokens['input_ids'].shape)  # (2, 77)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n\n**Step 3: Video preprocessing**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\nimport cv2\nimport numpy as np\n\nclass VideoPreprocessor:\n    def __init__(self, fps=1, frame_count=8, frame_size=224):\n        self.fps = fps\n        self.frame_count = frame_count\n        self.frame_size = frame_size\n\n    def extract_frames(self, video_path):\n        \"\"\"Extract frames from video\"\"\"\n        try:\n            cap = cv2.VideoCapture(video_path)\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n            # Sample frames evenly\n            frame_indices = np.linspace(\n                0, total_frames - 1,\n                self.frame_count,\n                dtype=int\n            )\n\n            frames = []\n            for idx in frame_indices:\n                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n                ret, frame = cap.read()\n\n                if ret:\n                    # Resize\n                    frame = cv2.resize(\n                        frame,\n                        (self.frame_size, self.frame_size)\n                    )\n                    # Convert BGR to RGB\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    frames.append(frame)\n\n            cap.release()\n\n            if frames:\n                return np.stack(frames)  # (frame_count, h, w, 3)\n            else:\n                return None\n\n        except Exception as e:\n            print(f\"Error processing video {video_path}: {e}\")\n            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example\nvideo_proc = VideoPreprocessor(frame_count=8)\nframes = video_proc.extract_frames('video.mp4')\nprint(frames.shape)  # (8, 224, 224, 3)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete Preprocessing Pipeline\n\n```python\nclass MultimodalDataPreprocessor:\n    \"\"\"Complete preprocessing for image-text-video data\"\"\"\n\n    def __init__(self, image_size=224, max_text_length=77,\n                 video_frames=8):\n        self.image_preprocessor = ImagePreprocessor(image_size)\n        self.text_preprocessor = TextPreprocessor(max_text_length)\n        self.video_preprocessor = VideoPreprocessor(frame_count=video_frames)\n\n    def process_sample(self, sample):\n        \"\"\"Process single multimodal sample\"\"\"\n        processed = {}\n\n        # Image\n        if 'image_path' in sample:\n            img = self.image_preprocessor.preprocess_image(\n                sample['image_path'],\n                is_train=sample.get('is_train', True)\n            )\n            if img is not None:\n                processed['image'] = img\n\n        # Text\n        if 'text' in sample:\n            tokens = self.text_preprocessor.tokenize(sample['text'])\n            processed['text_ids'] = tokens['input_ids'].squeeze()\n            processed['text_mask'] = tokens['attention_mask'].squeeze()\n\n        # Video\n        if 'video_path' in sample:\n            frames = self.video_preprocessor.extract_frames(\n                sample['video_path']\n            )\n            if frames is not None:\n                processed['video'] = torch.from_numpy(frames).float()\n\n        # Label (if available)\n        if 'label' in sample:\n            processed['label'] = torch.tensor(sample['label'])\n\n        return processed\n\n    def validate_sample(self, sample):\n        \"\"\"Check if sample is valid\"\"\"\n        required_keys = sample.get('required_modalities', ['image', 'text'])\n\n        for key in required_keys:\n            if key not in sample:\n                return False\n\n        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\npreprocessor = MultimodalDataPreprocessor()\n\nsample = {\n    'image_path': 'cat.jpg',\n    'text': 'A cute cat on a sofa',\n    'label': 0,\n    'is_train': True,\n    'required_modalities': ['image', 'text']\n}\n\nif preprocessor.validate_sample(sample):\n    processed = preprocessor.process_sample(sample)\n    print(f\"Image shape: {processed['image'].shape}\")\n    print(f\"Text IDs shape: {processed['text_ids'].shape}\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.2 Building Training Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading with Multiprocessing\n\n```python\nfrom torch.utils.data import Dataset, DataLoader\nimport multiprocessing as mp\n\nclass MultimodalDataset(Dataset):\n    \"\"\"Efficient multimodal dataset\"\"\"\n\n    def __init__(self, samples, preprocessor, cache_size=1000):\n        self.samples = samples\n        self.preprocessor = preprocessor\n        self.cache = {}\n        self.cache_size = cache_size\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # Check cache first\n        if idx in self.cache:\n            return self.cache[idx]\n\n        # Load and preprocess\n        sample = self.samples[idx]\n        processed = self.preprocessor.process_sample(sample)\n\n        # Cache if space available\n        if len(self.cache) < self.cache_size:\n            self.cache[idx] = processed\n\n        return processed\n\ndef create_dataloaders(train_samples, val_samples, batch_size=256,\n                      num_workers=8):\n    \"\"\"Create train and validation dataloaders\"\"\"\n\n    preprocessor = MultimodalDataPreprocessor()\n\n    train_dataset = MultimodalDataset(train_samples, preprocessor)\n    val_dataset = MultimodalDataset(val_samples, preprocessor)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=False\n    )\n\n    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\ntrain_loader, val_loader = create_dataloaders(\n    train_samples=train_data,\n    val_samples=val_data,\n    batch_size=256,\n    num_workers=8\n)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop with Best Practices\n\n```python\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nimport wandb\n\nclass MultimodalTrainer:\n    \"\"\"Production-ready trainer\"\"\"\n\n    def __init__(self, model, device='cuda', use_wandb=True):\n        self.model = model\n        self.device = device\n        self.use_wandb = use_wandb\n\n        if use_wandb:\n            wandb.init(project='multimodal-learning')\n\n    def train_epoch(self, train_loader, optimizer, scheduler,\n                   criterion, scaler=None):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        num_batches = 0\n\n        pbar = tqdm(train_loader, desc='Training')\n\n        for batch_idx, batch in enumerate(pbar):\n            # Move to device\n            images = batch['image'].to(self.device)\n            text_ids = batch['text_ids'].to(self.device)\n            text_mask = batch['text_mask'].to(self.device)\n\n            # Forward pass with mixed precision\n            if scaler is not None:\n                with torch.cuda.amp.autocast():\n                    logits = self.model(images, text_ids, text_mask)\n                    loss = criterion(logits, batch['label'].to(self.device))\n\n                # Backward pass\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n            else:\n                logits = self.model(images, text_ids, text_mask)\n                loss = criterion(logits, batch['label'].to(self.device))\n                loss.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(\n                self.model.parameters(),\n                max_norm=1.0\n            )\n\n            # Optimization step\n            if scaler is not None:\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.step()\n\n            optimizer.zero_grad()\n            scheduler.step()\n\n            total_loss += loss.item()\n            num_batches += 1\n\n            # Update progress bar\n            pbar.set_postfix({'loss': total_loss / num_batches})\n\n            # Log to wandb\n            if self.use_wandb and batch_idx % 100 == 0:\n                wandb.log({\n                    'train_loss': loss.item(),\n                    'learning_rate': scheduler.get_last_lr()[0]\n                })\n\n        return total_loss / num_batches\n\n    @torch.no_grad()\n    def evaluate(self, val_loader, criterion):\n        \"\"\"Evaluate on validation set\"\"\"\n        self.model.eval()\n        total_loss = 0\n        total_acc = 0\n        num_batches = 0\n\n        pbar = tqdm(val_loader, desc='Validating')\n\n        for batch in pbar:\n            images = batch['image'].to(self.device)\n            text_ids = batch['text_ids'].to(self.device)\n            text_mask = batch['text_mask'].to(self.device)\n            labels = batch['label'].to(self.device)\n\n            logits = self.model(images, text_ids, text_mask)\n            loss = criterion(logits, labels)\n\n            # Accuracy\n            preds = logits.argmax(dim=1)\n            acc = (preds == labels).float().mean()\n\n            total_loss += loss.item()\n            total_acc += acc.item()\n            num_batches += 1\n\n            pbar.set_postfix({\n                'loss': total_loss / num_batches,\n                'acc': total_acc / num_batches\n            })\n\n        return total_loss / num_batches, total_acc / num_batches\n\n    def train(self, train_loader, val_loader, num_epochs=10,\n             lr=1e-4, warmup_steps=1000):\n        \"\"\"Full training loop\"\"\"\n\n        # Optimizer\n        optimizer = AdamW(self.model.parameters(), lr=lr)\n\n        # Scheduler with warmup\n        total_steps = len(train_loader) * num_epochs\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=lr,\n            total_steps=total_steps,\n            pct_start=warmup_steps / total_steps\n        )\n\n        # Mixed precision\n        scaler = torch.cuda.amp.GradScaler()\n\n        # Loss\n        criterion = torch.nn.CrossEntropyLoss()\n\n        # Training loop\n        best_val_loss = float('inf')\n        patience = 5\n        patience_counter = 0\n\n        for epoch in range(num_epochs):\n            print(f\"\\n{'='*50}\")\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n            print(f\"{'='*50}\")\n\n            # Train\n            train_loss = self.train_epoch(\n                train_loader, optimizer, scheduler,\n                criterion, scaler\n            )\n\n            # Validate\n            val_loss, val_acc = self.evaluate(val_loader, criterion)\n\n            print(f\"Train Loss: {train_loss:.4f}\")\n            print(f\"Val Loss: {val_loss:.4f}\")\n            print(f\"Val Acc: {val_acc:.4f}\")\n\n            # Log to wandb\n            if self.use_wandb:\n                wandb.log({\n                    'epoch': epoch,\n                    'train_loss': train_loss,\n                    'val_loss': val_loss,\n                    'val_acc': val_acc\n                })\n\n            # Early stopping\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n\n                # Save checkpoint\n                self.save_checkpoint(f'best_model_epoch{epoch}.pt')\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping after {epoch+1} epochs\")\n                    break\n\n        if self.use_wandb:\n            wandb.finish()\n\n    def save_checkpoint(self, path):\n        \"\"\"Save model checkpoint\"\"\"\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'model_config': self.model.config if hasattr(self.model, 'config') else None\n        }, path)\n        print(f\"Saved checkpoint to {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nmodel = MultimodalModel()\ntrainer = MultimodalTrainer(model)\n\ntrainer.train(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=30,\n    lr=1e-4\n)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.3 Handling Edge Cases and Failures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Handling in Data Loading\n\n```python\nclass RobustDataLoader:\n    \"\"\"Data loader with error handling\"\"\"\n\n    def __init__(self, dataset, batch_size=32, num_workers=4):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.failed_indices = []\n\n    def load_with_retry(self, idx, max_retries=3):\n        \"\"\"Load sample with retry logic\"\"\"\n        for attempt in range(max_retries):\n            try:\n                return self.dataset[idx]\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    print(f\"Failed to load sample {idx} after {max_retries} attempts: {e}\")\n                    self.failed_indices.append(idx)\n                    return None\n\n    def get_valid_batch(self, indices):\n        \"\"\"Get batch skipping failed samples\"\"\"\n        batch = []\n        valid_indices = []\n\n        for idx in indices:\n            sample = self.load_with_retry(idx)\n            if sample is not None:\n                batch.append(sample)\n                valid_indices.append(idx)\n\n        if not batch:\n            return None, []\n\n        # Stack samples\n        try:\n            stacked = {}\n            for key in batch[0].keys():\n                stacked[key] = torch.stack([s[key] for s in batch])\n            return stacked, valid_indices\n        except Exception as e:\n            print(f\"Error stacking batch: {e}\")\n            return None, []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nrobust_loader = RobustDataLoader(dataset, batch_size=32)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation and Sanity Checks\n\n```python\nclass DataValidator:\n    \"\"\"Validate data quality\"\"\"\n\n    @staticmethod\n    def check_image_quality(image_tensor, min_entropy=0.5):\n        \"\"\"Check if image has meaningful content\"\"\"\n        # Calculate entropy\n        import torch.nn.functional as F\n\n        # Flatten and normalize to [0, 1]\n        flat = image_tensor.flatten()\n        flat = (flat - flat.min()) / (flat.max() - flat.min() + 1e-8)\n\n        # Histogram-based entropy\n        hist = torch.histc(flat, bins=256)\n        hist = hist / hist.sum()\n        entropy = -(hist * torch.log(hist + 1e-8)).sum()\n\n        return entropy > min_entropy\n\n    @staticmethod\n    def check_text_quality(text, min_length=5, max_length=1000):\n        \"\"\"Check if text is valid\"\"\"\n        if text is None or not isinstance(text, str):\n            return False\n\n        text = text.strip()\n\n        if len(text) < min_length or len(text) > max_length:\n            return False\n\n        # Check for too many special characters\n        special_chars = sum(1 for c in text if not c.isalnum() and c != ' ')\n        if special_chars / len(text) > 0.5:\n            return False\n\n        return True\n\n    @staticmethod\n    def check_alignment(image_tensor, text, similarity_fn):\n        \"\"\"Check if image and text are aligned\"\"\"\n        # Encode both\n        img_feat = image_encoder(image_tensor.unsqueeze(0))\n        txt_feat = text_encoder(text)\n\n        # Compute similarity\n        sim = similarity_fn(img_feat, txt_feat)\n\n        # Threshold (depends on model)\n        return sim > 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nvalidator = DataValidator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check a sample\nif validator.check_image_quality(image) and \\\n   validator.check_text_quality(text) and \\\n   validator.check_alignment(image, text, similarity_fn):\n    print(\"Sample is valid!\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.4 Optimization for Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Quantization\n\n```python\nclass ModelQuantizer:\n    \"\"\"Quantize model for faster inference\"\"\"\n\n    @staticmethod\n    def quantize_int8(model, sample_input):\n        \"\"\"Convert to INT8 quantization\"\"\"\n        model.eval()\n\n        # Dynamic quantization (easiest)\n        quantized = torch.quantization.quantize_dynamic(\n            model,\n            {torch.nn.Linear},\n            dtype=torch.qint8\n        )\n\n        return quantized\n\n    @staticmethod\n    def quantize_with_calibration(model, calibration_loader):\n        \"\"\"Quantization with calibration data\"\"\"\n        model.eval()\n        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n        # Insert observers\n        torch.quantization.prepare(model, inplace=True)\n\n        # Calibrate with sample data\n        with torch.no_grad():\n            for batch in calibration_loader:\n                _ = model(batch)\n\n        # Convert to quantized model\n        torch.quantization.convert(model, inplace=True)\n\n        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nquantizer = ModelQuantizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple quantization\nq_model = quantizer.quantize_int8(model, sample_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory savings\nprint(f\"Original model size: {get_model_size(model):.2f} MB\")\nprint(f\"Quantized model size: {get_model_size(q_model):.2f} MB\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeDistiller:\n    \"\"\"Distill large model to small student\"\"\"\n\n    def __init__(self, teacher_model, student_model, temperature=3.0):\n        self.teacher = teacher_model\n        self.student = student_model\n        self.temperature = temperature\n\n    def distillation_loss(self, student_logits, teacher_logits, labels,\n                         alpha=0.7):\n        \"\"\"Combined distillation + task loss\"\"\"\n        # KL divergence for distillation\n        kd_loss = torch.nn.functional.kl_div(\n            torch.nn.functional.log_softmax(\n                student_logits / self.temperature,\n                dim=1\n            ),\n            torch.nn.functional.softmax(\n                teacher_logits / self.temperature,\n                dim=1\n            ),\n            reduction='batchmean'\n        ) * (self.temperature ** 2)\n\n        # Task loss\n        task_loss = torch.nn.functional.cross_entropy(\n            student_logits,\n            labels\n        )\n\n        # Combined\n        return alpha * kd_loss + (1 - alpha) * task_loss\n\n    def train_student(self, train_loader, optimizer, num_epochs):\n        \"\"\"Train student model\"\"\"\n        self.teacher.eval()\n\n        for epoch in range(num_epochs):\n            total_loss = 0\n\n            for batch in train_loader:\n                # Teacher predictions (no gradients)\n                with torch.no_grad():\n                    teacher_logits = self.teacher(batch)\n\n                # Student predictions\n                student_logits = self.student(batch)\n\n                # Loss\n                loss = self.distillation_loss(\n                    student_logits,\n                    teacher_logits,\n                    batch['label']\n                )\n\n                # Backprop\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Serving with TorchServe\n\n```yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# config.yaml\nmodel_store: ./model_store\nncs: false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handler (my_handler.py)\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\nclass MultimodalHandler:\n    def __init__(self):\n        self.image_model = AutoModel.from_pretrained('model_name')\n        self.text_model = AutoModel.from_pretrained('model_name')\n        self.tokenizer = AutoTokenizer.from_pretrained('model_name')\n\n    def preprocess(self, data):\n        image = data['image']\n        text = data['text']\n\n        tokens = self.tokenizer(text, return_tensors='pt')\n\n        return image, tokens\n\n    def inference(self, image, tokens):\n        img_feat = self.image_model(image)\n        txt_feat = self.text_model(tokens['input_ids'])\n\n        # Compute similarity\n        similarity = torch.cosine_similarity(img_feat, txt_feat)\n\n        return similarity\n\n    def postprocess(self, output):\n        return {'similarity': float(output)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# torch-model-archiver --model-name multimodal \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#     --version 1.0 \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#     --model-file model.py \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#     --serialized-file model.pt \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#     --handler my_handler.py \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#     --export-path model_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# torchserve --start --model-store model_store \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#     --ncs --models multimodal=multimodal.mar\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5 Monitoring and Maintenance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Performance Monitoring\n\n```python\nimport logging\nfrom datetime import datetime\n\nclass ModelMonitor:\n    \"\"\"Monitor model performance in production\"\"\"\n\n    def __init__(self, log_file='model_performance.log'):\n        self.log_file = log_file\n        self.setup_logging()\n\n    def setup_logging(self):\n        logging.basicConfig(\n            filename=self.log_file,\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n\n    def check_drift(self, current_batch, reference_data):\n        \"\"\"Check for data drift\"\"\"\n        # Compare statistics\n        current_mean = current_batch.mean()\n        reference_mean = reference_data.mean()\n\n        # Z-test\n        drift_score = abs(current_mean - reference_mean) / reference_data.std()\n\n        if drift_score > 3.0:  # Threshold\n            logging.warning(f\"Data drift detected: {drift_score:.2f}\")\n            return True\n\n        return False\n\n    def log_prediction(self, input_id, prediction, confidence, latency):\n        \"\"\"Log prediction for audit trail\"\"\"\n        log_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'input_id': input_id,\n            'prediction': prediction,\n            'confidence': float(confidence),\n            'latency_ms': latency\n        }\n\n        logging.info(str(log_entry))\n\n    def detect_anomalies(self, predictions, threshold=2.0):\n        \"\"\"Detect anomalous predictions\"\"\"\n        confidences = [p['confidence'] for p in predictions]\n        mean_conf = np.mean(confidences)\n        std_conf = np.std(confidences)\n\n        anomalies = []\n        for i, pred in enumerate(predictions):\n            z_score = abs(pred['confidence'] - mean_conf) / (std_conf + 1e-6)\n            if z_score > threshold:\n                anomalies.append(i)\n\n        return anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nmonitor = ModelMonitor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# During inference\nfor batch in inference_batches:\n    predictions = model(batch)\n\n    for i, pred in enumerate(predictions):\n        monitor.log_prediction(\n            input_id=batch['id'][i],\n            prediction=pred['class'],\n            confidence=pred['confidence'],\n            latency=pred['latency_ms']\n        )\n\n    # Check for issues\n    if monitor.check_drift(batch, reference_batch):\n        print(\"Model may need retraining!\")\n\n    anomalies = monitor.detect_anomalies(predictions)\n    if anomalies:\n        print(f\"Anomalous predictions at indices: {anomalies}\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A/B Testing\n\n```python\nclass ABTester:\n    \"\"\"A/B testing for model updates\"\"\"\n\n    def __init__(self, model_a, model_b, split_ratio=0.5):\n        self.model_a = model_a\n        self.model_b = model_b\n        self.split_ratio = split_ratio\n        self.results = {'a': [], 'b': []}\n\n    def predict(self, input_data, user_id=None):\n        \"\"\"Route to model A or B\"\"\"\n        # Consistent routing per user\n        if user_id is not None:\n            use_a = hash(user_id) % 100 < (self.split_ratio * 100)\n        else:\n            use_a = np.random.rand() < self.split_ratio\n\n        if use_a:\n            prediction = self.model_a(input_data)\n            self.results['a'].append(prediction)\n            return prediction, 'a'\n        else:\n            prediction = self.model_b(input_data)\n            self.results['b'].append(prediction)\n            return prediction, 'b'\n\n    def get_statistics(self):\n        \"\"\"Compare model performance\"\"\"\n        def compute_stats(results):\n            accs = [r['accuracy'] for r in results]\n            return {\n                'mean_accuracy': np.mean(accs),\n                'std_accuracy': np.std(accs),\n                'count': len(accs)\n            }\n\n        stats_a = compute_stats(self.results['a'])\n        stats_b = compute_stats(self.results['b'])\n\n        # Statistical test\n        from scipy import stats\n        t_stat, p_value = stats.ttest_ind(\n            [r['accuracy'] for r in self.results['a']],\n            [r['accuracy'] for r in self.results['b']]\n        )\n\n        return {\n            'model_a': stats_a,\n            'model_b': stats_b,\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'winner': 'b' if stats_b['mean_accuracy'] > stats_a['mean_accuracy'] else 'a'\n        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nab_tester = ABTester(model_v1, model_v2, split_ratio=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In production\nfor request in requests:\n    prediction, model_used = ab_tester.predict(request, user_id=request['user_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# After collecting data\nstats = ab_tester.get_statistics()\nprint(f\"Winner: Model {stats['winner']}\")\nprint(f\"P-value: {stats['p_value']}\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Preprocessing is critical** - garbage in, garbage out\n- **Robust error handling** prevents cascading failures\n- **Monitoring catches issues early** - drift, anomalies, degradation\n- **Optimization techniques** make models production-ready\n- **A/B testing validates improvements** before full rollout\n- **MLOps practices** enable reliable systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n**‚≠ê Beginner:**\n1. Build image preprocessing pipeline\n2. Create text tokenization pipeline\n3. Implement basic data validation\n\n**‚≠ê‚≠ê Intermediate:**\n4. Build multimodal dataset loader\n5. Implement training loop with early stopping\n6. Add logging and monitoring\n\n**‚≠ê‚≠ê‚≠ê Advanced:**\n7. Implement model quantization\n8. Set up knowledge distillation\n9. Deploy model with monitoring\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Interactive Demonstrations\n\nTry these interactive examples to explore the concepts hands-on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive: Load and display a sample image\n",
        "def load_sample_image():\n",
        "    # Using a sample image URL (replace with your own)\n",
        "    url = \"https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(requests.get(url, stream=True).raw)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Sample Image for Multimodal Processing')\n",
        "        plt.show()\n",
        "        return img\n",
        "    except:\n",
        "        print(\"Could not load sample image. Please use your own image.\")\n",
        "        return None\n",
        "\n",
        "# Uncomment to run:\n",
        "# sample_img = load_sample_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Exercises\n\nTry these exercises to practice the concepts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise placeholder - students can implement solutions here\n",
        "def exercise_placeholder():\n",
        "    \"\"\"\n",
        "    Implement your solution here!\n",
        "    \n",
        "    Tips:\n",
        "    - Start with simple examples\n",
        "    - Add print statements to debug\n",
        "    - Use the provided helper functions\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Hands-On Implementation: Image-Caption Retrieval\n\n",
        "Let's build a complete image-caption retrieval system!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's install and import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Check if transformers is available\n",
        "try:\n",
        "    from transformers import CLIPProcessor, CLIPModel\n",
        "    CLIP_AVAILABLE = True\n",
        "    print(\"‚úÖ CLIP available for demonstrations\")\n",
        "except ImportError:\n",
        "    CLIP_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è CLIP not available, using simplified examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleMultimodalRetrieval:\n",
        "    \"\"\"Simple multimodal retrieval system for demonstration.\"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim=256):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.image_database = []\n",
        "        self.caption_database = []\n",
        "        self.image_embeddings = None\n",
        "        self.caption_embeddings = None\n",
        "    \n",
        "    def add_data(self, images, captions):\n",
        "        \"\"\"Add image-caption pairs to the database.\"\"\"\n",
        "        self.image_database.extend(images)\n",
        "        self.caption_database.extend(captions)\n",
        "        \n",
        "        # For demo: create random embeddings\n",
        "        # In practice, use proper encoders\n",
        "        num_items = len(images)\n",
        "        \n",
        "        # Simulate image embeddings\n",
        "        img_embeds = torch.randn(num_items, self.embed_dim)\n",
        "        img_embeds = F.normalize(img_embeds, dim=-1)\n",
        "        \n",
        "        # Simulate caption embeddings (with some correlation to images)\n",
        "        cap_embeds = 0.8 * img_embeds + 0.2 * torch.randn(num_items, self.embed_dim)\n",
        "        cap_embeds = F.normalize(cap_embeds, dim=-1)\n",
        "        \n",
        "        if self.image_embeddings is None:\n",
        "            self.image_embeddings = img_embeds\n",
        "            self.caption_embeddings = cap_embeds\n",
        "        else:\n",
        "            self.image_embeddings = torch.cat([self.image_embeddings, img_embeds])\n",
        "            self.caption_embeddings = torch.cat([self.caption_embeddings, cap_embeds])\n",
        "    \n",
        "    def retrieve_by_text(self, query_text, top_k=5):\n",
        "        \"\"\"Retrieve images by text query.\"\"\"\n",
        "        # Simulate query embedding\n",
        "        query_embed = torch.randn(1, self.embed_dim)\n",
        "        query_embed = F.normalize(query_embed, dim=-1)\n",
        "        \n",
        "        # Compute similarities\n",
        "        similarities = torch.matmul(query_embed, self.image_embeddings.T)\n",
        "        \n",
        "        # Get top-k\n",
        "        _, indices = torch.topk(similarities, top_k, dim=-1)\n",
        "        \n",
        "        results = []\n",
        "        for idx in indices[0]:\n",
        "            results.append({\n",
        "                'image_idx': idx.item(),\n",
        "                'image': self.image_database[idx],\n",
        "                'caption': self.caption_database[idx],\n",
        "                'similarity': similarities[0, idx].item()\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def retrieve_by_image(self, query_image_idx, top_k=5):\n",
        "        \"\"\"Retrieve captions by image query.\"\"\"\n",
        "        query_embed = self.image_embeddings[query_image_idx:query_image_idx+1]\n",
        "        \n",
        "        # Compute similarities with captions\n",
        "        similarities = torch.matmul(query_embed, self.caption_embeddings.T)\n",
        "        \n",
        "        # Get top-k\n",
        "        _, indices = torch.topk(similarities, top_k, dim=-1)\n",
        "        \n",
        "        results = []\n",
        "        for idx in indices[0]:\n",
        "            results.append({\n",
        "                'caption_idx': idx.item(),\n",
        "                'caption': self.caption_database[idx],\n",
        "                'similarity': similarities[0, idx].item()\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Create demo dataset\n",
        "demo_images = [f\"image_{i}.jpg\" for i in range(10)]\n",
        "demo_captions = [\n",
        "    \"A cute cat sitting on a windowsill\",\n",
        "    \"Beautiful sunset over the ocean\", \n",
        "    \"A red sports car driving on highway\",\n",
        "    \"Children playing in a garden\",\n",
        "    \"Snow-covered mountain peaks\",\n",
        "    \"A dog running in the park\",\n",
        "    \"Delicious pizza with vegetables\",\n",
        "    \"Modern cityscape at night\",\n",
        "    \"Colorful flowers in spring\",\n",
        "    \"Vintage bicycle on cobblestone street\"\n",
        "]\n",
        "\n",
        "# Initialize and populate retrieval system\n",
        "retrieval_system = SimpleMultimodalRetrieval()\n",
        "retrieval_system.add_data(demo_images, demo_captions)\n",
        "\n",
        "print(f\"‚úÖ Retrieval system initialized with {len(demo_images)} image-caption pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Text-to-Image Retrieval\n",
        "def demo_text_retrieval():\n",
        "    queries = [\"cute animal\", \"outdoor scene\", \"vehicle\"]\n",
        "    \n",
        "    for query in queries:\n",
        "        print(f\"\\nüîç Query: '{query}'\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        results = retrieval_system.retrieve_by_text(query, top_k=3)\n",
        "        \n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"{i}. {result['caption']} (similarity: {result['similarity']:.3f})\")\n",
        "\n",
        "demo_text_retrieval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Image-to-Text Retrieval\n",
        "def demo_image_retrieval():\n",
        "    # Try different images as queries\n",
        "    query_indices = [0, 2, 5]\n",
        "    \n",
        "    for idx in query_indices:\n",
        "        print(f\"\\nüñºÔ∏è Query Image: {demo_images[idx]}\")\n",
        "        print(f\"Ground Truth: {demo_captions[idx]}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        results = retrieval_system.retrieve_by_image(idx, top_k=3)\n",
        "        \n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"{i}. {result['caption']} (similarity: {result['similarity']:.3f})\")\n",
        "\n",
        "demo_image_retrieval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Evaluation Metrics for Multimodal Retrieval\n\n",
        "Let's implement common evaluation metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_retrieval_metrics(similarities, k_values=[1, 5, 10]):\n",
        "    \"\"\"Compute Recall@K for retrieval task.\"\"\"\n",
        "    batch_size = similarities.shape[0]\n",
        "    \n",
        "    # Get ranks (0-indexed)\n",
        "    ranks = torch.argsort(similarities, dim=1, descending=True)\n",
        "    \n",
        "    # For each item, find where the correct match appears\n",
        "    correct_ranks = []\n",
        "    for i in range(batch_size):\n",
        "        # Find position of correct answer (diagonal element)\n",
        "        correct_pos = (ranks[i] == i).nonzero().item()\n",
        "        correct_ranks.append(correct_pos)\n",
        "    \n",
        "    correct_ranks = torch.tensor(correct_ranks)\n",
        "    \n",
        "    # Compute Recall@K\n",
        "    metrics = {}\n",
        "    for k in k_values:\n",
        "        recall_at_k = (correct_ranks < k).float().mean().item()\n",
        "        metrics[f'Recall@{k}'] = recall_at_k\n",
        "    \n",
        "    # Median rank\n",
        "    metrics['Median_Rank'] = torch.median(correct_ranks.float()).item()\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Demo evaluation\n",
        "# Create synthetic similarity matrix\n",
        "batch_size = 50\n",
        "similarities = torch.randn(batch_size, batch_size)\n",
        "\n",
        "# Add some structure: make diagonal elements higher (correct matches)\n",
        "similarities += 2 * torch.eye(batch_size)\n",
        "\n",
        "metrics = compute_retrieval_metrics(similarities)\n",
        "\n",
        "print(\"üìä Retrieval Metrics:\")\n",
        "for metric, value in metrics.items():\n",
        "    if 'Recall' in metric:\n",
        "        print(f\"{metric}: {value:.1%}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value:.1f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}