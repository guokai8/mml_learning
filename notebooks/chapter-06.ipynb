{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Setup and Imports\n\nRun this cell first to install required packages and import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision transformers numpy matplotlib seaborn pandas pillow requests\n",
        "!pip install clip-by-openai sentence-transformers datasets\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"âœ… Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 6: Attention Mechanisms in Multimodal Systems\n\n**Interactive Jupyter Notebook Version**\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 6: Attention Mechanisms in Multimodal Systems\n\n---\n\n**Previous**: [Chapter 5: Fusion Strategies](chapter-05.md) | **Next**: [Chapter 7: Contrastive Learning](chapter-07.md) | **Home**: [Table of Contents](index.md)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 6: Attention Mechanisms in Multimodal Systems (FULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand attention mechanism fundamentals and intuition\n- Implement scaled dot-product attention from scratch\n- Understand multi-head attention and its role\n- Apply cross-attention for multimodal fusion\n- Visualize and interpret attention patterns\n- Debug attention-based models\n- Optimize attention for efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Foundations of Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Problem Attention Solves\n\n**Before attention (sequence-to-sequence models):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTask: Translate English to French\n\nEnglish: \"The quick brown fox jumps\"\nFrench:  \"Le rapide renard brun saute\"\n\nRNN approach (encoder-decoder):\n\nEncoder:\n  Step 1: Process \"The\" â†’ hâ‚\n  Step 2: Process \"quick\" â†’ hâ‚‚\n  Step 3: Process \"brown\" â†’ hâ‚ƒ\n  Step 4: Process \"fox\" â†’ hâ‚„\n  Step 5: Process \"jumps\" â†’ hâ‚…\n\n  Final state: hâ‚… (tries to contain all information!)\n\nDecoder:\n  Uses only hâ‚… to generate entire translation\n\n  Step 1: Generate \"Le\" from hâ‚…\n  Step 2: Generate \"rapide\" from hâ‚…\n  Step 3: Generate \"renard\" from hâ‚…\n  Step 4: Generate \"brun\" from hâ‚…\n  Step 5: Generate \"saute\" from hâ‚…\n\nProblem:\n  âœ— All information bottlenecked into single vector hâ‚…\n  âœ— Cannot remember which input word to focus on\n  âœ— Long sentences lose information\n  âœ— No obvious alignment between input and output\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**With attention:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEncoder (same):\n  Produces hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„, hâ‚…\n\nDecoder with attention:\n  Step 1: Generate \"Le\"\n    Where to look? \"The\" â†’ attention to hâ‚\n    Generate \"Le\" using context from hâ‚\n\n  Step 2: Generate \"rapide\"\n    Where to look? \"quick\" â†’ attention to hâ‚‚\n    Generate \"rapide\" using context from hâ‚‚\n\n  Step 3: Generate \"renard\"\n    Where to look? \"brown\" or \"fox\" â†’ attention to hâ‚ƒ and hâ‚„\n    Generate \"renard\" using blended context\n\n  Step 4: Generate \"brun\"\n    Where to look? \"brown\" â†’ attention to hâ‚ƒ\n    Generate \"brun\" using context from hâ‚ƒ\n\n  Step 5: Generate \"saute\"\n    Where to look? \"jumps\" â†’ attention to hâ‚…\n    Generate \"saute\" using context from hâ‚…\n\nBenefits:\n  âœ“ Each output can look at relevant inputs\n  âœ“ No information bottleneck\n  âœ“ Explicit alignment learned\n  âœ“ Works better on long sequences\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention Intuition\n\n**Analogy 1: Restaurant waiter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nScene: Busy restaurant with 10 tables\n\nWaiter's task: Serve Table 5\n\nProcess:\n  1. Look around (attention mechanism)\n  2. Pay attention to Table 5 specifically\n  3. Focus 90% on Table 5\n  4. Glance at nearby tables (10% split)\n  5. Retrieve correct order from Table 5\n  6. Serve Table 5\n\nAttention score for each table:\n  Table 1: 0.0  (far away)\n  Table 2: 0.02 (nearby but not relevant)\n  Table 3: 0.03\n  Table 4: 0.05\n  Table 5: 0.85 â† Focus here!\n  Table 6: 0.03\n  Table 7: 0.01\n  Table 8: 0.01\n  Table 9: 0.0\n  Table 10: 0.0\n\nResult: Service based on relevant information\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analogy 2: Reading comprehension**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nQuestion: \"What did the fox do?\"\n\nPassage: \"The quick brown fox jumped over the lazy dog\"\n\nHuman reading process:\n  1. Read question: \"What did the fox do?\"\n  2. Scan passage\n  3. Pay attention to parts mentioning \"fox\"\n    - \"brown fox\" â† relevant\n    - \"jumped over\" â† relevant\n  4. Ignore irrelevant parts\n    - \"quick\" â† less relevant\n    - \"lazy dog\" â† not about fox\n  5. Combine relevant information\n  6. Answer: \"jumped over the lazy dog\"\n\nAttention mechanism:\n  Query: \"fox\" (what are we asking about?)\n  Keys: [the, quick, brown, fox, jumped, over, the, lazy, dog]\n  Attention: Focus on \"fox\", \"jumped\", \"over\"\n  Values: Combine corresponding information\n  Result: Answer the question\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Attention is Powerful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nKey insight: Solve \"what to look at\" problem\n\nBefore attention:\n  Model processes everything equally\n  Must compress all info into fixed vector\n  Gradient flow: Diluted through all positions\n\nWith attention:\n  Model focuses on relevant information\n  Can dynamically select what matters\n  Gradient flow: Strong to important positions\n  Learning: Faster and better\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Scaled Dot-Product Attention (Complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Deep Dive\n\n**Core formula:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAttention(Q, K, V) = softmax(Q @ K^T / âˆšd_k) @ V\n\nComponents:\n  Q (Query): (batch, seq_len, d_k)\n  K (Key):   (batch, seq_len, d_k)\n  V (Value): (batch, seq_len, d_v)\n\n  Output: (batch, seq_len, d_v)\n\nDimensions typically:\n  d_k = 64\n  d_v = 64\n  seq_len = 196 (for image patches) or 77 (for text tokens)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step-by-Step Computation\n\n**Complete example with real numbers:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSetup:\n  Sequence: [\"cat\", \"sat\", \"mat\"]\n  Query dimension: 2 (for simplicity)\n\nQuery vectors:\n  Q = [\n    [1.0, 0.5],      # \"cat\"\n    [0.5, 1.0],      # \"sat\"\n    [0.3, 0.7]       # \"mat\"\n  ]\n\nKey vectors (same as queries in self-attention):\n  K = Q = [\n    [1.0, 0.5],\n    [0.5, 1.0],\n    [0.3, 0.7]\n  ]\n\nValue vectors:\n  V = [\n    [2, 1],          # \"cat\" value\n    [1, 2],          # \"sat\" value\n    [1.5, 1.5]       # \"mat\" value\n  ]\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nStep 1: Compute Q @ K^T (similarity)\n\nQ @ K^T:\n  Q[0] Â· K^T = [1.0, 0.5] @ [[1.0, 0.5, 0.3],\n                              [0.5, 1.0, 0.7]]\n             = [1.0*1.0 + 0.5*0.5,    1.0*0.5 + 0.5*1.0,   1.0*0.3 + 0.5*0.7]\n             = [1.0 + 0.25,           0.5 + 0.5,           0.3 + 0.35]\n             = [1.25,                 1.0,                 0.65]\n\n  Q[1] Â· K^T = [0.5, 1.0] @ ...\n             = [0.5*1.0 + 1.0*0.5,    0.5*0.5 + 1.0*1.0,   0.5*0.3 + 1.0*0.7]\n             = [0.5 + 0.5,            0.25 + 1.0,          0.15 + 0.7]\n             = [1.0,                  1.25,                0.85]\n\n  Q[2] Â· K^T = [0.3, 0.7] @ ...\n             = [0.3*1.0 + 0.7*0.5,    0.3*0.5 + 0.7*1.0,   0.3*0.3 + 0.7*0.7]\n             = [0.3 + 0.35,           0.15 + 0.7,          0.09 + 0.49]\n             = [0.65,                 0.85,                0.58]\n\nResult: Similarity matrix\n  [\n    [1.25, 1.0,  0.65],\n    [1.0,  1.25, 0.85],\n    [0.65, 0.85, 0.58]\n  ]\n\nInterpretation:\n  Position 0 most similar to: itself (1.25)\n  Position 1 most similar to: itself (1.25)\n  Position 2 most similar to: itself (0.58)\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nStep 2: Scale by 1/âˆšd_k\n\nd_k = 2, so âˆšd_k = âˆš2 â‰ˆ 1.414\n\nScaled:\n  [\n    [1.25/1.414,  1.0/1.414,  0.65/1.414],\n    [1.0/1.414,   1.25/1.414, 0.85/1.414],\n    [0.65/1.414,  0.85/1.414, 0.58/1.414]\n  ]\n= [\n    [0.884,  0.707, 0.460],\n    [0.707,  0.884, 0.601],\n    [0.460,  0.601, 0.410]\n  ]\n\nWhy scale?\n  Prevents dot product from getting too large\n  Keeps gradients reasonable\n  Stabilizes training\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nStep 3: Apply softmax\n\nFor position 0: [0.884, 0.707, 0.460]\n\nFirst compute exponentials:\n  e^0.884 â‰ˆ 2.42\n  e^0.707 â‰ˆ 2.03\n  e^0.460 â‰ˆ 1.58\n  Sum = 6.03\n\nSoftmax:\n  [2.42/6.03,  2.03/6.03,  1.58/6.03]\n= [0.401,      0.337,      0.262]\n\nInterpretation:\n  \"cat\" attends 40% to itself\n  \"cat\" attends 34% to \"sat\"\n  \"cat\" attends 26% to \"mat\"\n\nFor position 1: [0.707, 0.884, 0.601]\n  e^0.707 â‰ˆ 2.03\n  e^0.884 â‰ˆ 2.42\n  e^0.601 â‰ˆ 1.82\n  Sum = 6.27\n\n  Softmax: [0.324, 0.386, 0.290]\n\nFor position 2: [0.460, 0.601, 0.410]\n  e^0.460 â‰ˆ 1.58\n  e^0.601 â‰ˆ 1.82\n  e^0.410 â‰ˆ 1.51\n  Sum = 4.91\n\n  Softmax: [0.322, 0.371, 0.307]\n\nAttention matrix (after softmax):\n  [\n    [0.401, 0.337, 0.262],\n    [0.324, 0.386, 0.290],\n    [0.322, 0.371, 0.307]\n  ]\n\nEach row sums to 1 âœ“\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nStep 4: Apply to values\n\nFor position 0:\n  attention_output[0] = 0.401 * V[0] + 0.337 * V[1] + 0.262 * V[2]\n                      = 0.401 * [2, 1] + 0.337 * [1, 2] + 0.262 * [1.5, 1.5]\n                      = [0.802, 0.401] + [0.337, 0.674] + [0.393, 0.393]\n                      = [1.532, 1.468]\n\nFor position 1:\n  attention_output[1] = 0.324 * [2, 1] + 0.386 * [1, 2] + 0.290 * [1.5, 1.5]\n                      = [0.648, 0.324] + [0.386, 0.772] + [0.435, 0.435]\n                      = [1.469, 1.531]\n\nFor position 2:\n  attention_output[2] = 0.322 * [2, 1] + 0.371 * [1, 2] + 0.307 * [1.5, 1.5]\n                      = [0.644, 0.322] + [0.371, 0.742] + [0.461, 0.461]\n                      = [1.476, 1.525]\n\nFinal output:\n  [\n    [1.532, 1.468],\n    [1.469, 1.531],\n    [1.476, 1.525]\n  ]\n\nInterpretation:\n  Each position now contains weighted combination of all values\n  Weights determined by attention scores\n  Result: Context-aware representations\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation from Scratch\n\n```python\nimport torch\nimport torch.nn.functional as F\nimport math\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Compute scaled dot-product attention\n\n    Args:\n        Q: Query tensor (batch, seq_len, d_k)\n        K: Key tensor (batch, seq_len, d_k)\n        V: Value tensor (batch, seq_len, d_v)\n        mask: Optional mask for positions to ignore\n\n    Returns:\n        output: Attention output (batch, seq_len, d_v)\n        attention_weights: Attention scores (batch, seq_len, seq_len)\n    \"\"\"\n\n    # Get dimension\n    d_k = Q.shape[-1]\n\n    # Step 1: Compute similarity scores\n    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)\n\n    # Step 2: Scale by âˆšd_k\n    scores = scores / math.sqrt(d_k)\n\n    # Step 3: Apply mask (optional)\n    if mask is not None:\n        # Set masked positions to very negative number\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n\n    # Step 4: Apply softmax\n    attention_weights = torch.softmax(scores, dim=-1)\n\n    # Handle NaN from softmax(-inf)\n    attention_weights = torch.nan_to_num(attention_weights, 0.0)\n\n    # Step 5: Apply to values\n    output = torch.matmul(attention_weights, V)  # (batch, seq_len, d_v)\n\n    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example usage\nbatch_size = 2\nseq_len = 3\nd_k = 2\nd_v = 2\n\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_v)\n\noutput, attention_weights = scaled_dot_product_attention(Q, K, V)\n\nprint(f\"Output shape: {output.shape}\")  # (2, 3, 2)\nprint(f\"Attention weights shape: {attention_weights.shape}\")  # (2, 3, 3)\nprint(f\"Attention weights row sum: {attention_weights.sum(dim=-1)}\")  # Should be all 1s\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Gradients\n\n**Backpropagation through attention:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nForward pass:\n  Q @ K^T â†’ Scale â†’ Softmax â†’ @ V\n\nBackward pass:\n  dL/dV: Direct gradient from output\n  dL/dSoftmax: Chain from V gradient\n  dL/dScale: Chain from softmax gradient\n  dL/dScores: Chain from scale\n  dL/dK, dL/dQ: Chain from scores\n\nKey insight: Gradients flow through attention weights\n\nIf attention_weights[i,j] is high:\n  Position i receives strong gradient from j\n  Strong learning signal\n\nIf attention_weights[i,j] is low:\n  Position i receives weak gradient from j\n  Weak learning signal\n\nResult: Model learns to attend to relevant positions\n        through gradient flow\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3 Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Multiple Heads?\n\n**Problem with single head:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSingle attention head learns one type of relationship\n\nFor text \"The cat sat on the mat\":\n\nWhat if different relationships matter?\n  Syntactic: Articles attend to nouns\n  Semantic: Pronouns attend to antecedents\n  Discourse: Later sentences attend to earlier context\n\nSingle head must learn all simultaneously\nDifficult optimization problem\nLimited capacity\n\nSolution: Multiple heads\nEach head learns different relationships\nParallel processing\nCombine results\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture\n\n**Multi-head formula:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nMultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\n\nwhere head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)\n\nh = number of heads (typically 8-16)\nW_i^Q, W_i^K, W_i^V = Projection matrices for head i\nW^O = Output projection\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Detailed breakdown:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nInput: (batch, seq_len, d_model)\n\nFor each head i = 1 to h:\n\n  1. Project to smaller dimension\n     Q_i = input @ W_i^Q     (batch, seq_len, d_k)\n     K_i = input @ W_i^K     (batch, seq_len, d_k)\n     V_i = input @ W_i^V     (batch, seq_len, d_v)\n\n     Typical: d_model = 512, h = 8\n              d_k = d_v = 512/8 = 64\n\n  2. Compute attention\n     head_i = Attention(Q_i, K_i, V_i)  (batch, seq_len, 64)\n\n  3. Repeat for all 8 heads\n     Result: 8 attention outputs\n             Each (batch, seq_len, 64)\n\nConcatenate all heads:\n  Combined = [head_1 || head_2 || ... || head_8]\n           (batch, seq_len, 512)\n\nOutput projection:\n  output = Combined @ W^O\n         (batch, seq_len, d_model)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation\n\n```python\nclass MultiHeadAttention(torch.nn.Module):\n    \"\"\"Multi-head attention layer\"\"\"\n\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Linear projections\n        self.W_q = torch.nn.Linear(d_model, d_model)\n        self.W_k = torch.nn.Linear(d_model, d_model)\n        self.W_v = torch.nn.Linear(d_model, d_model)\n        self.W_o = torch.nn.Linear(d_model, d_model)\n\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, Q, K, V, mask=None):\n        \"\"\"\n        Args:\n            Q: Query (batch, seq_len_q, d_model)\n            K: Key (batch, seq_len_k, d_model)\n            V: Value (batch, seq_len_v, d_model)\n            mask: Optional attention mask\n\n        Returns:\n            output: (batch, seq_len_q, d_model)\n        \"\"\"\n        batch_size = Q.shape[0]\n\n        # Step 1: Linear projections\n        Q = self.W_q(Q)  # (batch, seq_len_q, d_model)\n        K = self.W_k(K)  # (batch, seq_len_k, d_model)\n        V = self.W_v(V)  # (batch, seq_len_v, d_model)\n\n        # Step 2: Reshape for multi-head attention\n        # Split into h heads\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        # (batch, num_heads, seq_len_q, d_k)\n\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        # (batch, num_heads, seq_len_k, d_k)\n\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        # (batch, num_heads, seq_len_v, d_k)\n\n        # Step 3: Attention for each head\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        # (batch, num_heads, seq_len_q, seq_len_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        attention_weights = torch.softmax(scores, dim=-1)\n        attention_weights = torch.nan_to_num(attention_weights, 0.0)\n        attention_weights = self.dropout(attention_weights)\n\n        # Apply to values\n        output = torch.matmul(attention_weights, V)\n        # (batch, num_heads, seq_len_q, d_k)\n\n        # Step 4: Concatenate heads\n        output = output.transpose(1, 2).contiguous()\n        # (batch, seq_len_q, num_heads, d_k)\n\n        output = output.view(batch_size, -1, self.d_model)\n        # (batch, seq_len_q, d_model)\n\n        # Step 5: Output projection\n        output = self.W_o(output)\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example\nmha = MultiHeadAttention(d_model=512, num_heads=8)\nQ = torch.randn(2, 10, 512)  # batch_size=2, seq_len=10, d_model=512\nK = torch.randn(2, 10, 512)\nV = torch.randn(2, 10, 512)\n\noutput = mha(Q, K, V)\nprint(f\"Output shape: {output.shape}\")  # (2, 10, 512)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Head Specialization\n\n**What different heads learn:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nExample: Sentence \"The cat sat on the mat\"\n\nHead 1 (Syntactic):\n  Attention pattern:\n    \"The\" â†’ \"cat\" (article to noun)\n    \"sat\" â†’ \"cat\", \"on\", \"mat\" (verb to objects)\n  Learns: Grammatical relationships\n\nHead 2 (Semantic):\n  Attention pattern:\n    \"cat\" â†’ \"mat\" (related nouns)\n    \"on\" â†’ \"cat\", \"mat\" (location relation)\n  Learns: Semantic relationships\n\nHead 3 (Long-range):\n  Attention pattern:\n    \"mat\" â†’ \"The\" (distant words)\n    \"sat\" â†’ \"cat\" (key pairs)\n  Learns: Global context\n\nHead 4 (Rare/Noise):\n  Attention pattern:\n    \"on\" â†’ \"on\", \"the\" (less obvious)\n    \"sat\" â†’ \"sat\" (self-attention)\n  Learns: Residual patterns\n\nResult: Complementary representations\n        Ensemble of different perspectives\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.4 Cross-Attention for Multimodal Fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concept and Setup\n\n**What is cross-attention?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSelf-attention:\n  Q, K, V all from same source\n  Example: Text attends to text\n  \"Which words are relevant to which other words?\"\n\nCross-attention:\n  Q from one modality, K/V from another\n  Example: Text queries image features\n  \"Which image regions are relevant to this word?\"\n\nBenefits for multimodal:\n  â‘  Explicit alignment between modalities\n  â‘¡ Each modality can query the other\n  â‘¢ Information flow controlled by queries\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Image-to-Text Cross-Attention\n\n**Setup:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage: Visual features from CNN/ViT\n  Shape: (batch, num_patches, d_image)\n  Example: (2, 196, 2048) from ResNet50\n\nText: Token embeddings from BERT\n  Shape: (batch, seq_len, d_text)\n  Example: (2, 77, 768)\n\nGoal: Text should understand image context\n      Image should influence text processing\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Cross-attention computation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nQuery: Text embeddings\n  Q = text_embeddings @ W_q\n  Shape: (batch, seq_len_text, d_k)\n\nKey/Value: Image features\n  K = image_features @ W_k\n  Shape: (batch, num_patches, d_k)\n\n  V = image_features @ W_v\n  Shape: (batch, num_patches, d_v)\n\nAttention:\n  scores = Q @ K^T / âˆšd_k\n  Shape: (batch, seq_len_text, num_patches)\n\n  Interpretation:\n    For each word (seq_len_text)\n    How relevant is each image patch (num_patches)?\n\n    Word \"red\" attends to:\n      Red patches in image (high score)\n      Other patches (low score)\n\nWeighted sum:\n  output = softmax(scores) @ V\n  Shape: (batch, seq_len_text, d_v)\n\n  Each word now contains information about\n  relevant image regions\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation\n\n```python\nclass CrossAttention(torch.nn.Module):\n    \"\"\"Cross-attention between two modalities\"\"\"\n\n    def __init__(self, d_q, d_k, d_v, num_heads=8):\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.d_k = d_k // num_heads\n        self.d_v = d_v // num_heads\n\n        # Query projection (from modality 1)\n        self.W_q = torch.nn.Linear(d_q, d_k)\n\n        # Key/Value projection (from modality 2)\n        self.W_k = torch.nn.Linear(d_k, d_k)\n        self.W_v = torch.nn.Linear(d_k, d_v)\n\n        # Output projection\n        self.W_o = torch.nn.Linear(d_v, d_v)\n\n    def forward(self, query_feats, key_value_feats, mask=None):\n        \"\"\"\n        Args:\n            query_feats: Queries from modality 1\n                        (batch, len_q, d_q)\n            key_value_feats: Keys/values from modality 2\n                            (batch, len_k, d_k)\n            mask: Optional mask\n\n        Returns:\n            output: (batch, len_q, d_v)\n        \"\"\"\n        batch_size = query_feats.shape[0]\n\n        # Project\n        Q = self.W_q(query_feats)  # (batch, len_q, d_k)\n        K = self.W_k(key_value_feats)  # (batch, len_k, d_k)\n        V = self.W_v(key_value_feats)  # (batch, len_k, d_v)\n\n        # Reshape for multi-head\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n\n        # Attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        weights = torch.softmax(scores, dim=-1)\n        output = torch.matmul(weights, V)\n\n        # Concatenate heads\n        output = output.transpose(1, 2).contiguous()\n        output = output.view(batch_size, -1, self.num_heads * self.d_v)\n        output = self.W_o(output)\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example: Text attending to image\nclass ImageTextFusionLayer(torch.nn.Module):\n    def __init__(self, d_text=768, d_image=2048):\n        super().__init__()\n\n        self.text_to_image = CrossAttention(\n            d_q=d_text,\n            d_k=d_image,\n            d_v=d_image,\n            num_heads=8\n        )\n\n        self.image_to_text = CrossAttention(\n            d_q=d_image,\n            d_k=d_text,\n            d_v=d_text,\n            num_heads=8\n        )\n\n    def forward(self, text_feats, image_feats):\n        \"\"\"\n        Args:\n            text_feats: (batch, len_text, d_text)\n            image_feats: (batch, num_patches, d_image)\n\n        Returns:\n            text_out: Text enriched with image context\n            image_out: Image enriched with text context\n        \"\"\"\n        # Text queries image\n        text_out = self.text_to_image(text_feats, image_feats)\n\n        # Image queries text\n        image_out = self.image_to_text(image_feats, text_feats)\n\n        return text_out, image_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nfusion_layer = ImageTextFusionLayer()\n\ntext_feats = torch.randn(2, 77, 768)  # Text features\nimage_feats = torch.randn(2, 196, 2048)  # Image patches\n\ntext_enhanced, image_enhanced = fusion_layer(text_feats, image_feats)\n\nprint(f\"Text enhanced shape: {text_enhanced.shape}\")  # (2, 77, 2048)\nprint(f\"Image enhanced shape: {image_enhanced.shape}\")  # (2, 196, 768)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bidirectional Fusion\n\n**Why both directions matter:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nText â†’ Image only:\n  Text understands image\n  But image doesn't know what text is asking\n  One-way flow\n\nImage â†’ Text only:\n  Image influences text\n  But text doesn't guide image processing\n  Unbalanced\n\nBoth directions (bidirectional):\n  Text and image mutually influence each other\n  Balanced information flow\n  Better alignment\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architecture with bidirectional fusion:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BidirectionalFusion(torch.nn.Module):\n    \"\"\"Bidirectional attention between text and image\"\"\"\n\n    def __init__(self, d_text=768, d_image=2048, num_layers=6):\n        super().__init__()\n\n        self.num_layers = num_layers\n\n        # Projections to common space\n        self.text_project = torch.nn.Linear(d_text, 512)\n        self.image_\n\n-----\n\n> continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n        self.image_project = torch.nn.Linear(d_image, 512)\n\n        # Layers of bidirectional attention\n        self.layers = torch.nn.ModuleList([\n            BidirectionalAttentionLayer(512, 512)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, text_feats, image_feats):\n        \"\"\"\n        Args:\n            text_feats: (batch, len_text, d_text)\n            image_feats: (batch, num_patches, d_image)\n\n        Returns:\n            text_out: (batch, len_text, 512)\n            image_out: (batch, num_patches, 512)\n        \"\"\"\n        # Project to common space\n        text = self.text_project(text_feats)  # (batch, len_text, 512)\n        image = self.image_project(image_feats)  # (batch, num_patches, 512)\n\n        # Apply bidirectional fusion layers\n        for layer in self.layers:\n            text_new, image_new = layer(text, image)\n\n            # Residual connections\n            text = text + text_new\n            image = image + image_new\n\n        return text, image\n\nclass BidirectionalAttentionLayer(torch.nn.Module):\n    \"\"\"Single layer of bidirectional attention\"\"\"\n\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n\n        # Cross-attention: text queries image\n        self.text_attn = torch.nn.MultiheadAttention(\n            d_model, num_heads=8, batch_first=True\n        )\n\n        # Cross-attention: image queries text\n        self.image_attn = torch.nn.MultiheadAttention(\n            d_model, num_heads=8, batch_first=True\n        )\n\n        # Feed-forward networks\n        self.text_ff = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n        self.image_ff = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n        # Layer normalization\n        self.text_norm1 = torch.nn.LayerNorm(d_model)\n        self.text_norm2 = torch.nn.LayerNorm(d_model)\n        self.image_norm1 = torch.nn.LayerNorm(d_model)\n        self.image_norm2 = torch.nn.LayerNorm(d_model)\n\n    def forward(self, text, image):\n        \"\"\"\n        Args:\n            text: (batch, len_text, d_model)\n            image: (batch, num_patches, d_model)\n\n        Returns:\n            text_out: (batch, len_text, d_model)\n            image_out: (batch, num_patches, d_model)\n        \"\"\"\n        # Text attends to image\n        text_norm = self.text_norm1(text)\n        text_attn_out, _ = self.text_attn(\n            text_norm,  # Query\n            image, image,  # Key, Value\n            need_weights=False\n        )\n        text = text + text_attn_out\n\n        # Text feed-forward\n        text_norm = self.text_norm2(text)\n        text = text + self.text_ff(text_norm)\n\n        # Image attends to text\n        image_norm = self.image_norm1(image)\n        image_attn_out, _ = self.image_attn(\n            image_norm,  # Query\n            text, text,  # Key, Value\n            need_weights=False\n        )\n        image = image + image_attn_out\n\n        # Image feed-forward\n        image_norm = self.image_norm2(image)\n        image = image + self.image_ff(image_norm)\n\n        return text, image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage\nfusion = BidirectionalFusion(d_text=768, d_image=2048, num_layers=6)\n\ntext_feats = torch.randn(2, 77, 768)\nimage_feats = torch.randn(2, 196, 2048)\n\ntext_out, image_out = fusion(text_feats, image_feats)\n\nprint(f\"Text output shape: {text_out.shape}\")  # (2, 77, 512)\nprint(f\"Image output shape: {image_out.shape}\")  # (2, 196, 512)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5 Attention Visualization and Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Attention Weights\n\n**Text-to-text attention visualization:**\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_attention(attention_weights, tokens, layer_idx=0, head_idx=0):\n    \"\"\"\n    Visualize attention weights for a single layer and head\n\n    Args:\n        attention_weights: (num_layers, batch, num_heads, seq_len, seq_len)\n        tokens: List of token strings\n        layer_idx: Which layer to visualize\n        head_idx: Which head to visualize\n    \"\"\"\n    # Extract attention for specific layer and head\n    attn = attention_weights[layer_idx, 0, head_idx]  # (seq_len, seq_len)\n    attn = attn.detach().cpu().numpy()\n\n    # Create heatmap\n    fig, ax = plt.subplots(figsize=(10, 10))\n    im = ax.imshow(attn, cmap='viridis')\n\n    # Set labels\n    ax.set_xticks(range(len(tokens)))\n    ax.set_yticks(range(len(tokens)))\n    ax.set_xticklabels(tokens, rotation=45, ha='right')\n    ax.set_yticklabels(tokens)\n\n    # Add colorbar\n    cbar = plt.colorbar(im, ax=ax)\n    cbar.set_label('Attention weight')\n\n    ax.set_title(f'Attention weights (Layer {layer_idx}, Head {head_idx})')\n    ax.set_xlabel('Key (attended to)')\n    ax.set_ylabel('Query (attending from)')\n\n    plt.tight_layout()\n    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example usage\ntokens = ['The', 'cat', 'sat', 'on', 'the', 'mat']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# attention_weights would come from model\nfig = visualize_attention(attention_weights, tokens, layer_idx=0, head_idx=0)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n\n**Pattern interpretation:**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different attention patterns reveal model behavior:\n\nPattern 1: Diagonal (self-attention)\n  â•± (each token attends mostly to itself)\n  Interpretation: Position focuses on its own context\n  Meaning: Refines own representation\n\nPattern 2: Stripes (position-based)\n  â•‘ â•‘ â•‘ (same columns attended)\n  Interpretation: Multiple positions attend to same word\n  Meaning: Word is important reference point\n\nPattern 3: Distributed\n  â–‘ (uniform attention across sequence)\n  Interpretation: No clear focus\n  Meaning: Context comes from multiple sources\n\nPattern 4: Concentrated\n  â—¾ (attention on few positions)\n  Interpretation: Clear focus\n  Meaning: Strong alignment to specific positions\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Modal Attention Visualization\n\n```python\ndef visualize_cross_attention(text_to_image_attn, text_tokens,\n                              image_patches, head_idx=0):\n    \"\"\"\n    Visualize what image regions text tokens attend to\n\n    Args:\n        text_to_image_attn: (seq_len_text, num_patches)\n        text_tokens: List of text tokens\n        image_patches: Could be image itself or placeholder\n        head_idx: Which head (if multi-head)\n    \"\"\"\n    attn = text_to_image_attn.detach().cpu().numpy()\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n\n    # For each text token, show what it attends to in image\n    for i, token in enumerate(text_tokens[:6]):\n        ax = axes[i]\n\n        # Get attention for this token\n        token_attn = attn[i]  # (num_patches,)\n\n        # Reshape to image grid (assuming 14x14 patches for 196 total)\n        grid_size = int(np.sqrt(len(token_attn)))\n        attn_grid = token_attn.reshape(grid_size, grid_size)\n\n        # Show as heatmap overlaid on image\n        im = ax.imshow(attn_grid, cmap='hot')\n        ax.set_title(f'Attention from \"{token}\"')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.colorbar(im, ax=ax)\n\n    plt.tight_layout()\n    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example usage\ntext_to_image = model.get_text_to_image_attention()\nfig = visualize_cross_attention(text_to_image[0, 0],\n                                text_tokens,\n                                image)\nplt.show()\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.6 Common Attention Patterns and Their Meanings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 1: Positional Attention\n\n**What it looks like:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAttention matrix with clear bands:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\npos1   â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\npos2   â–‘â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\npos3   â–‘â–‘â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\npos4   â–‘â–‘â–‘â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n\n(Each position mainly attends to neighbors)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nModel learns local structure\nEffective for sequences with local dependencies\nExamples: Natural language, time series\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When it occurs:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEarly layers of language models\nLocal relationships matter (syntax)\nLimited context needed\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 2: Hub Attention\n\n**What it looks like:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nOne column has high values:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“\npos1   â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“\npos2   â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“\npos3   â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“\npos4   â–‘â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“\n\n(All positions attend to pos1)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n\"Hub\" token is very important\nAll other tokens depend on it\nExamples: [CLS] token in BERT, verb in sentence\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When it occurs:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLate layers (higher abstraction)\nGlobal information needed\nOne position summarizes all others\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 3: Diagonal + Off-Diagonal\n\n**What it looks like:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSelf-attention plus other patterns:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–“â–‘â–‘â–‘\npos1   â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–“â–‘â–‘\npos2   â–‘â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–“â–‘\npos3   â–‘â–‘â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–“\npos4   â–‘â–‘â–‘â–‘â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n\n(Diagonal + secondary pattern)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSelf-attention + specific relationships\nExample: Each word attends to self + its subject\nComplex linguistic structure\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 4: Random/Noise\n\n**What it looks like:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNo clear pattern:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘\npos1   â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘\npos2   â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“\npos3   â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘\npos4   â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“â–‘â–“\n\n(Uniform or random)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nHead not learning clear patterns\nCould indicate:\n  - Poor training\n  - Redundant head\n  - Learning different subspace\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.7 Debugging Attention Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 1: Attention Collapse\n\n**Symptoms:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAttention weights become nearly uniform\nExample: [0.25, 0.25, 0.25, 0.25] instead of [0.8, 0.1, 0.05, 0.05]\n\nEffects:\n  No clear focus\n  All positions equally weighted\n  Information not well integrated\n  Model performance poor\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Causes:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nâ‘  Temperature scaling issue\n   Softmax too smooth\n   All values similar\n\nâ‘¡ Poorly initialized queries/keys\n   Q and K nearly orthogonal\n   All dot products similar\n\nâ‘¢ Gradients not flowing\n   Attention not updating during training\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solutions:**\n```python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Debug: Check attention entropy\ndef check_attention_collapse(attention_weights):\n    \"\"\"\n    High entropy = collapse (uniform distribution)\n    Low entropy = focused attention\n    \"\"\"\n    # entropy = -sum(p * log(p))\n    entropy = -(attention_weights * torch.log(attention_weights + 1e-10)).sum(dim=-1)\n\n    print(f\"Attention entropy: {entropy.mean().item():.4f}\")\n    print(f\"Max entropy (uniform): {torch.log(torch.tensor(attention_weights.shape[-1])).item():.4f}\")\n\n    if entropy.mean() > 0.8 * max_entropy:\n        print(\"WARNING: Attention may be collapsing!\")\n        return True\n    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fix: Increase temperature (smooth more)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Or fix: Reduce temperature (sharpen more)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Or fix: Check initialization\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 2: Attention Not Converging\n\n**Symptoms:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAttention weights don't change during training\nAlways [0.333, 0.333, 0.333] for 3 positions\n\nEffects:\n  Model can't learn what to focus on\n  No improvement over training\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Causes:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nâ‘  Learning rate too low\n   Gradients too tiny\n   No meaningful updates\n\nâ‘¡ Attention parameters frozen\n   Not being updated\n\nâ‘¢ No gradient signal\n   Previous layers not helping\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Debugging code:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def debug_attention_convergence(model, initial_weights, final_weights):\n    \"\"\"Check if attention changed\"\"\"\n\n    change = (final_weights - initial_weights).abs().mean()\n\n    print(f\"Attention weight change: {change.item():.6f}\")\n\n    if change < 1e-6:\n        print(\"WARNING: Attention not converging!\")\n\n        # Check gradients\n        for name, param in model.named_parameters():\n            if 'attention' in name:\n                if param.grad is not None:\n                    grad_norm = param.grad.norm()\n                    print(f\"  {name}: grad_norm = {grad_norm.item():.6f}\")\n                else:\n                    print(f\"  {name}: NO GRADIENT\")\n\n        return False\n    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3: Misaligned Cross-Attention\n\n**Symptoms:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCross-attention between modalities doesn't make sense\nExample: Word \"red\" attends to random image patches, not red regions\n\nEffects:\n  Poor multimodal alignment\n  Model can't understand relationship between modalities\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Debugging:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_cross_attention_alignment(text_tokens, image_labels,\n                                     cross_attn_weights):\n    \"\"\"\n    Check if cross-attention makes semantic sense\n\n    Args:\n        text_tokens: ['red', 'cat', 'on', 'mat']\n        image_labels: ['red_region', 'cat_region', 'ground', 'background']\n        cross_attn_weights: (len_text, num_patches)\n    \"\"\"\n\n    for i, token in enumerate(text_tokens):\n        attn = cross_attn_weights[i]  # Attention for this token\n        top_indices = torch.topk(attn, k=3).indices  # Top 3 attended regions\n\n        attended_regions = [image_labels[idx] for idx in top_indices]\n\n        print(f\"Token '{token}' attends to: {attended_regions}\")\n\n        # Simple heuristic: check if token and attended regions match\n        if token in ' '.join(attended_regions).lower():\n            print(f\"  âœ“ Makes sense!\")\n        else:\n            print(f\"  âœ— Misaligned!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.8 Attention Efficiency Optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge: Quadratic Complexity\n\n**Problem:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAttention complexity: O(nÂ²) where n = sequence length\n\nExamples:\n  n = 100: 10,000 operations\n  n = 1000: 1,000,000 operations\n  n = 10,000: 100,000,000 operations\n\nFor images with 196 patches: Manageable\nFor long documents with 4096 tokens: Problematic\nFor videos with 1000+ frames: Very difficult\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 1: Sparse Attention\n\n**Idea: Don't attend to all positions**\n\n```python\nclass SparseAttention(torch.nn.Module):\n    \"\"\"Attention with sparse connections\"\"\"\n\n    def __init__(self, window_size=32):\n        super().__init__()\n        self.window_size = window_size\n\n    def forward(self, Q, K, V):\n        \"\"\"\n        Only attend to nearby positions\n\n        Each position attends to:\n          - Itself\n          - window_size//2 positions before\n          - window_size//2 positions after\n        \"\"\"\n        seq_len = Q.shape[1]\n\n        # Create sparse mask\n        mask = torch.ones(seq_len, seq_len, device=Q.device)\n\n        for i in range(seq_len):\n            # Mask everything outside window\n            start = max(0, i - self.window_size // 2)\n            end = min(seq_len, i + self.window_size // 2)\n            mask[i, :start] = 0\n            mask[i, end:] = 0\n\n        # Standard attention with mask\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        attention_weights = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complexity: O(n * window_size) instead of O(nÂ²)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 2: Linear Attention\n\n**Idea: Approximate softmax with kernel methods**\n\n```python\nclass LinearAttention(torch.nn.Module):\n    \"\"\"Linear complexity attention\"\"\"\n\n    def forward(self, Q, K, V):\n        \"\"\"\n        Standard attention:\n          Attention(Q,K,V) = softmax(QK^T) @ V\n          Complexity: O(nÂ²)\n\n        Linear attention:\n          Approximate softmax with kernel\n          Ï†(QK^T) can be computed differently\n          Complexity: O(n)\n        \"\"\"\n\n        # Apply kernel function (e.g., elu + 1)\n        Q_proj = torch.nn.functional.elu(Q) + 1  # Ensure positivity\n        K_proj = torch.nn.functional.elu(K) + 1\n\n        # Rewrite attention:\n        # standard: softmax(QK^T) @ V\n        # linear: Ï†(Q) @ (Ï†(K)^T @ V) / (Ï†(Q) @ Ï†(K)^T @ 1)\n\n        numerator = torch.einsum('bne,bnd->bnd', K_proj, V)  # (batch, seq, d)\n        numerator = torch.einsum('bnd,bne->bnd', Q_proj, numerator)\n\n        denominator = torch.einsum('bne,bn->bne', Q_proj,\n                                   K_proj.sum(dim=1))  # (batch, seq, 1)\n        denominator = denominator + 1e-6  # Avoid division by zero\n\n        output = numerator / denominator\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complexity: O(n * dÂ²) where d is embedding dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For n >> d: Linear in n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 3: Flash Attention\n\n**Idea: GPU-friendly attention computation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStandard attention:\n  1. Compute QK^T: O(nÂ²) memory\n  2. Apply softmax\n  3. Multiply by V\n\nFlash Attention:\n  1. Compute attention in blocks\n  2. Fuse operations (CUDA)\n  3. Reduce memory and computation\n\nResult:\n  2-4Ã— faster\n  Less memory\n  Same result\n\nImplementation: Use existing libraries\n  torch.nn.functional.scaled_dot_product_attention  (PyTorch 2.0+)\n  flash-attn package\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Optimization Example\n\n```python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Before: Standard attention\nattention = torch.nn.MultiheadAttention(d_model=512, num_heads=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory: O(batch * seq_lenÂ²)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speed: Slower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# After: Optimized attention\nclass OptimizedAttention(torch.nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n\n        # Option 1: Use Flash Attention (PyTorch 2.0+)\n        self.use_flash = True\n\n        # Option 2: Use sparse attention for long sequences\n        if seq_len > 1000:\n            self.attention = SparseAttention(window_size=64)\n        else:\n            self.attention = torch.nn.MultiheadAttention(d_model, num_heads)\n\n    def forward(self, Q, K, V):\n        if self.use_flash:\n            return torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        else:\n            return self.attention(Q, K, V)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Attention solves \"what to look at\" problem** efficiently\n- **Scaled dot-product is the foundation** - normalize by âˆšd_k\n- **Multi-head attention learns diverse patterns** in parallel\n- **Cross-attention connects modalities** bidirectionally\n- **Visualization reveals model behavior** - debug with patterns\n- **Efficiency matters** - use sparse, linear, or flash attention for long sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n**â­ Beginner:**\n1. Implement scaled dot-product attention by hand\n2. Visualize attention weights from pre-trained model\n3. Understand what each attention head specializes in\n\n**â­â­ Intermediate:**\n4. Build cross-attention fusion layer\n5. Implement bidirectional attention\n6. Debug attention collapse in custom model\n\n**â­â­â­ Advanced:**\n7. Implement sparse attention\n8. Optimize attention with flash mechanisms\n9. Analyze cross-modal alignment quality\n\n---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}