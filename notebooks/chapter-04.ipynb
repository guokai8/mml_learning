{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Feature Alignment and Bridging Modalities\n\n**Interactive Jupyter Notebook Version**\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Feature Alignment and Bridging Modalities\n\n---\n\n**Previous**: [Chapter 3: Feature Representation for Each Modality](chapter-03.md) | **Next**: [Chapter 5: Fusion Strategies](chapter-05.md) | **Home**: [Table of Contents](index.md)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Feature Alignment and Bridging Modalities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand why alignment is necessary\n- Implement shared embedding spaces\n- Use cross-attention for fine-grained alignment\n- Handle bidirectional alignment\n- Solve alignment in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 The Alignment Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Alignment Matters\n\n**The Core Challenge:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage features: 2048-dimensional vector (from ResNet)\nText features: 768-dimensional vector (from BERT)\n\nQuestion: How similar are they?\n\nProblem:\n  ✗ Different dimensions (can't directly compare)\n  ✗ Different scales (0-1 for text, -infinity to infinity for images)\n  ✗ Different semantics (what does dimension 500 mean in each?)\n  ✗ No natural similarity metric\n\nWe need: ALIGNMENT\nGoal: Make image and text features \"understand\" each other\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Real-world consequence:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nApplication: Image-text search\n  User searches: \"red cat\"\n  System has: 10 million images + descriptions\n\nWithout alignment:\n  Can't compare image and text vectors\n  Search impossible\n\nWith alignment:\n  Image vectors and text vectors in same space\n  Similarity computed easily\n  Search works!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Levels of Alignment\n\n**Level 1: Coarse-grained (Document-level)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEntire image ↔ Entire text description\n\nExample:\n  Image: [Full photo of cat on chair]\n  Text: \"A tabby cat relaxing on a wooden chair\"\n\n  Alignment: Image matches entire text\n\nUse cases:\n  - Image-text retrieval\n  - Image classification with descriptions\n  - Document understanding (image + caption)\n\nChallenge: Image might have multiple objects\n           Text mentions most important ones\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Level 2: Fine-grained (Region-level)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage regions ↔ Text phrases\n\nExample:\n  Image regions:\n    Region 1: [Cat's head area]\n    Region 2: [Chair seat area]\n    Region 3: [Background]\n\n  Text phrases:\n    \"tabby cat\" ↔ Region 1\n    \"wooden chair\" ↔ Region 2\n    \"cozy room\" ↔ Region 3\n\nUse cases:\n  - Visual question answering (where are things?)\n  - Dense image captioning\n  - Object detection with descriptions\n  - Grounding language in images\n\nChallenge: Multiple valid region boundaries\n           Phrases don't perfectly correspond to regions\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Level 3: Very Fine-grained (Pixel/Token-level)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage pixels ↔ Text tokens\n\nExample:\n  Video frame:\n    [Pixels 100-200]: Red fur\n    [Pixels 500-600]: Cat's eye\n    [Pixels 800-900]: Chair texture\n\n  Text tokens:\n    \"red\" ↔ Red fur pixels\n    \"cat\" ↔ Cat structure pixels\n    \"chair\" ↔ Chair pixels\n\nUse cases:\n  - Semantic segmentation with text\n  - Dense video captioning with timestamps\n  - Pixel-level understanding with descriptions\n\nChallenge: Extremely fine-grained\n           Requires pixel-level annotations\n           Computationally expensive\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Alignment is Hard\n\n**Reason 1: One-to-many mappings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSingle image can have many valid descriptions:\n\nImage: [Cat on bed]\n\nValid descriptions:\n  ① \"A cat is sleeping on a bed\"\n  ② \"A cat on a bed\"\n  ③ \"Feline on furniture\"\n  ④ \"A cozy cat\"\n  ⑤ \"Kitty resting\"\n\nAll correct!\nNo single \"ground truth\" alignment\n\nChallenge: How to learn from multiple valid targets?\nSolution: Use soft targets or ranking-based losses\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reason 2: Implicit pairing in training data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWeb data structure:\n\n[Article with title: \"Beautiful pets\"]\n│\n├─ [Image 1]\n├─ [Image 2]\n├─ [Large paragraph mentioning pets]\n├─ [Image 3]\n└─ [Image 4]\n\nChallenge:\n  Which image goes with which sentence?\n  Are all images described equally?\n\nSolutions:\n  - Assume images near text match it\n  - Learn implicit pairings\n  - Use weak supervision signals\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reason 3: Semantic gaps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage and text express different aspects:\n\nImage: \"Tabby cat, orange color, on blue chair, sunny room\"\nText: \"A cat resting\"\n\nText is abstract summary\nImage is concrete visual\n\nHow to align?\n  Need to map concrete visual features\n  to abstract semantic concepts\n\nThis requires:\n  ① Understanding visual features\n  ② Understanding text semantics\n  ③ Bridging the gap\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reason 4: Missing or corrupted data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nData quality issues:\n\nSituation 1: Image and text don't match\n  Image: [Car]\n  Text: \"Beautiful sunset\"\n\n  Alignment should recognize mismatch\n\nSituation 2: Image is corrupted\n  Image: [Blank/noise]\n  Text: \"A dog running\"\n\n  Should still align based on text\n\nSituation 3: Text is poorly written\n  Image: [Cat photo]\n  Text: \"teh kat iz vry smrt\"\n\n  Should understand despite bad spelling\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Shared Embedding Space - The Standard Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Core Concept\n\n**Idea:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nProject both modalities to common space\nwhere similarity can be computed\n\nImage (2048D) --┐\n               ├─→ Shared Space (256D)\nText (768D) ───┘\n\nNow both in same space!\nCan compute cosine similarity directly\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation\n\n**Step 1: Learn projection matrices**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nFor images:\n  W_img ∈ ℝ^(2048 × 256)\n  img_proj = W_img @ img_features\n\nFor text:\n  W_txt ∈ ℝ^(768 × 256)\n  txt_proj = W_txt @ txt_features\n\nBoth outputs: 256-dimensional vectors\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Normalize in shared space**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# L2 normalize to unit length\nimg_proj = img_proj / ||img_proj||\ntxt_proj = txt_proj / ||txt_proj||\n\nResult:\n  Both vectors have magnitude 1\n  Can use cosine similarity = dot product\n  Similarity ∈ [-1, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n\n**Step 3: Compute similarity**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "similarity = img_proj · txt_proj\n\n= Σ(img_proj_i × txt_proj_i)\n\nResult interpretation:\n  > 0.8:   Very similar (matched pair)\n  0.5-0.8: Similar\n  0.3-0.5: Somewhat related\n  < 0.3:   Different (unrelated pair)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning the Projections\n\n**Training objective:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nGoal: Maximize similarity of matched pairs\n      Minimize similarity of unmatched pairs\n\nDataset: Pairs (image_i, text_i) where i means matched\n\nLoss function (InfoNCE / Contrastive):\n\nL = -log[ exp(sim(img_i, txt_i) / τ) /\n          (exp(sim(img_i, txt_i) / τ) + Σ_j≠i exp(sim(img_i, txt_j) / τ)) ]\n\nIntuition:\n  Numerator: Similarity of correct pair (should be high)\n  Denominator: All pairs including incorrect ones\n  Loss: Make correct pair stand out from all others\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Batching strategy:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nBatch of 32 samples:\n\n[img_1] ────────┐\n[img_2] ────────┼─ All project to shared space\n[img_3] ────────┤\n...             │\n[img_32] ───────┘\n\n[txt_1] ────────┐\n[txt_2] ────────┼─ All project to shared space\n[txt_3] ────────┤\n...             │\n[txt_32] ───────┘\n\nSimilarity matrix (32×32):\n  sim(img_1, txt_1) = 0.95  ← Matched\n  sim(img_1, txt_2) = 0.2   ← Unmatched\n  sim(img_2, txt_2) = 0.94  ← Matched\n  ...\n\nLoss: Make diagonal elements high\n      Make off-diagonal elements low\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dimension selection:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nChoice of shared space dimension:\n\nSmall (64D):\n  ✓ Fast computation\n  ✓ Less memory\n  ✗ Information loss\n  ✗ Can't capture fine details\n\nMedium (256D):\n  ✓ Good balance\n  ✓ Standard choice\n  ✓ Preserves information\n\nLarge (1024D):\n  ✓ Maximum information\n  ✗ Slow computation\n  ✗ More memory\n  ✗ Risk of overfitting\n\nTypical sweet spot: 256-512D\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Example\n\n**Image-Text Retrieval System:**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ImageTextAligner(nn.Module):\n    def __init__(self, img_dim=2048, txt_dim=768, shared_dim=256):\n        super().__init__()\n\n        # Projection layers\n        self.img_projection = nn.Linear(img_dim, shared_dim)\n        self.txt_projection = nn.Linear(txt_dim, shared_dim)\n\n    def forward(self, img_features, txt_features):\n        # Project to shared space\n        img_proj = self.img_projection(img_features)  # (batch, 256)\n        txt_proj = self.txt_projection(txt_features)  # (batch, 256)\n\n        # L2 normalize\n        img_proj = F.normalize(img_proj, p=2, dim=1)\n        txt_proj = F.normalize(txt_proj, p=2, dim=1)\n\n        return img_proj, txt_proj\n\n    def compute_similarity(self, img_proj, txt_proj):\n        # Cosine similarity = dot product of normalized vectors\n        similarity = torch.mm(img_proj, txt_proj.t())  # (batch, batch)\n        return similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training\nmodel = ImageTextAligner()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nfor batch in data_loader:\n    images, texts = batch\n\n    img_features = image_encoder(images)  # (batch, 2048)\n    txt_features = text_encoder(texts)    # (batch, 768)\n\n    # Align\n    img_proj, txt_proj = model(img_features, txt_features)\n\n    # Compute similarities\n    similarities = model.compute_similarity(img_proj, txt_proj)\n\n    # Contrastive loss\n    batch_size = img_proj.shape[0]\n    labels = torch.arange(batch_size).to(device)\n\n    loss_img = F.cross_entropy(similarities / temperature, labels)\n    loss_txt = F.cross_entropy(similarities.t() / temperature, labels)\n    loss = (loss_img + loss_txt) / 2\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Cross-Attention for Fine-Grained Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Motivation\n\n**Problem with shared space:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nShared embedding gives global similarity\nBut doesn't tell us WHAT matched\n\nImage representation: Single 256D vector\n  Represents entire image\n  Loses spatial structure\n\nText representation: Single 256D vector\n  Represents entire sentence\n  Loses word-level information\n\nResult: Good for retrieval\n        Bad for understanding fine-grained relationships\n\nSolution: Cross-attention!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Attention Mechanism\n\n**Core idea:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLet each part of one modality\n\"look at\" relevant parts of another modality\n\nText looks at image:\n  Word \"red\" ← attends to → Red pixels in image\n  Word \"cat\" ← attends to → Cat-shaped pixels in image\n\nImage looks at text:\n  Cat region ← attends to → \"cat\" and \"tabby\" words\n  Background ← attends to → \"room\" word\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Mathematical formulation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCross-attention (Text queries, Image keys/values):\n\nQuery = Text embeddings (sequence length = # words)\nKey = Image patches from CNN (# patches = 196 for ViT)\nValue = Image patches\n\nAttention = softmax(Query @ Key^T / √d_k) @ Value\n\nResult:\n  Each word gets weighted combination of image patches\n  Weights reflect relevance (attention)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Concrete example:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nText: \"The [red] cat [sits]\"\n       Word 1  Word 2  Word 3\n\nImage: Divided into 9 patches (3×3 grid)\n\n┌─────┬─────┬─────┐\n│ Sky │ Sky │ Sky │\n├─────┼─────┼─────┤\n│ Cat │ Cat │Chair│\n├─────┼─────┼─────┤\n│Grass│ Cat │Chair│\n└─────┴─────┴─────┘\n\nCross-attention: Word \"red\" attends to image patches\n\nAttention scores:\n  Sky: 0.1\n  Sky: 0.1\n  Sky: 0.1\n  Cat(red): 0.5  ← High! Red cat\n  Cat: 0.3\n  Chair: 0.0\n  Grass: 0.0\n  Cat: 0.3\n  Chair: 0.0\n\nAttention output: Weighted combination of patches\n  0.5 × Cat_patch_red + 0.3 × Cat_patch + 0.3 × Cat_patch\n  ≈ Feature vector emphasizing red cat region\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-head Cross-Attention\n\n**Why multiple heads?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nDifferent heads can attend to different aspects\n\nHead 1: Color-sensitive\n  Attends to: Patches with matching colors\n\nHead 2: Shape-sensitive\n  Attends to: Patches with matching shapes\n\nHead 3: Texture-sensitive\n  Attends to: Patches with matching textures\n\nAll heads run in parallel\nResults concatenated\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Implementation:**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pseudo-code for single attention head\n\nQuery = W_q @ text_embedding      # (seq_len, d_k)\nKey = W_k @ image_patches         # (num_patches, d_k)\nValue = W_v @ image_patches       # (num_patches, d_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compute attention weights\nscores = Query @ Key^T            # (seq_len, num_patches)\nweights = softmax(scores / √d_k)  # (seq_len, num_patches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Apply to values\noutput = weights @ Value          # (seq_len, d_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For multiple heads: repeat with different W_q, W_k, W_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Then concatenate outputs\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bidirectional Alignment\n\n**Problem:** Cross-attention only goes one way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nText attends to image: ✓ Good\n  Words understand image\n\nImage attends to text: ✗ Missing\n  Image regions don't understand text\n  Asymmetric!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution: Bidirectional attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStep 1: Text cross-attends to image\n  Query = Text\n  Key/Value = Image\n  Result: Text-aware-of-image\n\nStep 2: Image cross-attends to text\n  Query = Image patches\n  Key/Value = Text\n  Result: Image-aware-of-text\n\nStep 3: Combine both representations\n  Use refined representations for tasks\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architecture with bidirectional alignment:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nInitial representations:\n  Image patches: 196 vectors (from ViT)\n  Text tokens: 10 vectors (from BERT)\n\nLayer 1:\n  ├─ Text cross-attends to Image\n  │  └─ Output: Refined text (10 vectors)\n  │\n  └─ Image self-attends within itself\n     └─ Output: Refined image (196 vectors)\n\nLayer 2:\n  ├─ Image cross-attends to Text\n  │  └─ Output: Refined image (196 vectors)\n  │\n  └─ Text self-attends within itself\n     └─ Output: Refined text (10 vectors)\n\nLayer 3-6: Repeat above\n\nResult:\n  Both modalities refined with knowledge of other\n  Bidirectional influence\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example - VQA (Visual Question Answering):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage: [Photo of cat on chair]\nQuestion: \"What's on the chair?\"\n\nProcessing with bidirectional alignment:\n\n① Initial encoding:\n   Image patches: 196 ViT features\n   Question: \"What's\", \"on\", \"the\", \"chair\", \"?\" (5 tokens)\n\n② Text understands image context:\n   \"chair\" attends to chair-region patches\n   \"on\" understands preposition in spatial context\n\n   Result: Question tokens now image-aware\n\n③ Image understands question:\n   Chair region attends to \"chair\" token\n   Surrounding region attends to \"on\" (preposition)\n\n   Result: Image patches now question-aware\n\n④ Predict answer:\n   Question tokens generate: \"A cat\"\n\nBenefits:\n  - Question focuses on relevant image parts\n  - Image highlights relevant content for question\n  - Mutual refinement through layers\n  - Better understanding than independent processing\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Practical Alignment Challenges and Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 1: Handling Multiple Valid Alignments\n\n**Problem:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage: [Multi-object scene: cat, dog, table]\nText options:\n  ① \"Pets on table\"\n  ② \"Table with animals\"\n  ③ \"Room with furniture\"\n  ④ \"A table with a cat and dog\"\n\nAll valid descriptions!\nWhich should model learn?\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solutions:**\n\n**Solution 1: All positives training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTreat all valid descriptions as positive examples\n\nLoss = -log[exp(sim(img, txt1)) + exp(sim(img, txt2)) + exp(sim(img, txt3))]\n       / [exp(sim(img, txt1)) + exp(sim(img, txt2)) + exp(sim(img, txt3)) +\n          exp(sim(img, neg1)) + exp(sim(img, neg2)) + ...]\n\nCode:\n  positives = [text1, text2, text3]  # All valid\n  negatives = [text4, text5, ...]    # Invalid\n\n  for pos in positives:\n    loss += InfoNCE_loss(img, pos, negatives)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution 2: Soft targets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAssign soft probability to each description\n\nSimilarity scores: [0.9, 0.85, 0.7, 0.3, 0.1]\nProbabilities: [0.4, 0.4, 0.15, 0.04, 0.01]\n\nDistribution rather than hard binary labels\nModel learns to match range of good descriptions\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution 3: Ranking-based loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nInstead of absolute similarity,\noptimize relative ranking\n\nConstraint: sim(img, good_text) > sim(img, bad_text) + margin\n\nLoss = max(0, margin + sim(img, bad) - sim(img, good))\n\nModel ensures good descriptions rank higher\nNot concerned with absolute values\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2: Incomplete or Corrupted Data\n\n**Problem 1: Image-text mismatch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWeb data contains misaligned pairs:\n\nWebsite page:\n[Image1: Beautiful sunset]\n[Article about technology and computers]\n[Image2: Laptop]\n\nProblem:\n  Image1 doesn't match article\n  Article matches Image2 only\n\nSolution: Robustness to noise\n  - Training with some wrong pairs is okay\n  - Model learns that MOST pairs are correct\n  - Wrong pairs become negatives\n  - Loss still works\n\n  Empirically: Works even with 20-30% misaligned data\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Problem 2: Low-quality images or text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImages:\n  - Blurry photos\n  - Extreme lighting\n  - Occlusions\n  - Irrelevant backgrounds\n\nText:\n  - Spelling errors\n  - Grammar mistakes\n  - Abbreviations\n  - Emotional/subjective language\n\nSolution: Robust feature extraction\n  - Use pre-trained encoders (already robust)\n  - Encoders trained on diverse data\n  - Can handle degraded inputs\n  - Alignment robust if base features good\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Problem 3: Context-dependent meaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nText: \"A record player\"\nImage: [Phonograph]\n\nChallenge:\n  \"Record\" = LP vinyl record vs historical record\n  \"Player\" = music player vs sports player\n  Multiple interpretations!\n\nSolution: Context through attention\n  - Image patches clarify which \"record\"\n  - Text confirms image interpretation\n  - Cross-attention resolves ambiguity\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3: Scaling to Large Datasets\n\n**Problem:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nComputing similarity matrix for large batch:\n\nBatch size: 10,000 images and 10,000 texts\nSimilarity matrix size: 10,000 × 10,000 = 100M elements\n\nComputation:\n  ① Forward pass: 100M multiplies\n  ② Softmax: 100M exponentials\n  ③ Backward pass: 100M gradients\n\nResult: Extremely slow!\nGPU memory: 100M × float32 = 400MB just for similarities\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solutions:**\n\n**Solution 1: Smaller batches**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nBatch size: 256 instead of 10,000\nSimilarity matrix: 256 × 256 = 65K elements\n\nTrade-off:\n  ✓ Faster training\n  ✓ Less memory\n  ✗ Noisier gradients (fewer negatives)\n  ✗ More iterations needed\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution 2: Distributed training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSplit batch across multiple GPUs\n\nGPU 1: 2500 images and texts\nGPU 2: 2500 images and texts\nGPU 3: 2500 images and texts\nGPU 4: 2500 images and texts\n\nGradient computation happens locally\nAll-reduce aggregates gradients\n\nEnables:\n  - Larger effective batch size\n  - Better negatives for learning\n  - Faster training overall\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solution 3: Hard negative mining**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nInstead of all negatives,\nselect hard negatives (easily confused)\n\nFull set: 10,000 possible negatives\nSample: 32 hard negatives (ones model struggles with)\n\nBenefits:\n  - Reduces computation\n  - More efficient learning (focus on hard cases)\n  - Still effective despite smaller negative set\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5 Evaluating Alignment Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metrics for Alignment\n\n**1. Retrieval Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSetup: Given 1000 images and 1000 texts (properly paired)\nTask: For each image, rank texts by similarity\n\nMetrics:\n\nRecall@K:\n  Did correct text appear in top K?\n\n  Example (K=1):\n    For each image, check if correct text in top 1\n    Count successes / total images\n\n  Recall@1: 75% (750/1000 correct)\n  Recall@5: 95% (950/1000 correct)\n\n  Interpretation:\n    Recall@1 = Exact match retrieval rate\n    Recall@5 = Reasonable match rate\n\nMean Reciprocal Rank (MRR):\n  Average rank of correct match\n\n  Example:\n    Image 1: Correct text at rank 3 → 1/3\n    Image 2: Correct text at rank 1 → 1/1\n    Image 3: Correct text at rank 10 → 1/10\n    MRR = (1/3 + 1/1 + 1/10) / 3 ≈ 0.44\n\nNormalized DCGA (NDCG):\n  Accounts for relevance scores\n  Perfect ranking = 1.0\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Correlation Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nIdea: Good alignment means\n      similar images/texts have high correlation\n\nSpearman Correlation:\n  ① Rank pairs by human similarity judgment\n  ② Rank same pairs by model similarity\n  ③ Compute rank correlation\n\n  Perfect: Correlation = 1.0\n  Random: Correlation ≈ 0.0\n\nPearson Correlation:\n  Linear correlation between human and model scores\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Classification Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nBinary classification: Correct or incorrect pairing?\n\nDataset: 1000 correct pairs + 1000 incorrect pairs\n\nMetrics:\n  Accuracy: How many correct predictions?\n  Precision: Of positive predictions, how many correct?\n  Recall: Of correct pairs, how many identified?\n  F1: Harmonic mean of precision and recall\n\nExample results:\n  Accuracy: 95% (1900/2000 correct)\n  Precision: 96% (970/1010 predicted positive)\n  Recall: 97% (970/1000 actually positive)\n  F1: 0.965\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Evaluation Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score, ndcg_score\nimport numpy as np\n\ndef evaluate_alignment(img_features, txt_features, labels):\n    \"\"\"\n    img_features: (N, 256) aligned image embeddings\n    txt_features: (N, 256) aligned text embeddings\n    labels: (N,) ground truth labels (0=incorrect, 1=correct)\n    \"\"\"\n\n    # Compute similarities\n    similarities = np.dot(img_features, txt_features.T)\n\n    # Recall@K\n    recall_at_1 = compute_recall_at_k(similarities, labels, k=1)\n    recall_at_5 = compute_recall_at_k(similarities, labels, k=5)\n\n    # Classification metrics\n    binary_predictions = (similarities > threshold).astype(int)\n    accuracy = np.mean(binary_predictions == labels)\n    precision = precision_score(labels, binary_predictions)\n    recall = recall_score(labels, binary_predictions)\n    f1 = f1_score(labels, binary_predictions)\n\n    print(f\"Recall@1: {recall_at_1:.3f}\")\n    print(f\"Recall@5: {recall_at_5:.3f}\")\n    print(f\"Accuracy: {accuracy:.3f}\")\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall: {recall:.3f}\")\n    print(f\"F1: {f1:.3f}\")\n\n    return {\n        'recall_at_1': recall_at_1,\n        'recall_at_5': recall_at_5,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\ndef compute_recall_at_k(similarities, labels, k):\n    \"\"\"Compute Recall@K metric\"\"\"\n    n = similarities.shape[0]\n    recall_sum = 0\n\n    for i in range(n):\n        # Get top-K most similar texts for this image\n        top_k_idx = np.argsort(similarities[i])[-k:]\n\n        # Check if any of top-K are correct (label=1)\n        if np.any(labels[top_k_idx] == 1):\n            recall_sum += 1\n\n    return recall_sum / n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Alignment is essential** for connecting different modalities\n- **Shared embedding space** is standard, scalable solution\n- **Cross-attention** enables fine-grained alignment\n- **Bidirectional fusion** gives mutual understanding\n- **Practical challenges** require careful handling\n- **Multiple evaluation metrics** give comprehensive picture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n**⭐ Beginner:**\n1. Implement cosine similarity between image and text features\n2. Visualize shared embedding space using t-SNE\n3. Compute recall@K for sample retrieval task\n\n**⭐⭐ Intermediate:**\n4. Build shared embedding projection layers\n5. Implement contrastive loss training\n6. Evaluate alignment with multiple metrics\n\n**⭐⭐⭐ Advanced:**\n7. Implement bidirectional cross-attention from scratch\n8. Build hard negative mining strategy\n9. Compare different loss functions (InfoNCE, triplet, etc.)\n\n---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}