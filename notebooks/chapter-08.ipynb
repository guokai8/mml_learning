{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup and Imports\n\nRun this cell first to install required packages and import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision transformers numpy matplotlib seaborn pandas pillow requests\n",
        "!pip install clip-by-openai sentence-transformers datasets\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 8: Transformer Architecture\n\n**Interactive Jupyter Notebook Version**\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 8: Transformer Architecture\n\n---\n\n**Previous**: [Chapter 7: Contrastive Learning](chapter-07.md) | **Next**: [Chapter 9: Generative Models for Multimodal Data](chapter-09.md) | **Home**: [Table of Contents](index.md)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 8: Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand transformer fundamentals\n- Explain self-attention mechanism\n- Implement multi-head attention\n- Understand encoder-decoder architecture\n- Apply transformers to multimodal tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.1 The Problem Transformers Solve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limitations of Sequential Models (RNNs)\n\n**RNN limitations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nProcessing sequence: w1, w2, w3, w4, w5\n\nRNN forward pass (sequential):\n  h0 = initialization\n  h1 = RNN(w1, h0)  ‚Üê Must wait for h0\n  h2 = RNN(w2, h1)  ‚Üê Must wait for h1\n  h3 = RNN(w3, h2)  ‚Üê Must wait for h2\n  h4 = RNN(w4, h3)  ‚Üê Must wait for h3\n  h5 = RNN(w5, h4)  ‚Üê Must wait for h4\n\nProblems:\n‚ë† Cannot parallelize\n   Each step depends on previous\n   Sequential bottleneck\n\n‚ë° Gradient flow issues\n   Backprop through 5 steps:\n   gradient = ‚àÇh5/‚àÇh4 √ó ‚àÇh4/‚àÇh3 √ó ‚àÇh3/‚àÇh2 √ó ‚àÇh2/‚àÇh1 √ó ‚àÇh1/‚àÇh0\n\n   Each factor typically < 1:\n   0.9^5 = 0.59  (50% loss)\n   0.9^100 ‚âà 0   (vanishing gradient)\n\n‚ë¢ Limited context window\n   Position t can see positions [0, t-1]\n   Cannot look ahead (in some RNNs)\n   Information degrades over long sequences\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CNN Limitations for Sequences\n\n**CNN characteristics:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLocal receptive field:\n  3√ó3 kernel sees 9 neighbors\n  To see position distance 10:\n  Need log(10) ‚âà 4 layers\n\n  For long sequences:\n  Need many layers\n  Deep networks = hard to train\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Solution\n\n**Key insight:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWhy wait for sequential dependencies?\n\nWhat if every position could see every other position simultaneously?\n\nQuery: Position i\nKey/Value: All positions (including i)\n\nAttention: Position i attends to all positions\nResult: Global context immediately available!\n\nBenefit:\n‚ë† Fully parallelizable\n   All positions process simultaneously\n   Each GPU core handles one position\n\n‚ë° No sequential bottleneck\n\n‚ë¢ Long-range dependencies captured immediately\n   Position 0 can \"see\" position 100 in layer 1\n   No need for deep networks\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.2 Self-Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intuition\n\n**Example - Machine translation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEnglish: \"The animal didn't cross the street because it was too tired\"\n\nAmbiguity: What does \"it\" refer to?\n  Option A: \"animal\" (correct)\n  Option B: \"street\" (incorrect)\n\nHow humans understand:\n  Focus on \"it\" (pronoun)\n  Look back at possible referents: \"animal\", \"street\"\n  \"Animal\" makes more sense in context\n  ‚Üí \"it\" = \"animal\"\n\nSelf-attention for \"it\":\n  Query: \"it\"\n  Key/Value options: [\"The\", \"animal\", \"didn't\", ..., \"tired\"]\n  Attention: Which words help interpret \"it\"?\n    \"animal\": High attention (antecedent)\n    \"cross\": Medium attention (related event)\n    \"The\": Low attention (not informative)\n  Result: \"it\" representation influenced mainly by \"animal\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Definition\n\n**Components:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nQuery (Q): What am I asking about?\nKey (K): What information is available?\nValue (V): What to retrieve?\n\nAnalogy - Database:\n  Query: Search terms (\"animal\")\n  Keys: Database field names and values\n  Values: Data to retrieve\n\nExample:\n  Query: \"hungry\"\n  Key matches: \"starving\" (high similarity), \"tired\" (medium)\n  Values: Corresponding word embeddings\n  Result: Weighted sum of values based on key similarity to query\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Formula:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAttention(Q, K, V) = softmax(Q @ K^T / ‚àöd_k) @ V\n\nBreakdown:\n\nQ @ K^T:\n  Query dot Key\n  Shape: (seq_len, seq_len)\n  Result: similarity matrix\n  Element [i,j] = how much query_i matches key_j\n\n  / ‚àöd_k:\n  Normalization by embedding dimension\n  Prevents gradient explosion\n\nsoftmax(...):\n  Convert similarities to probabilities [0,1]\n  Sum to 1 per row\n  Interpretation: How much to \"pay attention\" to each position\n\n@ V:\n  Weight values by attention weights\n  Result: Weighted combination of value vectors\n  Each query gets context-specific value\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Example\n\n**Setup:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSequence: [\"The\", \"cat\", \"sat\"]\nEmbedding dimension: d_k = 4\n\nQuery vectors:\n  Q1 = [0.1, 0.2, 0.3, 0.1]  for \"The\"\n  Q2 = [0.4, 0.1, 0.2, 0.3]  for \"cat\"\n  Q3 = [0.2, 0.3, 0.1, 0.4]  for \"sat\"\n\nKey vectors (same as query in self-attention):\n  K1 = [0.1, 0.2, 0.3, 0.1]  for \"The\"\n  K2 = [0.4, 0.1, 0.2, 0.3]  for \"cat\"\n  K3 = [0.2, 0.3, 0.1, 0.4]  for \"sat\"\n\nValue vectors:\n  V1 = [1, 0, 0, 0]  for \"The\"\n  V2 = [0, 1, 0, 0]  for \"cat\"\n  V3 = [0, 0, 1, 0]  for \"sat\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Computation for first query (position 0: \"The\"):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStep 1: Q1 @ K^T (similarity scores)\n  Q1¬∑K1 = 0.1*0.1 + 0.2*0.2 + 0.3*0.3 + 0.1*0.1 = 0.15\n  Q1¬∑K2 = 0.1*0.4 + 0.2*0.1 + 0.3*0.2 + 0.1*0.3 = 0.15\n  Q1¬∑K3 = 0.1*0.2 + 0.2*0.3 + 0.3*0.1 + 0.1*0.4 = 0.15\n\n  Scores: [0.15, 0.15, 0.15]  (all equal - new in training)\n\nStep 2: Divide by ‚àöd_k = ‚àö4 = 2\n  [0.075, 0.075, 0.075]\n\nStep 3: Softmax\n  exp(0.075) ‚âà 1.078\n  exp(0.075) ‚âà 1.078\n  exp(0.075) ‚âà 1.078\n\n  Sum: 3.234\n\n  Softmax: [1.078/3.234, 1.078/3.234, 1.078/3.234]\n         = [0.333, 0.333, 0.333]\n         (uniform distribution)\n\nStep 4: Weight values\n  0.333 * V1 + 0.333 * V2 + 0.333 * V3\n  = 0.333 * [1,0,0,0] + 0.333 * [0,1,0,0] + 0.333 * [0,0,1,0]\n  = [0.333, 0.333, 0.333, 0]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**After training:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWith learned embeddings, differences emerge:\n\nStep 1: Q1 @ K^T\n  Q1¬∑K1 = 0.8   (high - \"The\" attends to itself)\n  Q1¬∑K2 = 0.2   (low - \"The\" doesn't attend to \"cat\")\n  Q1¬∑K3 = 0.3   (low - \"The\" doesn't attend to \"sat\")\n\nStep 2: After scaling and softmax\n  [0.7, 0.15, 0.15]\n\nStep 3: Weighted values\n  0.7 * V1 + 0.15 * V2 + 0.15 * V3\n  = [0.7, 0.15, 0.15, 0]\n\n  Interpretation:\n  \"The\" mostly looks at itself\n  Some information from neighboring words\n  Reasonable: \"The\" is article, not much context needed\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Head Attention\n\n**Why multiple heads?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSingle attention head learns one type of relationship\nDifferent heads can learn different patterns\n\nHead 1: Syntactic (grammar)\n  \"verb\" attends to \"object\"\n  \"noun\" attends to \"adjective\"\n\nHead 2: Semantic (meaning)\n  \"pronoun\" attends to \"antecedent\"\n  \"reference\" attends to \"entity\"\n\nHead 3: Long-range\n  \"end of sentence\" attends to \"beginning\"\n  Captures discourse structure\n\nHead 4: Word type\n  Different parts of speech have different patterns\n\nMultiple heads = multiple representation subspaces\nMore expressive than single head\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nInput: x (seq_len, d_model)\n\nFor each head h = 1 to num_heads:\n  ‚ë† Project to query space\n     Q_h = x @ W_q^(h)    (seq_len, d_k)\n\n  ‚ë° Project to key space\n     K_h = x @ W_k^(h)    (seq_len, d_k)\n\n  ‚ë¢ Project to value space\n     V_h = x @ W_v^(h)    (seq_len, d_v)\n\n  ‚ë£ Compute attention\n     head_h = Attention(Q_h, K_h, V_h)  (seq_len, d_v)\n\nConcatenate all heads:\n  MultiHead = [head_1 || head_2 || ... || head_h]\n              (seq_len, h*d_v)\n\nLinear projection:\n  output = MultiHead @ W_o\n           (seq_len, d_model)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example - 8 heads with d_model=512:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEach head operates in d_k = 512/8 = 64 dimensional space\n8 different projection matrices per Q, K, V\n\nResult:\n  8 independent attention mechanisms\n  Each learns different patterns\n  Combined through concatenation and final projection\n\nTotal parameters for multi-head attention:\n  Q projections: 8 √ó 512 √ó 64 = 262K\n  K projections: 8 √ó 512 √ó 64 = 262K\n  V projections: 8 √ó 512 √ó 64 = 262K\n  Output projection: 512 √ó 512 = 262K\n  Total: ~1M parameters per multi-head attention layer\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaled Dot-Product Attention Revisited\n\n**Why scale by 1/‚àöd_k?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nReason: Prevents gradient vanishing\n\nWithout scaling:\n  For large d_k:\n  Q @ K^T values become very large\n\n  Example: Q and K each 64D\n  Dot product: 64 independent terms\n  Average value: 64 * (avg term)\n\n  Large values ‚Üí softmax saturates ‚Üí gradients ‚Üí 0\n\nScaling by 1/‚àöd_k:\n  Normalizes dot product variance\n  Keep values in reasonable range [-1, 1] roughly\n  Softmax doesn't saturate\n  Gradients flow properly\n\nMathematical justification:\n  Var(Q @ K^T) = Var(Œ£ q_i * k_i)\n                = Œ£ Var(q_i * k_i)\n                = d_k  (if independent)\n\n  Std dev = ‚àöd_k\n\n  Scaling by 1/‚àöd_k makes std dev = 1\n  Keeps gradients stable\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.3 Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nInput sequence\n    ‚Üì\nEmbedding + Positional Encoding\n    ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Transformer Encoder Layer  ‚îÇ √óN (typically 12)\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ Multi-Head Attention   ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îÇ           ‚Üì                  ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ Add & Normalize         ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îÇ           ‚Üì                  ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ Feed-Forward Network    ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ (2 linear layers, ReLU) ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îÇ           ‚Üì                  ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ Add & Normalize         ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚Üì\nOutput (same shape as input)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Layer Breakdown\n\n**1. Positional Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nProblem: Self-attention is permutation invariant\n  Meaning: Word order doesn't matter!\n\n  Attention doesn't care about position\n  Just about content similarity\n\n  Example:\n    \"dog bites man\" vs \"man bites dog\"\n    Same words, different meaning\n    But attention treats them the same!\n\nSolution: Add position information\n\nSinusoidal encoding:\n  PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n  PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\n  Where:\n    pos = position in sequence (0, 1, 2, ...)\n    i = dimension index (0, 1, 2, ..., d_model/2)\n\nExample: Position 0, dimension 0\n  PE(0, 0) = sin(0) = 0\n\n  Position 1, dimension 0:\n  PE(1, 0) = sin(1 / 10000^0) = sin(1) ‚âà 0.84\n\nProperties:\n  ‚ë† Different positions have different encodings\n  ‚ë° Patterns repeat at different frequencies\n  ‚ë¢ Model can learn relative positions\n  ‚ë£ Can extrapolate to longer sequences than training\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Multi-Head Self-Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAll positions attend to all positions\n8-12 heads typically\nEach head learns different patterns\n\nOutput same shape as input\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Add & Normalize (Residual Connection + Layer Normalization)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nResidual connection:\n  output = attention_output + input\n\n  Why?\n  ‚ë† Preserves original information\n  ‚ë° Enables deep networks (gradient flows directly)\n  ‚ë¢ Output can learn \"residual\" (difference)\n\nLayer Normalization:\n  Normalize across feature dimension\n\n  mean = mean(x along d_model dimension)\n  variance = var(x along d_model dimension)\n  normalized = (x - mean) / sqrt(variance + epsilon)\n  output = Œ≥ * normalized + Œ≤\n\n  Œ≥, Œ≤ are learnable parameters\n\n  Why LN instead of Batch Norm?\n  ‚ë† Batch norm depends on batch statistics\n     Different at train/test time\n\n  ‚ë° Layer norm is deterministic\n     Doesn't depend on batch\n     Same at train/test\n\n  ‚ë¢ Works better for sequences\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Feed-Forward Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nMLP with 2 layers and ReLU:\n\nFFN(x) = Linear2(ReLU(Linear1(x)))\n\nDimensions:\n  Linear1: d_model ‚Üí d_ff (usually 4*d_model)\n  ReLU: d_ff ‚Üí d_ff\n  Linear2: d_ff ‚Üí d_model\n\nExample with d_model = 512:\n  Linear1: 512 ‚Üí 2048\n  ReLU: 2048 ‚Üí 2048\n  Linear2: 2048 ‚Üí 512\n\nWhy expand then contract?\n  ‚ë† Increases non-linearity\n  ‚ë° More expressive intermediate representation\n  ‚ë¢ Standard in deep learning\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Transformer Encoder\n\n**BERT (Bidirectional Encoder Representations from Transformers):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nArchitecture:\n  ‚ë† Input: Tokens or subword units\n\n  ‚ë° Embeddings:\n     - Token embedding (word id ‚Üí vector)\n     - Positional embedding (position ‚Üí vector)\n     - Segment embedding (which sentence)\n     - Sum all three\n\n  ‚ë¢ 12 layers (BERT-base) or 24 (BERT-large) of:\n     - Multi-head attention (8-12 heads)\n     - Feed-forward network\n     - Residual connections + Layer norm\n\n  ‚ë£ Output: Contextual embedding for each token\n\nExample input: \"The cat sat on the mat\"\n\nProcessing:\n  [CLS] The cat sat on the mat [SEP]\n    ‚Üì\n  Embed each token\n    ‚Üì\n  Add positional info\n    ‚Üì\n  Layer 1:\n    All tokens attend to all tokens\n    Self-attention with 12 different heads\n\n    \"The\" attends to: \"The\" (self), \"cat\", \"sat\", \"on\", \"the\", \"mat\"\n    Different heads pay attention to different words\n    Results concatenated\n\n    Feed-forward applied to each position\n    Residual + Layer norm\n\n  Layer 2-12:\n    Same process, but input is output from previous layer\n    Further refinement\n\nOutput:\n  12 vectors for each token\n  Plus one special [CLS] token representing full sequence\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.4 Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Causal Masking\n\n**Problem:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nDuring inference, generate one token at a time:\n  Step 1: Predict token 1 (no prior tokens)\n  Step 2: Predict token 2 (given token 1)\n  Step 3: Predict token 3 (given tokens 1, 2)\n\nDuring training (teacher forcing):\n  Complete sequence available: token 1, 2, 3, 4, 5\n\nTo prepare model for inference:\n  Hide future tokens during training\n\nMechanism: Causal mask\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Causal mask visualization:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAttention positions (what can attend to what):\n\nSequence: [token_1, token_2, token_3, token_4, token_5]\n\nPosition 1 (token_1):\n  Can attend to: position 1\n  Cannot attend to: positions 2, 3, 4, 5\n\nPosition 2 (token_2):\n  Can attend to: positions 1, 2\n  Cannot attend to: positions 3, 4, 5\n\nPosition 3 (token_3):\n  Can attend to: positions 1, 2, 3\n  Cannot attend to: positions 4, 5\n\nAttention matrix (‚úì = can attend, ‚úó = masked):\n\n       pos1  pos2  pos3  pos4  pos5\npos1    ‚úì     ‚úó     ‚úó     ‚úó     ‚úó\npos2    ‚úì     ‚úì     ‚úó     ‚úó     ‚úó\npos3    ‚úì     ‚úì     ‚úì     ‚úó     ‚úó\npos4    ‚úì     ‚úì     ‚úì     ‚úì     ‚úó\npos5    ‚úì     ‚úì     ‚úì     ‚úì     ‚úì\n\nMask implementation:\n  Before softmax, set masked positions to -‚àû\n  softmax(-‚àû) = 0\n  Effect: Attention weight = 0 for masked positions\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autoregressive Generation\n\n**Process:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n‚ë† Start: Input special token [START]\n         Decoder produces distribution over vocabulary\n\n‚ë° Step 1: Sample/select token with highest probability\n          Let's say we get \"A\"\n\n‚ë¢ Step 2: Input \"[START] A\"\n          Decoder predicts next token\n          Get \"red\"\n\n‚ë£ Step 3: Input \"[START] A red\"\n          Decoder predicts next token\n          Get \"cat\"\n\n‚ë§ Continue until [END] token or max length\n\nGenerated: \"A red cat\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key points:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n‚ë† Causal masking ensures only past tokens visible\n‚ë° Gradual refinement of representation\n‚ë¢ Can use greedy (highest probability) or sampling\n‚ë£ Sampling: More diverse but less controlled\n   Greedy: More consistent but can repeat\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Attention in Decoder\n\n**Integration with encoder:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEncoder processes source:\n  e.g., Image encoded to 196 patch embeddings\n\nDecoder:\n  ‚ë† Self-attention: Decoder attends to previously generated tokens\n  ‚ë° Cross-attention: Decoder attends to encoder output\n  ‚ë¢ Feed-forward\n\nCross-attention details:\n  Query: Current decoder hidden state\n  Key/Value: Encoder output\n\n  Result: Decoder can look at source modality\n          Ground generation in input\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example - Image captioning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage: [Cat photo]\n       ‚Üì\nImage encoder: 196 patch embeddings\n\nDecoder generating caption:\n\nStep 1:\n  Decoder input: [START]\n  Self-attention: Only [START], attends to itself\n  Cross-attention: [START] attends to all 196 patches\n                   \"What's in image?\"\n  Output: Probability distribution for first word\n\nStep 2:\n  Decoder input: [START] A\n  Self-attention: \"A\" attends to [START] and itself\n                  \"What context?\"\n  Cross-attention: \"A\" attends to patches\n                   \"What modifies 'A'?\"\n  Output: \"cat\"\n\nStep 3:\n  Decoder input: [START] A cat\n  Self-attention: \"cat\" attends to [START], \"A\", \"cat\"\n  Cross-attention: \"cat\" attends to patches\n                   \"What object is this?\"\n  Output: \"sitting\"\n\n...\n\nCaption: \"A cat sitting on a couch\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.5 Putting it Together: Vision Transformer (ViT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture for Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage (224√ó224√ó3)\n    ‚Üì\nDivide into patches (16√ó16)\n    ‚Üì\n14 √ó 14 = 196 patches\n    ‚Üì\nEach patch: 16√ó16√ó3 = 768D\n    ‚Üì\nLinear projection: 768D ‚Üí 768D embedding\n    ‚Üì\nAdd [CLS] special token\n    ‚Üì\nPositional encoding (196 + 1 positions)\n    ‚Üì\nConcatenate: [[CLS]; patch_1; patch_2; ...; patch_196]\n             (197 tokens of 768D each)\n    ‚Üì\nTransformer encoder (12 layers):\n  Multi-head attention (12 heads)\n  Feed-forward (3072D intermediate)\n  Residual connections + Layer norm\n    ‚Üì\nExtract [CLS] token representation\n    ‚Üì\nLinear classifier: 768D ‚Üí num_classes\n    ‚Üì\nOutput: Class probabilities\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why ViT Works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nKey insight 1: Patches as tokens\n  Images have spatial structure\n  Patches preserve local information\n  Transformer learns global relationships\n\nKey insight 2: Transformer is universal\n  Can process any sequence of tokens\n  Doesn't care if tokens are image or text\n  Same architecture works for both!\n\nKey insight 3: Attention gives global context\n  CNNs need many layers for global receptive field\n  ViT has global attention from layer 1\n  Enables fast learning\n\nEmpirical finding:\n  With small data: CNN >> ViT\n  With large data: ViT >> CNN\n\n  Trade-off: Inductive bias vs expressive power\n  CNN: Strong inductive bias (local structure)\n       Works with limited data\n  ViT: Weak inductive bias\n       Needs large data to learn structure\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Transformers** solve sequential bottleneck through attention\n- **Self-attention** computes context through similarity\n- **Multi-head attention** learns diverse patterns\n- **Positional encoding** preserves sequence order\n- **Residual connections** enable deep networks\n- **Feed-forward networks** add non-linearity\n- **Causal masking** enables autoregressive generation\n- **Cross-attention** connects source and target\n- **Vision Transformer** shows transformers work for images too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n**‚≠ê Beginner:**\n1. Compute self-attention by hand\n2. Understand causal masking\n3. Visualize positional encoding patterns\n\n**‚≠ê‚≠ê Intermediate:**\n4. Implement multi-head attention from scratch\n5. Build simple transformer encoder\n6. Visualize attention patterns\n\n**‚≠ê‚≠ê‚≠ê Advanced:**\n7. Implement full transformer encoder-decoder\n8. Build Vision Transformer\n9. Implement efficient attention variants\n\n---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}