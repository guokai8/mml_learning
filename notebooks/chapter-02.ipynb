{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Foundations and Core Concepts\n\n**Interactive Jupyter Notebook Version**\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Foundations and Core Concepts\n\n---\n\n**Previous**: [Chapter 1: Introduction to Multimodal Learning](chapter-01.md) | **Next**: [Chapter 3: Feature Representation for Each Modality](chapter-03.md) | **Home**: [Table of Contents](index.md)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Foundations and Core Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand the mathematical foundations of multimodal learning\n- Explain feature representation and embedding concepts\n- Describe similarity metrics used in multimodal systems\n- Understand modality normalization\n- Apply fundamental concepts to real problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Mathematical Foundations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectors and Embedding Spaces\n\n**Definition:** An embedding is a representation of data as a vector in a high-dimensional space.\n\n**Mathematical notation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nFor data point x, its embedding e is:\ne ∈ ℝ^d\n\nwhere:\nd = dimensionality of embedding space\nℝ^d = d-dimensional real number space\n\nExample:\nImage of cat → e_image = [0.2, -0.5, 0.8, ..., 0.1] ∈ ℝ^2048\nText \"cat\" → e_text = [0.1, 0.3, -0.2, ..., 0.5] ∈ ℝ^768\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why embeddings work:**\n\nEmbeddings capture semantic meaning through:\n1. **Distance** - Similar items have vectors close together\n2. **Direction** - Related concepts align directionally\n3. **Magnitude** - Can encode confidence or importance\n4. **Relationships** - Vector arithmetic can represent semantic operations\n\n**Example - Word2Vec:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEmpirically discovered vector relationships:\n\nking - man + woman ≈ queen\n\nThis works because:\n- \"king\" and \"queen\" appear in similar contexts\n- \"man\" and \"woman\" capture gender transformation\n- Vector arithmetic preserves semantic relationships\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Similarity Metrics\n\n**Core concept:** We need ways to measure how similar two embeddings are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cosine Similarity\n\n**Definition:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nsimilarity(u, v) = (u · v) / (||u|| × ||v||)\n\nwhere:\nu · v = dot product\n||u|| = L2 norm (magnitude)\n\nResult: Score in [-1, 1]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Geometric intuition:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nangle between vectors = arc_cos(similarity)\n\nsimilarity = 1  → Same direction (identical)\nsimilarity = 0  → Perpendicular (unrelated)\nsimilarity = -1 → Opposite direction (contradictory)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why preferred for embeddings:**\n- Invariant to magnitude (direction matters, not size)\n- Computationally efficient\n- Interpretable as angle between vectors\n- Works well in high-dimensional spaces\n\n**Example calculation:**\n\n```python\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vectors\nu = np.array([1, 0, 1, 0])\nv = np.array([1, 0, 1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cosine similarity\ndot_product = np.dot(u, v)  # 1*1 + 0*0 + 1*1 + 0*0 = 2\nmagnitude_u = np.linalg.norm(u)  # sqrt(1+0+1+0) = 1.414\nmagnitude_v = np.linalg.norm(v)  # sqrt(1+0+1+0) = 1.414\n\nsimilarity = dot_product / (magnitude_u * magnitude_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# = 2 / (1.414 * 1.414) = 1.0 (identical vectors)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Euclidean Distance\n\n**Definition:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\ndistance(u, v) = ||u - v|| = sqrt(Σ(u_i - v_i)^2)\n\nResult: Score in [0, ∞)\n- 0 means identical\n- Larger means more different\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When to use:**\n- When absolute differences matter\n- In clustering algorithms\n- When coordinates have physical meaning\n\n**Drawback for high dimensions:**\n- \"Curse of dimensionality\" - distances become less meaningful\n- Cosine similarity preferred for embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Manhattan Distance (L1 Distance)\n\n**Definition:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\ndistance(u, v) = Σ|u_i - v_i|\n\nResult: Sum of absolute differences\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Advantages:**\n- Computationally faster than Euclidean\n- Robust to outliers\n- Encourages sparsity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dot Product Similarity\n\n**Definition:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nsimilarity(u, v) = u · v = Σ(u_i × v_i)\n\nResult: Score in (-∞, ∞)\n- Unbounded, depends on magnitude\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When to use:**\n- In contrastive learning (with temperature scaling)\n- When magnitude carries meaning\n- With normalized vectors (then equivalent to cosine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalization and Standardization\n\n**Why normalize?**\n\nDifferent modalities have different value ranges:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage pixels: [0, 255] or [0, 1] after normalization\nText embeddings: [-1, 1] typically\nAudio: [-1, 1] normalized\n\nWithout normalization:\n- Similarity metrics behave unpredictably\n- Model training becomes unstable\n- Different modalities don't mix well\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Common normalization techniques:**\n\n**1. Min-Max Normalization (Scaling)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nx_normalized = (x - min(x)) / (max(x) - min(x))\n\nResult: All values in [0, 1]\nPreserves: Relationships and shape of distribution\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Z-Score Normalization (Standardization)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nx_normalized = (x - mean(x)) / std(x)\n\nResult: Mean = 0, Standard deviation = 1\nBenefit: Works well for values with Gaussian distribution\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. L2 Normalization (Unit Vector)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nx_normalized = x / ||x||\n\nResult: Vector has magnitude 1\nProperty: Cosine similarity = dot product of L2-normalized vectors\n\nUsed in: CLIP, many embedding models\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example - Normalizing image and text for comparison:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nRaw image features: [234, 1024, -500, ...]\nRaw text features: [0.023, -0.18, 0.51, ...]\n\nAfter L2 normalization:\nImage: [0.234, 0.298, -0.145, ...] (magnitude = 1)\nText: [0.0412, -0.321, 0.911, ...] (magnitude = 1)\n\nNow: Cosine similarity ≈ dot product\nBoth: Fair comparison despite different scales\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Representing Data as Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Information-to-Vector Mapping\n\n**Challenge:** Convert diverse data types to vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nText: \"I love cats\"\n      ↓ (encoder)\n      [0.23, -0.51, 0.82, ..., 0.15] (768D)\n\nImage: [Cat photo]\n       ↓ (encoder)\n       [0.45, 0.12, -0.33, ..., 0.67] (2048D)\n\nAudio: [Cat meow sound]\n       ↓ (encoder)\n       [0.11, -0.09, 0.54, ..., -0.22] (768D)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Desiderata (Desired Properties) for Embeddings\n\nA good embedding should have:\n\n1. **Meaningfulness**\n   - Similar inputs → similar vectors\n   - Related concepts → nearby in space\n\n2. **Efficiency**\n   - Reasonable dimensionality (not too large)\n   - Fast to compute\n   - Fast to compare\n\n3. **Stability**\n   - Small input changes → small embedding changes\n   - Noise in input shouldn't drastically change embedding\n\n4. **Interpretability** (optional but helpful)\n   - Can understand what dimensions represent\n   - Some dimensions → face detection, others → color, etc.\n\n5. **Transferability**\n   - Learned embeddings work across tasks\n   - Generalizes to new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensionality Considerations\n\n**Common embedding dimensions:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTask                          Typical Dimension\n────────────────────────────────────────────\nWord embeddings (Word2Vec)    300\nSentence embeddings (BERT)    768\nContextual text (GPT)         1536-2048\nImage (ResNet50)              2048\nImage (Vision Transformer)    768\nAudio (Wav2Vec2)              768\nMultimodal shared space       256-512\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dimensionality trade-off:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nToo small (e.g., 32D):\n  ✗ Information loss\n  ✗ Cannot capture complex relationships\n  ✓ Fast computation\n  ✓ Less memory\n  ✓ Less prone to overfitting\n\nToo large (e.g., 8192D):\n  ✓ Can capture fine details\n  ✓ Better expressiveness\n  ✗ Slow computation\n  ✗ High memory usage\n  ✗ Prone to overfitting\n  ✗ Curse of dimensionality (distances become uninformative)\n\nSweet spot: 256-2048D for most applications\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Core Concepts Illustrated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Space Visualization\n\nWhile we can't visualize high-dimensional spaces, we can use dimensionality reduction:\n\n**t-SNE (t-Distributed Stochastic Neighbor Embedding):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n2048D embedding space\n        ↓ (project to 2D while preserving relationships)\n2D visualization\n\nExample - CLIP embeddings of common objects:\n\n         \"dog\"\n        /    \\\n      dog   cat -- \"cat\"\n        \\    /\n         \"cat image\"\n\nSimilar items cluster together\nDifferent items spread apart\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Semantic Relationships in Embedding Space\n\nEmbeddings capture semantic meaning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nVector relationships:\n\n┌─────────────────────────────────────────┐\n│ SEMANTIC RELATIONSHIPS IN EMBEDDING     │\n├─────────────────────────────────────────┤\n│                                         │\n│    queen                                │\n│      •                                  │\n│     /│                                  │\n│    / │ (king - man + woman)             │\n│   /  │                                  │\n│  •───•───•                              │\n│ king man woman                          │\n│                                         │\n│ Geometric interpretation:               │\n│ - Parallelogram property               │\n│ - Vector arithmetic = semantic ops      │\n│                                         │\n└─────────────────────────────────────────┘\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Real multimodal example - CLIP space:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCLIP learns joint image-text space where:\n\nImage of cat ────────────► [embedding]\n    \"A picture of a cat\"──► [similar embedding]\n\nImage of dog ────────────► [distant embedding]\n    \"A picture of a dog\"──► [similar embedding]\n\nStructure:\n  - Cat images cluster with \"cat\" texts\n  - Dog images cluster with \"dog\" texts\n  - Cross-category items are far apart\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Understanding Modality-Specific Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text as Vectors\n\n**Properties:**\n- Discrete symbols (words, subwords)\n- Sequential structure (word order matters)\n- Compositional (words combine into sentences)\n- Abstract (can express concepts beyond physical)\n\n**Representation levels:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLEVEL 1 - Character level:\n  \"cat\" → [c, a, t]\n  Problem: Loses semantic meaning\n\nLEVEL 2 - Word level:\n  \"The cat sat\" → [[the], [cat], [sat]]\n  Standard approach\n\nLEVEL 3 - Subword level (BPE):\n  \"unbelievable\" → [un, believable]\n  Handles rare words\n\nLEVEL 4 - Contextual:\n  \"The bank by the river\" → [context-aware embeddings]\n  Same word, different representation based on context\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vector representation methods:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nMETHOD 1 - One-hot encoding:\n  Vocabulary: [the, cat, sat, on, mat]\n  \"cat\" → [0, 1, 0, 0, 0]\n  Dimension = vocabulary size\n  Problem: Very high-dimensional, no semantic meaning\n\nMETHOD 2 - Word embeddings:\n  \"cat\" → [0.2, -0.5, 0.8, ..., 0.1]\n  Dimension = 300 (fixed)\n  Benefit: Captures semantic meaning\n\nMETHOD 3 - Contextual embeddings:\n  \"The cat sat\" →\n  [context_embedding_1, context_embedding_2, context_embedding_3]\n  Dimension = 768 (fixed) per token\n  Benefit: Handles polysemy (multiple meanings)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key insight for multimodal:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nText is interpretable!\nTokens map to meaningful units\nWe can reason about which text parts matter\n\nThis differs from image/audio where interpretation is harder\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Images as Vectors\n\n**Properties:**\n- Continuous values (pixel intensities)\n- 2D spatial structure (nearby pixels correlated)\n- Hierarchical features (edges → shapes → objects)\n- Translational equivariance (object can be anywhere)\n\n**Representation hierarchy:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLEVEL 1 - Pixel level:\n  Image 224×224×3 → 150,528 values\n  Problem: Too high-dimensional, redundant\n\nLEVEL 2 - Low-level features:\n  Edges, corners, textures\n  Extracted by early CNN layers\n\nLEVEL 3 - Mid-level features:\n  Shapes, parts, patterns\n  From middle CNN layers\n\nLEVEL 4 - High-level features:\n  Objects, scenes, semantic concepts\n  From final CNN layers\n\nLEVEL 5 - Global representation:\n  Single vector representing entire image\n  From average pooling or final layer\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CNN feature hierarchy visualization:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nInput image (224×224×3)\n        ↓\nLayer 1: Edges [112×112×64]\n        ↓\nLayer 2: Textures [56×56×128]\n        ↓\nLayer 3: Shapes [28×28×256]\n        ↓\nLayer 4: Parts [14×14×512]\n        ↓\nLayer 5: Objects [7×7×512]\n        ↓\nAverage Pool: [2048D vector]\n\nEach layer extracts higher-level patterns\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key insight for multimodal:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImages are not immediately interpretable\nThe 2048 dimensions don't correspond to human-understandable concepts\n(Except through attention visualization)\n\nThis makes image-text alignment challenging\nMust learn mappings between image features and text concepts\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Audio as Vectors\n\n**Properties:**\n- 1D signal over time\n- Varying frequency content\n- Temporal structure (order matters)\n- Perceptually motivated (frequency relevance to humans)\n\n**Feature extraction process:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nRaw waveform (16kHz, 1 second) → 16,000 samples\n            ↓\nSplit into frames (25ms each) → 40 frames\n            ↓\nExtract spectrogram → 40×513 (time × frequency)\n            ↓\nApply Mel-scale + log → 40×128 (more human-like)\n            ↓\nFinal MFCCs or spectrogram features\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Representation methods:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nMETHOD 1 - MFCC (Mel-Frequency Cepstral Coefficients):\n  Frame → Spectrum → Mel-scale → Cepstral → [39D per frame]\n  Mimics human hearing\n  Traditional approach\n\nMETHOD 2 - Spectrogram:\n  Frame → FFT → Power spectrum → [513D per frame]\n  All frequency information\n  Used in deep learning\n\nMETHOD 3 - Learned features (Wav2Vec):\n  Raw waveform → CNN → Quantized codes\n  → Transformer → [768D learned representation]\n  Modern approach\n  Learns task-relevant features\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key insight for multimodal:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAudio is temporal but can be converted to spectral view\nFrequency information is similar to visual features\n(Both are \"spectral\" representations)\n\nThis can facilitate audio-visual alignment\n(e.g., beat synchronization in music videos)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 The Feature Extraction Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General Pipeline Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nRaw Data\n    ↓\n[Preprocessing]\n  - Normalization\n  - Augmentation\n  - Format conversion\n    ↓\n[Feature Extraction]\n  - Shallow features (SIFT, MFCC)\n  - Or deep features (CNN, Transformer)\n    ↓\n[Post-processing]\n  - Normalization (L2)\n  - Dimensionality reduction\n  - Feature selection\n    ↓\n[Embedding Vector]\n  Ready for comparison or fusion\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Considerations\n\n**1. Batch processing efficiency:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nDon't extract features one at a time\nProcess batches for GPU efficiency\n\nBatch size trade-offs:\n  Larger batch: Better GPU utilization\n  Smaller batch: Less memory, more iterations needed\n  Typical: 32-256 depending on data type\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Feature caching:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nFor large-scale retrieval systems:\n\nOnline phase (expensive):\n  Extract features once, cache them\n\nRetrieval phase (cheap):\n  Query by similarity to cached features\n  No need to re-extract\n\nExample:\n  E-commerce with 10M products\n  Extract features once (hours of computation)\n  Serve queries (milliseconds)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Approximate similarity:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nExact nearest neighbor search is slow for large datasets\nUse approximate methods:\n\nHashing: Map similar embeddings to same hash bucket\nLSH: Locality-Sensitive Hashing\nFAISS: Facebook AI Similarity Search\nScaNN: Scalable Nearest Neighbors\n\nTrade-off: Accuracy vs speed\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Practical Example: Building Feature Extractors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image Feature Extractor\n\n```python\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load pre-trained ResNet50\nmodel = models.resnet50(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Remove classification head\nmodel = torch.nn.Sequential(*list(model.children())[:-1])\nmodel.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract features\ndef extract_image_features(image_path):\n    image = Image.open(image_path)\n    image = preprocess(image)\n    image = image.unsqueeze(0)  # Add batch dimension\n\n    with torch.no_grad():\n        features = model(image)\n\n    # Flatten and L2-normalize\n    features = features.flatten()\n    features = features / torch.norm(features)\n\n    return features.numpy()\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Feature Extractor\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load BERT\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\nmodel.eval()\n\ndef extract_text_features(text):\n    # Tokenize\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n        padding=True\n    )\n\n    # Extract embeddings\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Use [CLS] token representation\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n\n    # L2-normalize\n    cls_embedding = cls_embedding / torch.norm(cls_embedding)\n\n    return cls_embedding.numpy()\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computing Similarity\n\n```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract features\nimage_feat = extract_image_features(\"cat.jpg\")  # Shape: (2048,)\ntext_feat = extract_text_features(\"A cute cat\")  # Shape: (768,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem: Different dimensions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solution: Project to shared space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple approach: L2 normalization and dimension reduction\nfrom sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project both to 256D\npca_img = PCA(n_components=256)\npca_txt = PCA(n_components=256)\n\nimg_proj = pca_img.fit_transform(image_feat.reshape(1, -1))\ntxt_proj = pca_txt.fit_transform(text_feat.reshape(1, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compute similarity\nsimilarity = cosine_similarity(img_proj, txt_proj)\nprint(f\"Similarity: {similarity[0][0]:.3f}\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Embeddings** are vector representations that capture semantic meaning through geometry\n- **Cosine similarity** is the preferred metric for comparing embeddings\n- **Normalization** is essential when working with multimodal data\n- **Different modalities** have different properties requiring specialized handling\n- **Feature extraction** is a pipeline from raw data to interpretable vectors\n- **Dimensionality** is a critical design choice balancing expressiveness and efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n**⭐ Beginner:**\n1. Calculate cosine similarity between three vectors by hand\n2. Explain why L2 normalization makes cosine similarity equal to dot product\n3. Describe the properties of text, image, and audio modalities\n\n**⭐⭐ Intermediate:**\n4. Implement a similarity search system for 1000 pre-extracted embeddings\n5. Compare cosine, Euclidean, and dot product similarity on sample data\n6. Visualize embeddings using t-SNE and interpret clusters\n\n**⭐⭐⭐ Advanced:**\n7. Design a hybrid normalization scheme for multimodal data\n8. Implement approximate nearest neighbor search with locality-sensitive hashing\n9. Analyze how normalization affects downstream multimodal fusion\n\n---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}