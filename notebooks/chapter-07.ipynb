{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Contrastive Learning\n\n**Interactive Jupyter Notebook Version**\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Contrastive Learning\n\n---\n\n**Previous**: [Chapter 6: Attention Mechanisms in Multimodal Systems](chapter-06.md) | **Next**: [Chapter 8: Transformer Architecture](chapter-08.md) | **Home**: [Table of Contents](index.md)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand contrastive learning principles and motivation\n- Implement InfoNCE loss\n- Understand CLIP's revolutionary approach\n- Compare different contrastive methods\n- Apply contrastive learning to your own problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 The Problem Contrastive Learning Solves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Traditional Supervised Learning\n\n**Standard approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTraining data: (input, label) pairs\n\nTask: Image classification\n  Input: Image\n  Label: \"cat\" or \"dog\"\n\n  Process:\n  ① Pass image through network\n  ② Output logits for each class\n  ③ Cross-entropy loss compares to label\n  ④ Backprop updates weights\n\nRequirements:\n  ✗ Requires labels for everything\n  ✗ Labels are expensive (human annotation)\n  ✗ Limited to labeled dataset size\n  ✗ New task = new labeled data needed\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Bottleneck in practice:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nProblem: Most data is unlabeled\n\nExample:\n  ImageNet: 1.4M labeled images\n  Internet: Billions of images daily\n\n  Ratio: ~1 labeled per 1 million unlabeled!\n\nQuestion: How to leverage the vast unlabeled data?\n\nTraditional supervised learning: Can't use it!\nSolution: Contrastive learning\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-Supervised Learning Intuition\n\n**Key insight:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nDon't need explicit labels!\nCreate labels from data itself using natural structure\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example - Image rotation prediction:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nUnlabeled image:\n  [Photo of cat]\n\nCreate self-supervised task:\n  Rotate image 90°\n\n  Rotated image → Network → Predict rotation\n\nLabel is free! (We created it by rotation)\n\nTraining:\n  ① Rotate image by random angle (0°, 90°, 180°, 270°)\n  ② Network predicts angle\n  ③ Loss: Cross-entropy between predicted and actual angle\n\nResult:\n  Network learns visual representations\n  Without any human labels!\n\nBenefit:\n  Can train on billions of unlabeled images\n  Representations useful for downstream tasks\n  Transfer to real tasks with small labeled dataset\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this works:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTo predict rotation, network must understand:\n  - What's the \"up\" direction? (spatial orientation)\n  - What are objects and their structure? (semantics)\n  - What's foreground vs background? (attention)\n\nThese are useful representations for other tasks!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contrastive Learning Idea\n\n**Core concept:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSupervised learning: \"Is this input A or B or C?\"\nContrastive learning: \"Which B is similar to A?\"\n\nExample:\n  Supervised:      \"Is this a dog?\" (Yes/No)\n  Contrastive:     \"Given this dog photo, which text matches best?\n                    A) 'A dog running'\n                    B) 'A cat sleeping'\n                    C) 'A car parked'\"\n\nContrastive doesn't need explicit labels\nJust needs relative similarities!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why it's powerful:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAdvantage 1: No labels needed\n  ✓ Use unlabeled data directly\n  ✓ Billions of image-text pairs from web\n  ✓ Much cheaper than labeling\n\nAdvantage 2: Richer signal\n  Binary classification: Yes/No (1 bit)\n  Contrastive: Ranking among many (log₂(N) bits)\n\n  With N=1000 options:\n  Ranking gives ~10 bits of information\n  vs 1 bit for binary\n\nAdvantage 3: Metric learning\n  Directly optimize for similarity\n  Better representations for retrieval\n  Natural distance metrics\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 InfoNCE Loss - The Foundation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Loss\n\n**Name breakdown:**\n- **Info** = Information theory\n- **NCE** = Noise Contrastive Estimation\n\n**Goal:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nMake positive pairs similar\nMake negative pairs dissimilar\n\nPositive pair: (cat image, \"cat\" text)\nNegative pair: (cat image, \"dog\" text)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Formulation\n\n**Formula:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nL = -log [ exp(sim(q,k+)/τ) / (exp(sim(q,k+)/τ) + Σⱼ exp(sim(q,k⁻ⱼ)/τ)) ]\n\nBreakdown:\n\nq = query (e.g., image)\nk+ = positive key (e.g., matching text)\nk⁻ⱼ = negative keys (non-matching texts)\nτ = temperature (controls sharpness)\nsim = similarity function (cosine, dot product)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step-by-step explanation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStep 1: Compute similarities\n  sim(query, positive) = dot product\n  sim(query, negative₁) = dot product\n  sim(query, negative₂) = dot product\n  ...\n\n  Result: Scores (could be any value)\n\nStep 2: Scale by temperature\n  Score / τ\n\n  Temperature effect:\n    τ small (0.01): Scores become extreme\n    τ normal (0.1): Moderate scaling\n    τ large (1.0): Minimal scaling\n\n  Why temperature?\n    Prevents softmax from being too sharp\n    Allows gradient flow during training\n\nStep 3: Exponential\n  exp(score / τ)\n\n  Result: All positive (e^x > 0 for all x)\n\n  Effect:\n    Larger scores → larger exponents\n    Softmax then emphasizes them\n\nStep 4: Softmax (normalize)\n  exp(positive) / (exp(positive) + Σ exp(negatives))\n\n  Result: Probability in [0, 1]\n\n  Interpretation:\n    Probability that positive is highest ranked\n    Perfect: Probability = 1.0\n    Random: Probability = 1/(1+num_negatives)\n\nStep 5: Negative log\n  -log(probability)\n\n  If probability = 1.0: loss = 0 (perfect!)\n  If probability = 0.1: loss = -log(0.1) = 2.3 (bad)\n  If probability = 0.5: loss = -log(0.5) = 0.69 (medium)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Example\n\n**Setup:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nQuery: Image of red cat\nPositive: Text \"a red cat\"\nNegatives:\n  - \"a blue dog\"\n  - \"a green parrot\"\n  - \"a car\"\n\nSimilarities (before temperature):\n  sim(query, positive) = 0.8    (high, should be!)\n  sim(query, neg1) = 0.2        (low, good)\n  sim(query, neg2) = 0.15       (low, good)\n  sim(query, neg3) = 0.1        (low, good)\n\nTemperature τ = 0.1\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Computing loss:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStep 1: Scale by temperature\n  0.8 / 0.1 = 8.0\n  0.2 / 0.1 = 2.0\n  0.15 / 0.1 = 1.5\n  0.1 / 0.1 = 1.0\n\nStep 2: Exponentials\n  e^8.0 ≈ 2981\n  e^2.0 ≈ 7.4\n  e^1.5 ≈ 4.5\n  e^1.0 ≈ 2.7\n\nStep 3: Softmax (probability)\n  2981 / (2981 + 7.4 + 4.5 + 2.7)\n  = 2981 / 2995.6\n  ≈ 0.995   (99.5% probability positive is best!)\n\nStep 4: Loss\n  loss = -log(0.995) ≈ 0.005   (very small! Model doing great)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What if model was bad:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSimilarities:\n  sim(query, positive) = 0.1    (low! bad!)\n  sim(query, neg1) = 0.5        (high! worse)\n  sim(query, neg2) = 0.4\n  sim(query, neg3) = 0.3\n\nAfter temperature scaling (τ = 0.1):\n  0.1 / 0.1 = 1.0     → e^1.0 ≈ 2.7\n  0.5 / 0.1 = 5.0     → e^5.0 ≈ 148\n  0.4 / 0.1 = 4.0     → e^4.0 ≈ 55\n  0.3 / 0.1 = 3.0     → e^3.0 ≈ 20\n\nSoftmax:\n  2.7 / (2.7 + 148 + 55 + 20)\n  = 2.7 / 225.7\n  ≈ 0.012   (1.2% probability - terrible!)\n\nLoss:\n  -log(0.012) ≈ 4.4   (very large! Forces update)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why This Works\n\n**Mathematical properties:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Bounded between 0 and log(1+N)\n   where N = number of negatives\n\n   N=10: Loss ∈ [0, log(11) ≈ 2.4]\n   N=100: Loss ∈ [0, log(101) ≈ 4.6]\n\n   Interpretable scale\n\n2. Gradient is informative\n\n   Perfect case (prob ≈ 1): gradient ≈ 0\n   Good case (prob ≈ 0.9): gradient ≈ small\n   Bad case (prob ≈ 0.1): gradient ≈ large\n\n   Automatically focuses on hard cases\n\n3. Invariant to scale\n\n   If all similarities multiplied by constant K:\n   exp(K*sim) has same relative ordering\n   Softmax still works correctly\n\n   Enables using unnormalized similarities\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Temperature Parameter\n\n**Role of τ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTemperature controls softmax sharpness\n\nτ = 0.01 (very cold):\n  Softmax becomes nearly one-hot\n  exp(5) = 148\n  exp(4) = 55\n  exp(3) = 20\n  Ratio: 148/55 = 2.7x difference\n\n  Large differences between outputs\n  Large gradients\n  Potential instability\n\nτ = 0.1 (standard):\n  Moderate softmax\n  exp(0.5) = 1.65\n  exp(0.4) = 1.49\n  exp(0.3) = 1.35\n  Ratio: 1.65/1.49 = 1.1x difference\n\n  Balanced gradients\n  Stable training\n  Common choice\n\nτ = 1.0 (very hot):\n  Softmax becomes smooth\n  exp(0.05) = 1.05\n  exp(0.04) = 1.04\n  exp(0.03) = 1.03\n  Ratio: 1.05/1.04 ≈ 1.01x difference\n\n  Small differences between outputs\n  Small gradients\n  Slow learning\n\nτ = 10.0 (extremely hot):\n  Softmax nearly uniform\n  All classes almost equally likely\n  Nearly no signal\n  Training doesn't work\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Effect on learning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nOptimal temperature depends on:\n  - Number of negatives\n  - Difficulty of task\n  - Data quality\n\nTypical range: τ ∈ [0.05, 0.2]\n\nCLIP uses: τ ≈ 0.07 (learned during training)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 CLIP - Contrastive Learning Success Story"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Context and Impact\n\n**Problem statement (2020):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nExisting vision models:\n  - Trained on ImageNet (1.4M images)\n  - Limited to 1000 classes\n  - Can't generalize to new concepts\n  - Require supervised fine-tuning\n\nQuestion:\n  Can we use web data (unsupervised) for vision?\n  Can we match NLP's success with massive unlabeled data?\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CLIP solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nData: 400M image-caption pairs from web\nTask: Learn from natural language supervision\nMethod: Contrastive learning on image-text pairs\n\nResult: Revolutionary zero-shot transfer\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CLIP Architecture\n\n**Components:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage encoder:           Text encoder:\n  Vision Transformer      Transformer (BERT-like)\n  Input: 224×224 image    Input: Text tokens\n  Output: 512D vector     Output: 512D vector\n\n            ↓                     ↓\n\n    [Normalize to unit length]\n\n            ↓                     ↓\n\n    Similarity computation (dot product of normalized)\n\n            ↓\n\n    Contrastive loss\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data collection:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n400 million image-caption pairs from internet\n\nSources:\n  - Web pages with images and captions\n  - Publicly available image databases\n  - Social media posts with text\n  - Stock photo sites with descriptions\n\nQuality:\n  - Uncurated and diverse\n  - Contains noise and biases\n  - Reflects web distribution\n  - Natural language (not formal labels)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Process\n\n**Batch construction:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nBatch size: 32,768 (massive!)\n\nImages: [img_1, img_2, ..., img_32k]\nCaptions: [caption_1, caption_2, ..., caption_32k]\n\nEncode all:\n  Image embeddings: 32k × 512\n  Caption embeddings: 32k × 512\n\nCompute similarity matrix (32k × 32k):\n  sim[i,j] = image_i · caption_j\n\nGoal:\n  Diagonal elements high (matched pairs)\n  Off-diagonal elements low (mismatched pairs)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Loss computation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nFor each image:\n  Compute InfoNCE loss\n  Positive: matching caption\n  Negatives: all other 32k-1 captions\n\nFor each caption:\n  Compute InfoNCE loss\n  Positive: matching image\n  Negatives: all other 32k-1 images\n\nTotal loss = average of all losses\n\nOptimization:\n  Adam optimizer\n  Learning rate: 5×10⁻⁴\n  Training: ~2 weeks on large clusters\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Transfer - Revolutionary Capability\n\n**Traditional approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNew task: Classify images of birds (not in ImageNet)\n\nSteps:\n  1. Get labeled training data for birds\n  2. Fine-tune ImageNet model\n  3. Get predictions\n\nProblem: Need labeled bird data!\nCost: Expensive annotation\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CLIP zero-shot approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNew task: Classify images of birds\n\nNo training needed!\n\nSteps:\n  1. Text prompts: \"a photo of a bird\"\n                   \"a photo of a person\"\n                   \"a photo of a car\"\n\n  2. Encode each prompt with CLIP text encoder\n     → 512D vectors\n\n  3. For test image:\n     - Encode with CLIP image encoder\n     - Compute similarity to each prompt\n     - Select highest similarity\n\n  4. Done! Zero-shot classification\n\nExample:\n  Image similarity scores:\n    \"a photo of a bird\": 0.92    ← Highest\n    \"a photo of a person\": 0.15\n    \"a photo of a car\": 0.08\n\n  Prediction: Bird\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why it works:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCLIP trained on 400M diverse image-caption pairs\nLearned that:\n  - Images with birds cluster with \"bird\" text\n  - Images with people cluster with \"person\" text\n  - Images with cars cluster with \"car\" text\n\nThese mappings generalize to new images!\n\nTransfer learning without fine-tuning:\n  - No labeled data needed\n  - No training required\n  - Immediate deployment\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Benchmark Results\n\n**Zero-shot transfer (ImageNet classification):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTraditional supervised:\n  ResNet-50: 76.1% accuracy\n\nCLIP zero-shot:\n  CLIP-ViT-L/14: 62.8% accuracy\n\nSeems lower, BUT:\n  - CLIP trained on NO labeled images\n  - Just 400M raw internet data\n  - Immediately applicable to any category\n  - ResNet trained with 1.4M labeled ImageNet\n\nAdjusted for training data:\n  ResNet: 76.1% on specific dataset\n  CLIP: 62.8% on ANY dataset (zero-shot)\n\n  CLIP more generalizable!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**After fine-tuning on small labeled sets:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImageNet (1% labeled):\n  CLIP: 76.3% accuracy\n\nComparison:\n  - CLIP fine-tuned with 1% labels ≈ ResNet with 100% labels\n  - 100× more data-efficient!\n  - Shows power of pre-training\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Other domains:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTransfer to new datasets:\n\nSTL10 (airplane, bird, car, etc.):\n  CLIP: 92.9% zero-shot\n\nFood101 (food classification):\n  CLIP: 88.3% zero-shot\n\nEuroSAT (satellite imagery):\n  CLIP: 58.4% zero-shot\n\nWorks across diverse domains!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why CLIP is Revolutionary\n\n**1. Scale:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n400M image-text pairs >> 1.4M ImageNet\nShows power of scale in representation learning\nUnlabeled data is abundant!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Natural supervision:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLanguage is natural way to describe images\nNot forced to 1000 classes like ImageNet\nFlexible descriptors\nCan specify any attribute\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Zero-shot transfer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNo fine-tuning needed\nImmediate deployment\nNo labeled data required\nGeneralizes across domains\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Open-ended prediction:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNot limited to predefined classes\nCan describe images with any text\n\"A cat wearing a hat\"\n\"A red car on a mountain\"\nAny description works!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impact on Field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCLIP (April 2021) was watershed moment\n\nBefore CLIP:\n  - Supervised learning paradigm dominant\n  - Limited to ImageNet 1000 classes\n  - Required labeled data for new tasks\n  - Struggled on out-of-distribution data\n\nAfter CLIP:\n  - Contrastive learning became mainstream\n  - Foundation model era began\n  - Zero-shot transfer became practical\n  - Industry adopted language-grounded vision\n\nInspired:\n  - ALIGN (Google)\n  - LiT (Google)\n  - COCA (Meta)\n  - Flamingo (DeepMind)\n  - BLIP (Salesforce)\n  - Many others...\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Variants and Extensions of Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: SimCLR - Self-Supervised Vision\n\n**Motivation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCLIP uses text for supervision\nWhat if we only have unlabeled images?\n\nAnswer: Use image augmentations as \"supervision\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Core idea:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSingle image:\n  [Original cat photo]\n\nCreate two augmented versions:\n  [Rotated, cropped, color-adjusted]\n  [Different rotation, crop, colors]\n\nTreat as positive pair:\n  Both should have similar representations\n  (Same cat, different augmentations)\n\nNegatives:\n  Other images in batch\n\nLoss: Make augmentations similar,\n      other images dissimilar\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Process:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Sample image x from dataset\n\n2. Create two augmented versions:\n   x_i = Aug(x)  (augmentation 1)\n   x_j = Aug(x)  (augmentation 2)\n\n   Different random augmentations!\n\n3. Encode both through network f:\n   h_i = f(x_i)\n   h_j = f(x_j)\n\n4. Project to embedding space:\n   z_i = g(h_i)\n   z_j = g(h_j)\n\n5. Contrastive loss:\n   sim(z_i, z_j) should be high\n   sim(z_i, z_k) should be low (for k ≠ i,j)\n\n6. Backprop updates f and g\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWhy this works:\n\nAssumptions:\n  1. Augmentations preserve content\n  2. Different images are different\n\nImplications:\n  Model learns representations that:\n  - Survive augmentations (robust features)\n  - Differ between images (discriminative features)\n  - Capture semantic content (not style)\n\nResult:\n  Representations useful for downstream tasks\n  Without any labels!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Augmentations used:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStrong augmentations needed for self-supervised learning:\n\nRandom crop:\n  (up to 85% crop)\n  ↑ Forces learning of part representations\n\nColor jittering:\n  Brightness, contrast, saturation, hue\n  ↑ Prevents learning from color only\n\nGaussian blur:\n  Blurs fine details\n  ↑ Forces learning of structure, not pixels\n\nRandom grayscale:\n  Removes color information\n  ↑ Forces learning of shape and texture\n\nGaussian noise:\n  Adds random noise\n  ↑ Makes features robust\n\nNote: Extreme augmentations avoid (would destroy content)\n  - Extreme rotation: Flips meaning\n  - Extreme scaling: Makes object invisible\n  - Extreme distortion: No longer recognizable\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Differences from CLIP:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n                SimCLR          CLIP\n────────────────────────────────────\nSupervision     Image augment   Text\nData            Unlabeled       Image-caption pairs\nRequires        Images only     Images + text\nGeneralization  Moderate        Excellent\nTask alignment  Generic vision  Language grounding\nTransfer        Good            Excellent\nInterpretable   No              Yes (language)\n\nWhen to use:\n  SimCLR: When you only have unlabeled images\n  CLIP: When you have image-caption pairs\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: MoCo - Momentum Contrast\n\n**Problem with SimCLR:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSimCLR requires large batch size:\n  - Small batch: Few negatives → weak learning signal\n  - Large batch: Better negatives → better learning\n\n  Batch size 4096 requires massive GPU memory\n  And distributed training complexity\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MoCo solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nUse memory bank instead of current batch\n\nBenefits:\n  ✓ Can use smaller batch size\n  ✓ Negatives more diverse (from different times)\n  ✓ More efficient\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nOnline encoder: f_q\n  Learns from current batch\n  Updated every step\n\nMemory bank: Queue\n  Stores recent representations\n  Old representations pushed out as new added\n\nMomentum encoder: f_k\n  Slowly following online encoder\n  f_k = α × f_k + (1-α) × f_q\n\n  Typically α = 0.999\n  Moves slowly (momentum!)\n\nProcess:\n\n1. Current batch through online encoder\n   → query embeddings q\n\n2. Pop old representations from queue\n   → memory negatives\n\n3. Compute loss using:\n   - query from online encoder (positive)\n   - memory from momentum encoder (negatives)\n\n4. Push new representations to queue\n\n5. Update momentum encoder (slowly follows online)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why momentum encoder:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWithout it:\n  Queue contains representations from old network\n  Network keeps changing → representations inconsistent\n  Training unstable\n\nWith momentum encoder:\n  Queue contains representations from slow network\n  Representations are consistent\n  Training stable\n\nEffect:\n  Momentum = inertia\n  Small updates accumulate\n  Smooth trajectory\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Performance:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImageNet pre-training → transfer to other tasks\n\n                Top-1 Accuracy\n────────────────────────────────\nSupervised      76.5% (ResNet-50)\nSimCLR          69.3% (requires large batch)\nMoCo v1         60.6% (with 65K negatives)\nMoCo v2         71.3% (improved version)\nMoCo v3         76.7% (vision transformer)\n\nNote: Self-supervised eventually matched supervised!\n      Shows power of approach\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 3: BYOL - Contrastive Without Negatives\n\n**Surprising finding (Grill et al., 2020):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nDo we even need negative examples?\n\nTraditional contrastive:\n  Make positives similar\n  Make negatives dissimilar\n\nBYOL:\n  Only make positives similar\n  No explicit negatives!\n\nQuestion: How does this work?\n\nAnswer: Still has implicit negatives\n        (Through model architecture and learning dynamics)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nOnline network:\n  Encoder f + Projector g\n  Input: image → output: representation\n  Updated every step\n\nTarget network:\n  Copy of online network\n  Parameter updates: EMA (exponential moving average)\n  target_param = α × target_param + (1-α) × online_param\n\nPredictor h:\n  Additional MLP on top of online network\n  NOT on target network (asymmetry!)\n\nLoss:\n  For two augmentations of same image:\n  loss = ||h(online(aug1)) - target(aug2)||²\n\n  Make online and target predictions close\n  Using MSE loss (not contrastive!)\n\n  Also symmetrically:\n  loss += ||h(online(aug2)) - target(aug1)||²\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this works (still debated!):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nPossible explanations:\n\n1. Implicit negatives through optimization\n   - Mini-batch gradient descent creates diversity\n   - Network can't collapse to constant\n   - Similar to negative mining\n\n2. Momentum encoder provides stability\n   - Target network changes slowly\n   - Creates effective \"negatives\" through difference\n\n3. Predictor prevents mode collapse\n   - Without predictor: Would learn trivial solution\n   - With predictor: Breaks symmetry\n   - Forces meaningful learning\n\nEmpirical results:\n  BYOL works surprisingly well!\n  Without explicit negatives!\n  Counterintuitive but effective\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Advantages:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n✓ Doesn't need negative pairs\n✓ Don't need image-text pairs (image-only sufficient)\n✓ Works with small batches\n✓ Stable training\n✓ Strong performance (competitive with SimCLR)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Disadvantages:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n✗ Why it works still not fully understood\n✗ Less interpretable\n✗ More complex architecture\n✗ Harder to debug when it fails\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Practical Guide to Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Contrastive Learning\n\n**Basic template:**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContrastiveLearningModel(nn.Module):\n    def __init__(self, encoder, projection_dim=256):\n        super().__init__()\n        self.encoder = encoder\n        self.projector = nn.Linear(encoder.output_dim, projection_dim)\n\n    def forward(self, x):\n        # Encode\n        h = self.encoder(x)\n\n        # Project\n        z = self.projector(h)\n\n        # Normalize\n        z = F.normalize(z, p=2, dim=1)\n\n        return z\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        Compute NT-Xent loss\n        z_i, z_j: (batch_size, embedding_dim) tensors\n        \"\"\"\n        batch_size = z_i.shape[0]\n\n        # Concatenate: positive pairs are diagonal\n        z = torch.cat([z_i, z_j], dim=0)  # (2*batch, dim)\n\n        # Similarity matrix\n        similarity = torch.mm(z, z.t()) / self.temperature\n\n        # Create labels: diagonal elements are positives\n        labels = torch.arange(batch_size, device=z.device)\n        labels = torch.cat([labels, labels])\n\n        # Positive pairs at positions (i, batch+i) and (batch+i, i)\n        # Compute loss: each sample should match its pair\n\n        # Loss for all positions\n        loss = F.cross_entropy(similarity, labels)\n\n        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop\ndef train_contrastive(model, data_loader, optimizer, device, epochs=100):\n    criterion = ContrastiveLoss(temperature=0.07)\n\n    for epoch in range(epochs):\n        total_loss = 0\n\n        for images in data_loader:\n            # Get two augmented versions\n            x_i = augment(images)\n            x_j = augment(images)\n\n            x_i = x_i.to(device)\n            x_j = x_j.to(device)\n\n            # Forward pass\n            z_i = model(x_i)\n            z_j = model(x_j)\n\n            # Compute loss\n            loss = criterion(z_i, z_j)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(data_loader)\n        print(f\"Epoch {epoch}: Loss = {avg_loss:.3f}\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing Hyperparameters\n\n**Temperature:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nRange: [0.05, 0.2]\n\nDiagnostic:\n  Training loss plateaus at high value?\n    → Temperature too low (sharp, unstable)\n    → Increase τ\n\n  Training loss decreases but very slowly?\n    → Temperature too high (smooth, weak signal)\n    → Decrease τ\n\nRule of thumb:\n  Start with τ = 0.1\n  Adjust based on loss curve\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Batch size:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLarger batch = more negatives = better signal\n\nTypical choices:\n  Small GPU: 256-512\n  Medium GPU: 1024-2048\n  Large GPU: 4096+\n  Multi-GPU: 32K+ (like CLIP)\n\nTrade-off:\n  Larger batch: Better learning, slower per epoch\n  Smaller batch: Worse learning, faster per epoch\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Projection dimension:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEmbedding dimension (before projection): 1024-2048 (from encoder)\nProjection dimension: 128-512\n\nCommon choices:\n  256D (standard)\n  128D (more compression)\n  512D (less compression)\n\nEffect:\n  Smaller: Faster computation, less memory\n  Larger: More expressive, risk of overfitting\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of negatives:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWithin batch:\n  Batch size 256 → 255 negatives per sample\n\nMemory bank (MoCo):\n  Queue size 65536 → 65535 negatives\n\nMore negatives → better learning signal\nBut more computation\nTypical: 255-65K negatives\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Contrastive Models\n\n**Method 1: Linear evaluation protocol**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Train contrastive model on unlabeled data\n   → Get representations\n\n2. Freeze encoder\n   → Don't update weights\n\n3. Train linear classifier on representations\n   → Small labeled dataset\n\n4. Evaluate on test set\n\nMetric: Accuracy of linear classifier\nInsight: If representations good → linear classifier accurate\n\nExample:\n  CIFAR-10 (50K training images)\n  Contrastive pre-training: All 50K unlabeled\n  Linear eval: 5K labeled for training, 10K for testing\n\n  Result: 96% accuracy\n  Interpretation: Representations capture meaningful patterns\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Method 2: Transfer learning evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Train contrastive model on source dataset\n2. Fine-tune on target task\n3. Compare to:\n   - Supervised baseline\n   - Random initialization\n   - Other pre-training methods\n\nMetric: Downstream task accuracy\nInsight: Better representations → better transfer\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Method 3: Downstream task performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nPre-training dataset: ImageNet (unlabeled contrastive)\nDownstream tasks:\n  1. ImageNet-100 classification (supervised fine-tune)\n  2. CIFAR-10 classification\n  3. STL10 classification\n  4. Transfer to object detection\n  5. Transfer to segmentation\n\nResults show generalization across tasks\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Troubleshooting Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 1: Loss not decreasing\n\n**Potential causes:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n① Temperature too low\n   Effect: Softmax too sharp\n   Solution: Increase τ (e.g., 0.1 → 0.2)\n\n② Learning rate too small\n   Effect: Updates too tiny\n   Solution: Increase learning rate\n\n③ Batch size too small\n   Effect:\n\n-----\n\n> Continue\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Effect: Weak learning signal\n   Solution: Increase batch size if possible\n\n④ Bad initialization\n   Effect: Starting in bad local minimum\n   Solution: Use proper weight initialization\n\n⑤ Augmentations too weak\n   Effect: Positive pairs too similar anyway\n   Solution: Increase augmentation strength\n\n⑥ Augmentations too strong\n   Effect: Positive pairs become different objects\n   Solution: Decrease augmentation strength"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n\n**Debugging steps:**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Check loss values\nprint(f\"Initial loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should decrease over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# If increasing or constant: something wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Check similarity matrix\nsimilarity = torch.mm(z, z.t())\nprint(f\"Max similarity: {similarity.max():.3f}\")\nprint(f\"Min similarity: {similarity.min():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should: Max ≈ 1, Min ≈ -1 for normalized vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Check gradients\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name}: grad_norm={param.grad.norm():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should be reasonable values (not 0, not inf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Check temperature effect\ntemperatures = [0.01, 0.05, 0.1, 0.2, 0.5]\nfor tau in temperatures:\n    loss = compute_loss(embeddings, tau)\n    print(f\"τ={tau}: loss={loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should have sweet spot, not too high/low everywhere\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 2: Representation collapse\n\n**What is it:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nModel learns to make all representations nearly identical\n\nExample:\n  All images → representation [0.5, 0.5, 0.5, ...]\n  All images → representation [0.51, 0.49, 0.50, ...]\n\n  Trivial solution: \"All same = all similar\"\n  Loss can be artificially low!\n  But representations useless for downstream tasks\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Symptoms:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n✓ Loss decreasing nicely\n✗ Linear evaluation performance poor\n✗ Representations clustered at single point\n✗ Variance of representations near zero\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Causes and solutions:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCause 1: No negatives (only positives)\n  Solution: Ensure you have negatives in batch\n\nCause 2: Batch too small\n  Solution: Increase batch size\n\nCause 3: No regularization\n  Solution: Add normalization (L2 normalization helps)\n\nCause 4: Poor augmentations\n  Solution: Ensure augmentations are meaningful\n  (Reproduce the issue with weak augmentations)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Prevention:**\n\n```python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monitor variance\ndef monitor_collapse(z):\n    \"\"\"Check if representations are collapsing\"\"\"\n    # Variance across batch\n    variance = torch.var(z, dim=0).mean()\n\n    # Std across batch\n    std = torch.std(z, dim=0).mean()\n\n    print(f\"Variance: {variance:.4f}\")\n    print(f\"Std: {std:.4f}\")\n\n    if variance < 0.001:\n        print(\"WARNING: Representations collapsing!\")\n        return False\n    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# During training\nfor z_i, z_j in batches:\n    if not monitor_collapse(z_i):\n        # Take corrective action\n        # Adjust learning rate, batch size, etc.\n        pass\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3: Slow convergence\n\n**Causes:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n① Learning rate too small\n   → Gradients don't produce meaningful updates\n   → Training takes forever\n\n② Too few negatives\n   → Weak learning signal\n   → Takes many steps to learn\n\n③ Bad data augmentation\n   → Positive pairs too similar/different\n   → Model confused about what to learn\n\n④ Model too complex\n   → Slow to train\n   → Consider simpler architecture\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solutions:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Learning rate warmup\n   Gradually increase LR from 0 to target\n   Helps with stability\n\n   Schedule:\n   LR(t) = target_lr * min(1, t / warmup_steps)\n\n2. Learning rate scheduling\n   Reduce LR as training progresses\n   Helps fine-tuning\n\n   CosineAnnealingLR: Common choice\n\n3. Increase batch size\n   If hardware permits\n   Each sample gets more negatives\n   Stronger learning signal\n\n4. Use momentum\n   Keep moving average of gradients\n   Smooths noisy gradient signal\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Contrastive learning** learns from similarity/dissimilarity without labels\n- **InfoNCE loss** is the foundation: maximize positive similarity relative to negatives\n- **CLIP** revolutionized the field with language-grounded vision at scale\n- **Temperature** controls softmax sharpness and learning signal\n- **Self-supervised variants** (SimCLR, MoCo, BYOL) enable learning from unlabeled data\n- **Large batch size** provides more negatives and stronger signal\n- **Hyperparameter tuning** (temperature, batch size, augmentation) is crucial\n- **Representation collapse** is a real risk to monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n**⭐ Beginner:**\n1. Implement InfoNCE loss from scratch\n2. Compute temperature effects on loss\n3. Understand positive/negative pairs in a batch\n\n**⭐⭐ Intermediate:**\n4. Build image-text contrastive model on small dataset\n5. Implement temperature scheduling\n6. Compare different similarity metrics\n\n**⭐⭐⭐ Advanced:**\n7. Implement SimCLR with proper augmentations\n8. Build MoCo with momentum encoder\n9. Debug and fix representation collapse\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Interactive Demo: Mini-CLIP Implementation\n\n",
        "Let's implement a simplified version of CLIP to understand contrastive learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MiniCLIP(nn.Module):\n",
        "    def __init__(self, image_dim=512, text_dim=512, embed_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Image encoder (simplified)\n",
        "        self.image_encoder = nn.Sequential(\n",
        "            nn.Linear(image_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, embed_dim),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Text encoder (simplified)\n",
        "        self.text_encoder = nn.Sequential(\n",
        "            nn.Linear(text_dim, 512),\n",
        "            nn.ReLU(), \n",
        "            nn.Linear(512, embed_dim),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Learnable temperature parameter\n",
        "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "    \n",
        "    def encode_image(self, image_features):\n",
        "        return F.normalize(self.image_encoder(image_features), dim=-1)\n",
        "    \n",
        "    def encode_text(self, text_features):\n",
        "        return F.normalize(self.text_encoder(text_features), dim=-1)\n",
        "    \n",
        "    def forward(self, image_features, text_features):\n",
        "        # Get normalized embeddings\n",
        "        image_embeds = self.encode_image(image_features)\n",
        "        text_embeds = self.encode_text(text_features)\n",
        "        \n",
        "        # Compute similarity matrix\n",
        "        logits = torch.matmul(image_embeds, text_embeds.T) * self.temperature.exp()\n",
        "        \n",
        "        return logits, image_embeds, text_embeds\n",
        "\n",
        "# Initialize model\n",
        "model = MiniCLIP()\n",
        "print(\"✅ Mini-CLIP model created!\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contrastive_loss(logits):\n",
        "    \"\"\"Compute symmetric contrastive loss.\"\"\"\n",
        "    batch_size = logits.shape[0]\n",
        "    labels = torch.arange(batch_size)\n",
        "    \n",
        "    # Image-to-text loss\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    \n",
        "    # Text-to-image loss  \n",
        "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
        "    \n",
        "    return (loss_i2t + loss_t2i) / 2\n",
        "\n",
        "# Demo with synthetic data\n",
        "batch_size = 8\n",
        "image_features = torch.randn(batch_size, 512)\n",
        "text_features = torch.randn(batch_size, 512)\n",
        "\n",
        "# Forward pass\n",
        "logits, image_embeds, text_embeds = model(image_features, text_features)\n",
        "loss = contrastive_loss(logits)\n",
        "\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Contrastive loss: {loss.item():.4f}\")\n",
        "print(f\"Temperature: {model.temperature.exp().item():.4f}\")\n",
        "\n",
        "# Visualize similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(logits.detach().numpy(), cmap='viridis')\n",
        "plt.colorbar(label='Similarity Score')\n",
        "plt.title('Image-Text Similarity Matrix')\n",
        "plt.xlabel('Text Index')\n",
        "plt.ylabel('Image Index')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Exercise: Understanding Contrastive Learning\n\n",
        "Try these experiments to deepen your understanding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Effect of temperature on similarity\n",
        "def experiment_temperature():\n",
        "    temperatures = [0.01, 0.07, 0.1, 0.5, 1.0]\n",
        "    \n",
        "    plt.figure(figsize=(15, 3))\n",
        "    \n",
        "    for i, temp in enumerate(temperatures):\n",
        "        model.temperature.data = torch.log(torch.tensor(1 / temp))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits, _, _ = model(image_features, text_features)\n",
        "        \n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(logits.numpy(), cmap='viridis')\n",
        "        plt.title(f'τ = {temp}')\n",
        "        plt.colorbar()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Effect of Temperature on Similarity Matrix', y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "# Run experiment\n",
        "experiment_temperature()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Training simulation\n",
        "def simulate_training(num_steps=100):\n",
        "    model = MiniCLIP()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Generate synthetic paired data\n",
        "        image_feat = torch.randn(8, 512)\n",
        "        text_feat = torch.randn(8, 512)\n",
        "        \n",
        "        # Add some correlation to make the task learnable\n",
        "        noise = torch.randn(8, 512) * 0.5\n",
        "        text_feat = 0.7 * image_feat + 0.3 * noise\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits, _, _ = model(image_feat, text_feat)\n",
        "        loss = contrastive_loss(logits)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "    \n",
        "    # Plot training curve\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Show final similarity matrix\n",
        "    with torch.no_grad():\n",
        "        final_logits, _, _ = model(image_feat, text_feat)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(final_logits.numpy(), cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Final Similarity Matrix')\n",
        "    plt.xlabel('Text Index')\n",
        "    plt.ylabel('Image Index')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Run training simulation\n",
        "print(\"🏃‍♂️ Running training simulation...\")\n",
        "trained_model = simulate_training()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}