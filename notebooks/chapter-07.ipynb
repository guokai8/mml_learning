{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Contrastive Learning\n\n**Interactive Jupyter Notebook Version**\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Contrastive Learning\n\n---\n\n**Previous**: [Chapter 6: Attention Mechanisms in Multimodal Systems](chapter-06.md) | **Next**: [Chapter 8: Transformer Architecture](chapter-08.md) | **Home**: [Table of Contents](index.md)\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand contrastive learning principles and motivation\n- Implement InfoNCE loss\n- Understand CLIP's revolutionary approach\n- Compare different contrastive methods\n- Apply contrastive learning to your own problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 The Problem Contrastive Learning Solves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Traditional Supervised Learning\n\n**Standard approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTraining data: (input, label) pairs\n\nTask: Image classification\n  Input: Image\n  Label: \"cat\" or \"dog\"\n\n  Process:\n  â‘  Pass image through network\n  â‘¡ Output logits for each class\n  â‘¢ Cross-entropy loss compares to label\n  â‘£ Backprop updates weights\n\nRequirements:\n  âœ— Requires labels for everything\n  âœ— Labels are expensive (human annotation)\n  âœ— Limited to labeled dataset size\n  âœ— New task = new labeled data needed\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Bottleneck in practice:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nProblem: Most data is unlabeled\n\nExample:\n  ImageNet: 1.4M labeled images\n  Internet: Billions of images daily\n\n  Ratio: ~1 labeled per 1 million unlabeled!\n\nQuestion: How to leverage the vast unlabeled data?\n\nTraditional supervised learning: Can't use it!\nSolution: Contrastive learning\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-Supervised Learning Intuition\n\n**Key insight:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nDon't need explicit labels!\nCreate labels from data itself using natural structure\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example - Image rotation prediction:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nUnlabeled image:\n  [Photo of cat]\n\nCreate self-supervised task:\n  Rotate image 90Â°\n\n  Rotated image â†’ Network â†’ Predict rotation\n\nLabel is free! (We created it by rotation)\n\nTraining:\n  â‘  Rotate image by random angle (0Â°, 90Â°, 180Â°, 270Â°)\n  â‘¡ Network predicts angle\n  â‘¢ Loss: Cross-entropy between predicted and actual angle\n\nResult:\n  Network learns visual representations\n  Without any human labels!\n\nBenefit:\n  Can train on billions of unlabeled images\n  Representations useful for downstream tasks\n  Transfer to real tasks with small labeled dataset\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this works:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTo predict rotation, network must understand:\n  - What's the \"up\" direction? (spatial orientation)\n  - What are objects and their structure? (semantics)\n  - What's foreground vs background? (attention)\n\nThese are useful representations for other tasks!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contrastive Learning Idea\n\n**Core concept:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSupervised learning: \"Is this input A or B or C?\"\nContrastive learning: \"Which B is similar to A?\"\n\nExample:\n  Supervised:      \"Is this a dog?\" (Yes/No)\n  Contrastive:     \"Given this dog photo, which text matches best?\n                    A) 'A dog running'\n                    B) 'A cat sleeping'\n                    C) 'A car parked'\"\n\nContrastive doesn't need explicit labels\nJust needs relative similarities!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why it's powerful:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nAdvantage 1: No labels needed\n  âœ“ Use unlabeled data directly\n  âœ“ Billions of image-text pairs from web\n  âœ“ Much cheaper than labeling\n\nAdvantage 2: Richer signal\n  Binary classification: Yes/No (1 bit)\n  Contrastive: Ranking among many (logâ‚‚(N) bits)\n\n  With N=1000 options:\n  Ranking gives ~10 bits of information\n  vs 1 bit for binary\n\nAdvantage 3: Metric learning\n  Directly optimize for similarity\n  Better representations for retrieval\n  Natural distance metrics\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 InfoNCE Loss - The Foundation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Loss\n\n**Name breakdown:**\n- **Info** = Information theory\n- **NCE** = Noise Contrastive Estimation\n\n**Goal:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nMake positive pairs similar\nMake negative pairs dissimilar\n\nPositive pair: (cat image, \"cat\" text)\nNegative pair: (cat image, \"dog\" text)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Formulation\n\n**Formula:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nL = -log [ exp(sim(q,k+)/Ï„) / (exp(sim(q,k+)/Ï„) + Î£â±¼ exp(sim(q,kâ»â±¼)/Ï„)) ]\n\nBreakdown:\n\nq = query (e.g., image)\nk+ = positive key (e.g., matching text)\nkâ»â±¼ = negative keys (non-matching texts)\nÏ„ = temperature (controls sharpness)\nsim = similarity function (cosine, dot product)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step-by-step explanation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStep 1: Compute similarities\n  sim(query, positive) = dot product\n  sim(query, negativeâ‚) = dot product\n  sim(query, negativeâ‚‚) = dot product\n  ...\n\n  Result: Scores (could be any value)\n\nStep 2: Scale by temperature\n  Score / Ï„\n\n  Temperature effect:\n    Ï„ small (0.01): Scores become extreme\n    Ï„ normal (0.1): Moderate scaling\n    Ï„ large (1.0): Minimal scaling\n\n  Why temperature?\n    Prevents softmax from being too sharp\n    Allows gradient flow during training\n\nStep 3: Exponential\n  exp(score / Ï„)\n\n  Result: All positive (e^x > 0 for all x)\n\n  Effect:\n    Larger scores â†’ larger exponents\n    Softmax then emphasizes them\n\nStep 4: Softmax (normalize)\n  exp(positive) / (exp(positive) + Î£ exp(negatives))\n\n  Result: Probability in [0, 1]\n\n  Interpretation:\n    Probability that positive is highest ranked\n    Perfect: Probability = 1.0\n    Random: Probability = 1/(1+num_negatives)\n\nStep 5: Negative log\n  -log(probability)\n\n  If probability = 1.0: loss = 0 (perfect!)\n  If probability = 0.1: loss = -log(0.1) = 2.3 (bad)\n  If probability = 0.5: loss = -log(0.5) = 0.69 (medium)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Example\n\n**Setup:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nQuery: Image of red cat\nPositive: Text \"a red cat\"\nNegatives:\n  - \"a blue dog\"\n  - \"a green parrot\"\n  - \"a car\"\n\nSimilarities (before temperature):\n  sim(query, positive) = 0.8    (high, should be!)\n  sim(query, neg1) = 0.2        (low, good)\n  sim(query, neg2) = 0.15       (low, good)\n  sim(query, neg3) = 0.1        (low, good)\n\nTemperature Ï„ = 0.1\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Computing loss:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStep 1: Scale by temperature\n  0.8 / 0.1 = 8.0\n  0.2 / 0.1 = 2.0\n  0.15 / 0.1 = 1.5\n  0.1 / 0.1 = 1.0\n\nStep 2: Exponentials\n  e^8.0 â‰ˆ 2981\n  e^2.0 â‰ˆ 7.4\n  e^1.5 â‰ˆ 4.5\n  e^1.0 â‰ˆ 2.7\n\nStep 3: Softmax (probability)\n  2981 / (2981 + 7.4 + 4.5 + 2.7)\n  = 2981 / 2995.6\n  â‰ˆ 0.995   (99.5% probability positive is best!)\n\nStep 4: Loss\n  loss = -log(0.995) â‰ˆ 0.005   (very small! Model doing great)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What if model was bad:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSimilarities:\n  sim(query, positive) = 0.1    (low! bad!)\n  sim(query, neg1) = 0.5        (high! worse)\n  sim(query, neg2) = 0.4\n  sim(query, neg3) = 0.3\n\nAfter temperature scaling (Ï„ = 0.1):\n  0.1 / 0.1 = 1.0     â†’ e^1.0 â‰ˆ 2.7\n  0.5 / 0.1 = 5.0     â†’ e^5.0 â‰ˆ 148\n  0.4 / 0.1 = 4.0     â†’ e^4.0 â‰ˆ 55\n  0.3 / 0.1 = 3.0     â†’ e^3.0 â‰ˆ 20\n\nSoftmax:\n  2.7 / (2.7 + 148 + 55 + 20)\n  = 2.7 / 225.7\n  â‰ˆ 0.012   (1.2% probability - terrible!)\n\nLoss:\n  -log(0.012) â‰ˆ 4.4   (very large! Forces update)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why This Works\n\n**Mathematical properties:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Bounded between 0 and log(1+N)\n   where N = number of negatives\n\n   N=10: Loss âˆˆ [0, log(11) â‰ˆ 2.4]\n   N=100: Loss âˆˆ [0, log(101) â‰ˆ 4.6]\n\n   Interpretable scale\n\n2. Gradient is informative\n\n   Perfect case (prob â‰ˆ 1): gradient â‰ˆ 0\n   Good case (prob â‰ˆ 0.9): gradient â‰ˆ small\n   Bad case (prob â‰ˆ 0.1): gradient â‰ˆ large\n\n   Automatically focuses on hard cases\n\n3. Invariant to scale\n\n   If all similarities multiplied by constant K:\n   exp(K*sim) has same relative ordering\n   Softmax still works correctly\n\n   Enables using unnormalized similarities\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Temperature Parameter\n\n**Role of Ï„:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTemperature controls softmax sharpness\n\nÏ„ = 0.01 (very cold):\n  Softmax becomes nearly one-hot\n  exp(5) = 148\n  exp(4) = 55\n  exp(3) = 20\n  Ratio: 148/55 = 2.7x difference\n\n  Large differences between outputs\n  Large gradients\n  Potential instability\n\nÏ„ = 0.1 (standard):\n  Moderate softmax\n  exp(0.5) = 1.65\n  exp(0.4) = 1.49\n  exp(0.3) = 1.35\n  Ratio: 1.65/1.49 = 1.1x difference\n\n  Balanced gradients\n  Stable training\n  Common choice\n\nÏ„ = 1.0 (very hot):\n  Softmax becomes smooth\n  exp(0.05) = 1.05\n  exp(0.04) = 1.04\n  exp(0.03) = 1.03\n  Ratio: 1.05/1.04 â‰ˆ 1.01x difference\n\n  Small differences between outputs\n  Small gradients\n  Slow learning\n\nÏ„ = 10.0 (extremely hot):\n  Softmax nearly uniform\n  All classes almost equally likely\n  Nearly no signal\n  Training doesn't work\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Effect on learning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nOptimal temperature depends on:\n  - Number of negatives\n  - Difficulty of task\n  - Data quality\n\nTypical range: Ï„ âˆˆ [0.05, 0.2]\n\nCLIP uses: Ï„ â‰ˆ 0.07 (learned during training)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 CLIP - Contrastive Learning Success Story"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Context and Impact\n\n**Problem statement (2020):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nExisting vision models:\n  - Trained on ImageNet (1.4M images)\n  - Limited to 1000 classes\n  - Can't generalize to new concepts\n  - Require supervised fine-tuning\n\nQuestion:\n  Can we use web data (unsupervised) for vision?\n  Can we match NLP's success with massive unlabeled data?\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CLIP solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nData: 400M image-caption pairs from web\nTask: Learn from natural language supervision\nMethod: Contrastive learning on image-text pairs\n\nResult: Revolutionary zero-shot transfer\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CLIP Architecture\n\n**Components:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImage encoder:           Text encoder:\n  Vision Transformer      Transformer (BERT-like)\n  Input: 224Ã—224 image    Input: Text tokens\n  Output: 512D vector     Output: 512D vector\n\n            â†“                     â†“\n\n    [Normalize to unit length]\n\n            â†“                     â†“\n\n    Similarity computation (dot product of normalized)\n\n            â†“\n\n    Contrastive loss\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data collection:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n400 million image-caption pairs from internet\n\nSources:\n  - Web pages with images and captions\n  - Publicly available image databases\n  - Social media posts with text\n  - Stock photo sites with descriptions\n\nQuality:\n  - Uncurated and diverse\n  - Contains noise and biases\n  - Reflects web distribution\n  - Natural language (not formal labels)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Process\n\n**Batch construction:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nBatch size: 32,768 (massive!)\n\nImages: [img_1, img_2, ..., img_32k]\nCaptions: [caption_1, caption_2, ..., caption_32k]\n\nEncode all:\n  Image embeddings: 32k Ã— 512\n  Caption embeddings: 32k Ã— 512\n\nCompute similarity matrix (32k Ã— 32k):\n  sim[i,j] = image_i Â· caption_j\n\nGoal:\n  Diagonal elements high (matched pairs)\n  Off-diagonal elements low (mismatched pairs)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Loss computation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nFor each image:\n  Compute InfoNCE loss\n  Positive: matching caption\n  Negatives: all other 32k-1 captions\n\nFor each caption:\n  Compute InfoNCE loss\n  Positive: matching image\n  Negatives: all other 32k-1 images\n\nTotal loss = average of all losses\n\nOptimization:\n  Adam optimizer\n  Learning rate: 5Ã—10â»â´\n  Training: ~2 weeks on large clusters\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Transfer - Revolutionary Capability\n\n**Traditional approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNew task: Classify images of birds (not in ImageNet)\n\nSteps:\n  1. Get labeled training data for birds\n  2. Fine-tune ImageNet model\n  3. Get predictions\n\nProblem: Need labeled bird data!\nCost: Expensive annotation\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CLIP zero-shot approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNew task: Classify images of birds\n\nNo training needed!\n\nSteps:\n  1. Text prompts: \"a photo of a bird\"\n                   \"a photo of a person\"\n                   \"a photo of a car\"\n\n  2. Encode each prompt with CLIP text encoder\n     â†’ 512D vectors\n\n  3. For test image:\n     - Encode with CLIP image encoder\n     - Compute similarity to each prompt\n     - Select highest similarity\n\n  4. Done! Zero-shot classification\n\nExample:\n  Image similarity scores:\n    \"a photo of a bird\": 0.92    â† Highest\n    \"a photo of a person\": 0.15\n    \"a photo of a car\": 0.08\n\n  Prediction: Bird\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why it works:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCLIP trained on 400M diverse image-caption pairs\nLearned that:\n  - Images with birds cluster with \"bird\" text\n  - Images with people cluster with \"person\" text\n  - Images with cars cluster with \"car\" text\n\nThese mappings generalize to new images!\n\nTransfer learning without fine-tuning:\n  - No labeled data needed\n  - No training required\n  - Immediate deployment\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Benchmark Results\n\n**Zero-shot transfer (ImageNet classification):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTraditional supervised:\n  ResNet-50: 76.1% accuracy\n\nCLIP zero-shot:\n  CLIP-ViT-L/14: 62.8% accuracy\n\nSeems lower, BUT:\n  - CLIP trained on NO labeled images\n  - Just 400M raw internet data\n  - Immediately applicable to any category\n  - ResNet trained with 1.4M labeled ImageNet\n\nAdjusted for training data:\n  ResNet: 76.1% on specific dataset\n  CLIP: 62.8% on ANY dataset (zero-shot)\n\n  CLIP more generalizable!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**After fine-tuning on small labeled sets:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImageNet (1% labeled):\n  CLIP: 76.3% accuracy\n\nComparison:\n  - CLIP fine-tuned with 1% labels â‰ˆ ResNet with 100% labels\n  - 100Ã— more data-efficient!\n  - Shows power of pre-training\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Other domains:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nTransfer to new datasets:\n\nSTL10 (airplane, bird, car, etc.):\n  CLIP: 92.9% zero-shot\n\nFood101 (food classification):\n  CLIP: 88.3% zero-shot\n\nEuroSAT (satellite imagery):\n  CLIP: 58.4% zero-shot\n\nWorks across diverse domains!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why CLIP is Revolutionary\n\n**1. Scale:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n400M image-text pairs >> 1.4M ImageNet\nShows power of scale in representation learning\nUnlabeled data is abundant!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Natural supervision:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLanguage is natural way to describe images\nNot forced to 1000 classes like ImageNet\nFlexible descriptors\nCan specify any attribute\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Zero-shot transfer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNo fine-tuning needed\nImmediate deployment\nNo labeled data required\nGeneralizes across domains\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Open-ended prediction:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nNot limited to predefined classes\nCan describe images with any text\n\"A cat wearing a hat\"\n\"A red car on a mountain\"\nAny description works!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impact on Field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCLIP (April 2021) was watershed moment\n\nBefore CLIP:\n  - Supervised learning paradigm dominant\n  - Limited to ImageNet 1000 classes\n  - Required labeled data for new tasks\n  - Struggled on out-of-distribution data\n\nAfter CLIP:\n  - Contrastive learning became mainstream\n  - Foundation model era began\n  - Zero-shot transfer became practical\n  - Industry adopted language-grounded vision\n\nInspired:\n  - ALIGN (Google)\n  - LiT (Google)\n  - COCA (Meta)\n  - Flamingo (DeepMind)\n  - BLIP (Salesforce)\n  - Many others...\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Variants and Extensions of Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: SimCLR - Self-Supervised Vision\n\n**Motivation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCLIP uses text for supervision\nWhat if we only have unlabeled images?\n\nAnswer: Use image augmentations as \"supervision\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Core idea:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSingle image:\n  [Original cat photo]\n\nCreate two augmented versions:\n  [Rotated, cropped, color-adjusted]\n  [Different rotation, crop, colors]\n\nTreat as positive pair:\n  Both should have similar representations\n  (Same cat, different augmentations)\n\nNegatives:\n  Other images in batch\n\nLoss: Make augmentations similar,\n      other images dissimilar\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Process:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Sample image x from dataset\n\n2. Create two augmented versions:\n   x_i = Aug(x)  (augmentation 1)\n   x_j = Aug(x)  (augmentation 2)\n\n   Different random augmentations!\n\n3. Encode both through network f:\n   h_i = f(x_i)\n   h_j = f(x_j)\n\n4. Project to embedding space:\n   z_i = g(h_i)\n   z_j = g(h_j)\n\n5. Contrastive loss:\n   sim(z_i, z_j) should be high\n   sim(z_i, z_k) should be low (for k â‰  i,j)\n\n6. Backprop updates f and g\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWhy this works:\n\nAssumptions:\n  1. Augmentations preserve content\n  2. Different images are different\n\nImplications:\n  Model learns representations that:\n  - Survive augmentations (robust features)\n  - Differ between images (discriminative features)\n  - Capture semantic content (not style)\n\nResult:\n  Representations useful for downstream tasks\n  Without any labels!\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Augmentations used:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nStrong augmentations needed for self-supervised learning:\n\nRandom crop:\n  (up to 85% crop)\n  â†‘ Forces learning of part representations\n\nColor jittering:\n  Brightness, contrast, saturation, hue\n  â†‘ Prevents learning from color only\n\nGaussian blur:\n  Blurs fine details\n  â†‘ Forces learning of structure, not pixels\n\nRandom grayscale:\n  Removes color information\n  â†‘ Forces learning of shape and texture\n\nGaussian noise:\n  Adds random noise\n  â†‘ Makes features robust\n\nNote: Extreme augmentations avoid (would destroy content)\n  - Extreme rotation: Flips meaning\n  - Extreme scaling: Makes object invisible\n  - Extreme distortion: No longer recognizable\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Differences from CLIP:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n                SimCLR          CLIP\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSupervision     Image augment   Text\nData            Unlabeled       Image-caption pairs\nRequires        Images only     Images + text\nGeneralization  Moderate        Excellent\nTask alignment  Generic vision  Language grounding\nTransfer        Good            Excellent\nInterpretable   No              Yes (language)\n\nWhen to use:\n  SimCLR: When you only have unlabeled images\n  CLIP: When you have image-caption pairs\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: MoCo - Momentum Contrast\n\n**Problem with SimCLR:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nSimCLR requires large batch size:\n  - Small batch: Few negatives â†’ weak learning signal\n  - Large batch: Better negatives â†’ better learning\n\n  Batch size 4096 requires massive GPU memory\n  And distributed training complexity\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MoCo solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nUse memory bank instead of current batch\n\nBenefits:\n  âœ“ Can use smaller batch size\n  âœ“ Negatives more diverse (from different times)\n  âœ“ More efficient\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nOnline encoder: f_q\n  Learns from current batch\n  Updated every step\n\nMemory bank: Queue\n  Stores recent representations\n  Old representations pushed out as new added\n\nMomentum encoder: f_k\n  Slowly following online encoder\n  f_k = Î± Ã— f_k + (1-Î±) Ã— f_q\n\n  Typically Î± = 0.999\n  Moves slowly (momentum!)\n\nProcess:\n\n1. Current batch through online encoder\n   â†’ query embeddings q\n\n2. Pop old representations from queue\n   â†’ memory negatives\n\n3. Compute loss using:\n   - query from online encoder (positive)\n   - memory from momentum encoder (negatives)\n\n4. Push new representations to queue\n\n5. Update momentum encoder (slowly follows online)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why momentum encoder:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWithout it:\n  Queue contains representations from old network\n  Network keeps changing â†’ representations inconsistent\n  Training unstable\n\nWith momentum encoder:\n  Queue contains representations from slow network\n  Representations are consistent\n  Training stable\n\nEffect:\n  Momentum = inertia\n  Small updates accumulate\n  Smooth trajectory\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Performance:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nImageNet pre-training â†’ transfer to other tasks\n\n                Top-1 Accuracy\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSupervised      76.5% (ResNet-50)\nSimCLR          69.3% (requires large batch)\nMoCo v1         60.6% (with 65K negatives)\nMoCo v2         71.3% (improved version)\nMoCo v3         76.7% (vision transformer)\n\nNote: Self-supervised eventually matched supervised!\n      Shows power of approach\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 3: BYOL - Contrastive Without Negatives\n\n**Surprising finding (Grill et al., 2020):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nDo we even need negative examples?\n\nTraditional contrastive:\n  Make positives similar\n  Make negatives dissimilar\n\nBYOL:\n  Only make positives similar\n  No explicit negatives!\n\nQuestion: How does this work?\n\nAnswer: Still has implicit negatives\n        (Through model architecture and learning dynamics)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nOnline network:\n  Encoder f + Projector g\n  Input: image â†’ output: representation\n  Updated every step\n\nTarget network:\n  Copy of online network\n  Parameter updates: EMA (exponential moving average)\n  target_param = Î± Ã— target_param + (1-Î±) Ã— online_param\n\nPredictor h:\n  Additional MLP on top of online network\n  NOT on target network (asymmetry!)\n\nLoss:\n  For two augmentations of same image:\n  loss = ||h(online(aug1)) - target(aug2)||Â²\n\n  Make online and target predictions close\n  Using MSE loss (not contrastive!)\n\n  Also symmetrically:\n  loss += ||h(online(aug2)) - target(aug1)||Â²\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this works (still debated!):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nPossible explanations:\n\n1. Implicit negatives through optimization\n   - Mini-batch gradient descent creates diversity\n   - Network can't collapse to constant\n   - Similar to negative mining\n\n2. Momentum encoder provides stability\n   - Target network changes slowly\n   - Creates effective \"negatives\" through difference\n\n3. Predictor prevents mode collapse\n   - Without predictor: Would learn trivial solution\n   - With predictor: Breaks symmetry\n   - Forces meaningful learning\n\nEmpirical results:\n  BYOL works surprisingly well!\n  Without explicit negatives!\n  Counterintuitive but effective\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Advantages:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nâœ“ Doesn't need negative pairs\nâœ“ Don't need image-text pairs (image-only sufficient)\nâœ“ Works with small batches\nâœ“ Stable training\nâœ“ Strong performance (competitive with SimCLR)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Disadvantages:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nâœ— Why it works still not fully understood\nâœ— Less interpretable\nâœ— More complex architecture\nâœ— Harder to debug when it fails\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Practical Guide to Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Contrastive Learning\n\n**Basic template:**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContrastiveLearningModel(nn.Module):\n    def __init__(self, encoder, projection_dim=256):\n        super().__init__()\n        self.encoder = encoder\n        self.projector = nn.Linear(encoder.output_dim, projection_dim)\n\n    def forward(self, x):\n        # Encode\n        h = self.encoder(x)\n\n        # Project\n        z = self.projector(h)\n\n        # Normalize\n        z = F.normalize(z, p=2, dim=1)\n\n        return z\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        Compute NT-Xent loss\n        z_i, z_j: (batch_size, embedding_dim) tensors\n        \"\"\"\n        batch_size = z_i.shape[0]\n\n        # Concatenate: positive pairs are diagonal\n        z = torch.cat([z_i, z_j], dim=0)  # (2*batch, dim)\n\n        # Similarity matrix\n        similarity = torch.mm(z, z.t()) / self.temperature\n\n        # Create labels: diagonal elements are positives\n        labels = torch.arange(batch_size, device=z.device)\n        labels = torch.cat([labels, labels])\n\n        # Positive pairs at positions (i, batch+i) and (batch+i, i)\n        # Compute loss: each sample should match its pair\n\n        # Loss for all positions\n        loss = F.cross_entropy(similarity, labels)\n\n        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop\ndef train_contrastive(model, data_loader, optimizer, device, epochs=100):\n    criterion = ContrastiveLoss(temperature=0.07)\n\n    for epoch in range(epochs):\n        total_loss = 0\n\n        for images in data_loader:\n            # Get two augmented versions\n            x_i = augment(images)\n            x_j = augment(images)\n\n            x_i = x_i.to(device)\n            x_j = x_j.to(device)\n\n            # Forward pass\n            z_i = model(x_i)\n            z_j = model(x_j)\n\n            # Compute loss\n            loss = criterion(z_i, z_j)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(data_loader)\n        print(f\"Epoch {epoch}: Loss = {avg_loss:.3f}\")\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing Hyperparameters\n\n**Temperature:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nRange: [0.05, 0.2]\n\nDiagnostic:\n  Training loss plateaus at high value?\n    â†’ Temperature too low (sharp, unstable)\n    â†’ Increase Ï„\n\n  Training loss decreases but very slowly?\n    â†’ Temperature too high (smooth, weak signal)\n    â†’ Decrease Ï„\n\nRule of thumb:\n  Start with Ï„ = 0.1\n  Adjust based on loss curve\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Batch size:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nLarger batch = more negatives = better signal\n\nTypical choices:\n  Small GPU: 256-512\n  Medium GPU: 1024-2048\n  Large GPU: 4096+\n  Multi-GPU: 32K+ (like CLIP)\n\nTrade-off:\n  Larger batch: Better learning, slower per epoch\n  Smaller batch: Worse learning, faster per epoch\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Projection dimension:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nEmbedding dimension (before projection): 1024-2048 (from encoder)\nProjection dimension: 128-512\n\nCommon choices:\n  256D (standard)\n  128D (more compression)\n  512D (less compression)\n\nEffect:\n  Smaller: Faster computation, less memory\n  Larger: More expressive, risk of overfitting\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Number of negatives:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nWithin batch:\n  Batch size 256 â†’ 255 negatives per sample\n\nMemory bank (MoCo):\n  Queue size 65536 â†’ 65535 negatives\n\nMore negatives â†’ better learning signal\nBut more computation\nTypical: 255-65K negatives\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Contrastive Models\n\n**Method 1: Linear evaluation protocol**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Train contrastive model on unlabeled data\n   â†’ Get representations\n\n2. Freeze encoder\n   â†’ Don't update weights\n\n3. Train linear classifier on representations\n   â†’ Small labeled dataset\n\n4. Evaluate on test set\n\nMetric: Accuracy of linear classifier\nInsight: If representations good â†’ linear classifier accurate\n\nExample:\n  CIFAR-10 (50K training images)\n  Contrastive pre-training: All 50K unlabeled\n  Linear eval: 5K labeled for training, 10K for testing\n\n  Result: 96% accuracy\n  Interpretation: Representations capture meaningful patterns\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Method 2: Transfer learning evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Train contrastive model on source dataset\n2. Fine-tune on target task\n3. Compare to:\n   - Supervised baseline\n   - Random initialization\n   - Other pre-training methods\n\nMetric: Downstream task accuracy\nInsight: Better representations â†’ better transfer\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Method 3: Downstream task performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nPre-training dataset: ImageNet (unlabeled contrastive)\nDownstream tasks:\n  1. ImageNet-100 classification (supervised fine-tune)\n  2. CIFAR-10 classification\n  3. STL10 classification\n  4. Transfer to object detection\n  5. Transfer to segmentation\n\nResults show generalization across tasks\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Troubleshooting Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 1: Loss not decreasing\n\n**Potential causes:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nâ‘  Temperature too low\n   Effect: Softmax too sharp\n   Solution: Increase Ï„ (e.g., 0.1 â†’ 0.2)\n\nâ‘¡ Learning rate too small\n   Effect: Updates too tiny\n   Solution: Increase learning rate\n\nâ‘¢ Batch size too small\n   Effect:\n\n-----\n\n> Continue\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Effect: Weak learning signal\n   Solution: Increase batch size if possible\n\nâ‘£ Bad initialization\n   Effect: Starting in bad local minimum\n   Solution: Use proper weight initialization\n\nâ‘¤ Augmentations too weak\n   Effect: Positive pairs too similar anyway\n   Solution: Increase augmentation strength\n\nâ‘¥ Augmentations too strong\n   Effect: Positive pairs become different objects\n   Solution: Decrease augmentation strength"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n\n**Debugging steps:**\n\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Check loss values\nprint(f\"Initial loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should decrease over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# If increasing or constant: something wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Check similarity matrix\nsimilarity = torch.mm(z, z.t())\nprint(f\"Max similarity: {similarity.max():.3f}\")\nprint(f\"Min similarity: {similarity.min():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should: Max â‰ˆ 1, Min â‰ˆ -1 for normalized vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Check gradients\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name}: grad_norm={param.grad.norm():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should be reasonable values (not 0, not inf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Check temperature effect\ntemperatures = [0.01, 0.05, 0.1, 0.2, 0.5]\nfor tau in temperatures:\n    loss = compute_loss(embeddings, tau)\n    print(f\"Ï„={tau}: loss={loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Should have sweet spot, not too high/low everywhere\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 2: Representation collapse\n\n**What is it:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nModel learns to make all representations nearly identical\n\nExample:\n  All images â†’ representation [0.5, 0.5, 0.5, ...]\n  All images â†’ representation [0.51, 0.49, 0.50, ...]\n\n  Trivial solution: \"All same = all similar\"\n  Loss can be artificially low!\n  But representations useless for downstream tasks\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Symptoms:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nâœ“ Loss decreasing nicely\nâœ— Linear evaluation performance poor\nâœ— Representations clustered at single point\nâœ— Variance of representations near zero\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Causes and solutions:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nCause 1: No negatives (only positives)\n  Solution: Ensure you have negatives in batch\n\nCause 2: Batch too small\n  Solution: Increase batch size\n\nCause 3: No regularization\n  Solution: Add normalization (L2 normalization helps)\n\nCause 4: Poor augmentations\n  Solution: Ensure augmentations are meaningful\n  (Reproduce the issue with weak augmentations)\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Prevention:**\n\n```python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monitor variance\ndef monitor_collapse(z):\n    \"\"\"Check if representations are collapsing\"\"\"\n    # Variance across batch\n    variance = torch.var(z, dim=0).mean()\n\n    # Std across batch\n    std = torch.std(z, dim=0).mean()\n\n    print(f\"Variance: {variance:.4f}\")\n    print(f\"Std: {std:.4f}\")\n\n    if variance < 0.001:\n        print(\"WARNING: Representations collapsing!\")\n        return False\n    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# During training\nfor z_i, z_j in batches:\n    if not monitor_collapse(z_i):\n        # Take corrective action\n        # Adjust learning rate, batch size, etc.\n        pass\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3: Slow convergence\n\n**Causes:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\nâ‘  Learning rate too small\n   â†’ Gradients don't produce meaningful updates\n   â†’ Training takes forever\n\nâ‘¡ Too few negatives\n   â†’ Weak learning signal\n   â†’ Takes many steps to learn\n\nâ‘¢ Bad data augmentation\n   â†’ Positive pairs too similar/different\n   â†’ Model confused about what to learn\n\nâ‘£ Model too complex\n   â†’ Slow to train\n   â†’ Consider simpler architecture\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solutions:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n1. Learning rate warmup\n   Gradually increase LR from 0 to target\n   Helps with stability\n\n   Schedule:\n   LR(t) = target_lr * min(1, t / warmup_steps)\n\n2. Learning rate scheduling\n   Reduce LR as training progresses\n   Helps fine-tuning\n\n   CosineAnnealingLR: Common choice\n\n3. Increase batch size\n   If hardware permits\n   Each sample gets more negatives\n   Stronger learning signal\n\n4. Use momentum\n   Keep moving average of gradients\n   Smooths noisy gradient signal\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- **Contrastive learning** learns from similarity/dissimilarity without labels\n- **InfoNCE loss** is the foundation: maximize positive similarity relative to negatives\n- **CLIP** revolutionized the field with language-grounded vision at scale\n- **Temperature** controls softmax sharpness and learning signal\n- **Self-supervised variants** (SimCLR, MoCo, BYOL) enable learning from unlabeled data\n- **Large batch size** provides more negatives and stronger signal\n- **Hyperparameter tuning** (temperature, batch size, augmentation) is crucial\n- **Representation collapse** is a real risk to monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n**â­ Beginner:**\n1. Implement InfoNCE loss from scratch\n2. Compute temperature effects on loss\n3. Understand positive/negative pairs in a batch\n\n**â­â­ Intermediate:**\n4. Build image-text contrastive model on small dataset\n5. Implement temperature scheduling\n6. Compare different similarity metrics\n\n**â­â­â­ Advanced:**\n7. Implement SimCLR with proper augmentations\n8. Build MoCo with momentum encoder\n9. Debug and fix representation collapse\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Interactive Demo: Mini-CLIP Implementation\n\n",
        "Let's implement a simplified version of CLIP to understand contrastive learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MiniCLIP(nn.Module):\n",
        "    def __init__(self, image_dim=512, text_dim=512, embed_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Image encoder (simplified)\n",
        "        self.image_encoder = nn.Sequential(\n",
        "            nn.Linear(image_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, embed_dim),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Text encoder (simplified)\n",
        "        self.text_encoder = nn.Sequential(\n",
        "            nn.Linear(text_dim, 512),\n",
        "            nn.ReLU(), \n",
        "            nn.Linear(512, embed_dim),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Learnable temperature parameter\n",
        "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "    \n",
        "    def encode_image(self, image_features):\n",
        "        return F.normalize(self.image_encoder(image_features), dim=-1)\n",
        "    \n",
        "    def encode_text(self, text_features):\n",
        "        return F.normalize(self.text_encoder(text_features), dim=-1)\n",
        "    \n",
        "    def forward(self, image_features, text_features):\n",
        "        # Get normalized embeddings\n",
        "        image_embeds = self.encode_image(image_features)\n",
        "        text_embeds = self.encode_text(text_features)\n",
        "        \n",
        "        # Compute similarity matrix\n",
        "        logits = torch.matmul(image_embeds, text_embeds.T) * self.temperature.exp()\n",
        "        \n",
        "        return logits, image_embeds, text_embeds\n",
        "\n",
        "# Initialize model\n",
        "model = MiniCLIP()\n",
        "print(\"âœ… Mini-CLIP model created!\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contrastive_loss(logits):\n",
        "    \"\"\"Compute symmetric contrastive loss.\"\"\"\n",
        "    batch_size = logits.shape[0]\n",
        "    labels = torch.arange(batch_size)\n",
        "    \n",
        "    # Image-to-text loss\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    \n",
        "    # Text-to-image loss  \n",
        "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
        "    \n",
        "    return (loss_i2t + loss_t2i) / 2\n",
        "\n",
        "# Demo with synthetic data\n",
        "batch_size = 8\n",
        "image_features = torch.randn(batch_size, 512)\n",
        "text_features = torch.randn(batch_size, 512)\n",
        "\n",
        "# Forward pass\n",
        "logits, image_embeds, text_embeds = model(image_features, text_features)\n",
        "loss = contrastive_loss(logits)\n",
        "\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Contrastive loss: {loss.item():.4f}\")\n",
        "print(f\"Temperature: {model.temperature.exp().item():.4f}\")\n",
        "\n",
        "# Visualize similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(logits.detach().numpy(), cmap='viridis')\n",
        "plt.colorbar(label='Similarity Score')\n",
        "plt.title('Image-Text Similarity Matrix')\n",
        "plt.xlabel('Text Index')\n",
        "plt.ylabel('Image Index')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Exercise: Understanding Contrastive Learning\n\n",
        "Try these experiments to deepen your understanding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Effect of temperature on similarity\n",
        "def experiment_temperature():\n",
        "    temperatures = [0.01, 0.07, 0.1, 0.5, 1.0]\n",
        "    \n",
        "    plt.figure(figsize=(15, 3))\n",
        "    \n",
        "    for i, temp in enumerate(temperatures):\n",
        "        model.temperature.data = torch.log(torch.tensor(1 / temp))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits, _, _ = model(image_features, text_features)\n",
        "        \n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(logits.numpy(), cmap='viridis')\n",
        "        plt.title(f'Ï„ = {temp}')\n",
        "        plt.colorbar()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Effect of Temperature on Similarity Matrix', y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "# Run experiment\n",
        "experiment_temperature()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Training simulation\n",
        "def simulate_training(num_steps=100):\n",
        "    model = MiniCLIP()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Generate synthetic paired data\n",
        "        image_feat = torch.randn(8, 512)\n",
        "        text_feat = torch.randn(8, 512)\n",
        "        \n",
        "        # Add some correlation to make the task learnable\n",
        "        noise = torch.randn(8, 512) * 0.5\n",
        "        text_feat = 0.7 * image_feat + 0.3 * noise\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits, _, _ = model(image_feat, text_feat)\n",
        "        loss = contrastive_loss(logits)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "    \n",
        "    # Plot training curve\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Show final similarity matrix\n",
        "    with torch.no_grad():\n",
        "        final_logits, _, _ = model(image_feat, text_feat)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(final_logits.numpy(), cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Final Similarity Matrix')\n",
        "    plt.xlabel('Text Index')\n",
        "    plt.ylabel('Image Index')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Run training simulation\n",
        "print(\"ðŸƒâ€â™‚ï¸ Running training simulation...\")\n",
        "trained_model = simulate_training()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}