<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter-10 – Multimodal Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-56df71c9454ca07313afc907ff0d97f5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Multimodal Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./preface.html"> 
<span class="menu-text">Preface</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./how-to-use.html"> 
<span class="menu-text">How to Use</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li class="dropdown-header">Part I: Foundations</li>
        <li>
    <a class="dropdown-item" href="./chapter-01.html">
 <span class="dropdown-text">Chapter 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-02.html">
 <span class="dropdown-text">Chapter 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-03.html">
 <span class="dropdown-text">Chapter 3</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part II: Core Techniques</li>
        <li>
    <a class="dropdown-item" href="./chapter-04.html">
 <span class="dropdown-text">Chapter 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-05.html">
 <span class="dropdown-text">Chapter 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-06.html">
 <span class="dropdown-text">Chapter 6</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-07.html">
 <span class="dropdown-text">Chapter 7</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part III: Architectures</li>
        <li>
    <a class="dropdown-item" href="./chapter-08.html">
 <span class="dropdown-text">Chapter 8</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-09.html">
 <span class="dropdown-text">Chapter 9</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-10.html">
 <span class="dropdown-text">Chapter 10</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part IV: Practice</li>
        <li>
    <a class="dropdown-item" href="./chapter-11.html">
 <span class="dropdown-text">Chapter 11</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-12.html">
 <span class="dropdown-text">Chapter 12</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="./appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./README.md">
 <span class="dropdown-text">About</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guokai8/mml_learning"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guokai8@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-08.html">Part III: Architectures</a></li><li class="breadcrumb-item"><a href="./chapter-10.html">Chapter 10: Seminal Models and Architectures</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📚 Multimodal Learning: Theory, Practice, and Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./how-to-use.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Use This Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction to Multimodal Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: Foundations and Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 3: Feature Representation for Each Modality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Core Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Feature Alignment and Bridging Modalities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: Fusion Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Attention Mechanisms in Multimodal Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part III: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Generative Models for Multimodal Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Chapter 10: Seminal Models and Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Part IV: Practice</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 11: Practical Implementation Guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 12: Advanced Topics and Future Directions</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comprehensive Appendix and Resources</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-10-seminal-models-and-architectures" id="toc-chapter-10-seminal-models-and-architectures" class="nav-link active" data-scroll-target="#chapter-10-seminal-models-and-architectures"><span class="header-section-number">1</span> Chapter 10: Seminal Models and Architectures</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#clip-learning-transferable-models-from-natural-language-supervision" id="toc-clip-learning-transferable-models-from-natural-language-supervision" class="nav-link" data-scroll-target="#clip-learning-transferable-models-from-natural-language-supervision"><span class="header-section-number">1.2</span> 10.1 CLIP: Learning Transferable Models From Natural Language Supervision</a>
  <ul class="collapse">
  <li><a href="#revolution-in-vision-understanding" id="toc-revolution-in-vision-understanding" class="nav-link" data-scroll-target="#revolution-in-vision-understanding"><span class="header-section-number">1.2.1</span> Revolution in Vision Understanding</a></li>
  <li><a href="#clip-architecture" id="toc-clip-architecture" class="nav-link" data-scroll-target="#clip-architecture"><span class="header-section-number">1.2.2</span> CLIP Architecture</a></li>
  <li><a href="#zero-shot-transfer" id="toc-zero-shot-transfer" class="nav-link" data-scroll-target="#zero-shot-transfer"><span class="header-section-number">1.2.3</span> Zero-Shot Transfer</a></li>
  <li><a href="#benchmark-results" id="toc-benchmark-results" class="nav-link" data-scroll-target="#benchmark-results"><span class="header-section-number">1.2.4</span> Benchmark Results</a></li>
  <li><a href="#impact-on-field" id="toc-impact-on-field" class="nav-link" data-scroll-target="#impact-on-field"><span class="header-section-number">1.2.5</span> Impact on Field</a></li>
  </ul></li>
  <li><a href="#blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders" id="toc-blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders" class="nav-link" data-scroll-target="#blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders"><span class="header-section-number">1.3</span> 10.2 BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders</a>
  <ul class="collapse">
  <li><a href="#context-and-motivation" id="toc-context-and-motivation" class="nav-link" data-scroll-target="#context-and-motivation"><span class="header-section-number">1.3.1</span> Context and Motivation</a></li>
  <li><a href="#blip-2-architecture" id="toc-blip-2-architecture" class="nav-link" data-scroll-target="#blip-2-architecture"><span class="header-section-number">1.3.2</span> BLIP-2 Architecture</a></li>
  <li><a href="#why-it-works" id="toc-why-it-works" class="nav-link" data-scroll-target="#why-it-works"><span class="header-section-number">1.3.3</span> Why It Works</a></li>
  <li><a href="#blip-2-capabilities" id="toc-blip-2-capabilities" class="nav-link" data-scroll-target="#blip-2-capabilities"><span class="header-section-number">1.3.4</span> BLIP-2 Capabilities</a></li>
  <li><a href="#benchmark-results-1" id="toc-benchmark-results-1" class="nav-link" data-scroll-target="#benchmark-results-1"><span class="header-section-number">1.3.5</span> Benchmark Results</a></li>
  </ul></li>
  <li><a href="#gpt-4v-multimodal-reasoning" id="toc-gpt-4v-multimodal-reasoning" class="nav-link" data-scroll-target="#gpt-4v-multimodal-reasoning"><span class="header-section-number">1.4</span> 10.3 GPT-4V: Multimodal Reasoning</a>
  <ul class="collapse">
  <li><a href="#revolutionary-capabilities" id="toc-revolutionary-capabilities" class="nav-link" data-scroll-target="#revolutionary-capabilities"><span class="header-section-number">1.4.1</span> Revolutionary Capabilities</a></li>
  <li><a href="#examples-of-capabilities" id="toc-examples-of-capabilities" class="nav-link" data-scroll-target="#examples-of-capabilities"><span class="header-section-number">1.4.2</span> Examples of Capabilities</a></li>
  <li><a href="#architecture-inferred" id="toc-architecture-inferred" class="nav-link" data-scroll-target="#architecture-inferred"><span class="header-section-number">1.4.3</span> Architecture (Inferred)</a></li>
  <li><a href="#capabilities-and-limitations" id="toc-capabilities-and-limitations" class="nav-link" data-scroll-target="#capabilities-and-limitations"><span class="header-section-number">1.4.4</span> Capabilities and Limitations</a></li>
  <li><a href="#usage-and-access" id="toc-usage-and-access" class="nav-link" data-scroll-target="#usage-and-access"><span class="header-section-number">1.4.5</span> Usage and Access</a></li>
  <li><a href="#practical-usage-example" id="toc-practical-usage-example" class="nav-link" data-scroll-target="#practical-usage-example"><span class="header-section-number">1.4.6</span> Practical Usage Example</a></li>
  </ul></li>
  <li><a href="#vision-transformers-vit" id="toc-vision-transformers-vit" class="nav-link" data-scroll-target="#vision-transformers-vit"><span class="header-section-number">1.5</span> 10.4 Vision Transformers (ViT)</a>
  <ul class="collapse">
  <li><a href="#architecture-deep-dive" id="toc-architecture-deep-dive" class="nav-link" data-scroll-target="#architecture-deep-dive"><span class="header-section-number">1.5.1</span> Architecture Deep Dive</a></li>
  <li><a href="#vit-variants" id="toc-vit-variants" class="nav-link" data-scroll-target="#vit-variants"><span class="header-section-number">1.5.2</span> ViT Variants</a></li>
  <li><a href="#training-vit" id="toc-training-vit" class="nav-link" data-scroll-target="#training-vit"><span class="header-section-number">1.5.3</span> Training ViT</a></li>
  <li><a href="#why-vit-works" id="toc-why-vit-works" class="nav-link" data-scroll-target="#why-vit-works"><span class="header-section-number">1.5.4</span> Why ViT Works</a></li>
  </ul></li>
  <li><a href="#comparison-and-selection-guide" id="toc-comparison-and-selection-guide" class="nav-link" data-scroll-target="#comparison-and-selection-guide"><span class="header-section-number">1.6</span> 10.5 Comparison and Selection Guide</a>
  <ul class="collapse">
  <li><a href="#performance-comparison" id="toc-performance-comparison" class="nav-link" data-scroll-target="#performance-comparison"><span class="header-section-number">1.6.1</span> Performance Comparison</a></li>
  <li><a href="#choosing-between-models" id="toc-choosing-between-models" class="nav-link" data-scroll-target="#choosing-between-models"><span class="header-section-number">1.6.2</span> Choosing Between Models</a></li>
  <li><a href="#model-selection-guide" id="toc-model-selection-guide" class="nav-link" data-scroll-target="#model-selection-guide"><span class="header-section-number">1.6.3</span> Model Selection Guide</a></li>
  <li><a href="#hybrid-approaches" id="toc-hybrid-approaches" class="nav-link" data-scroll-target="#hybrid-approaches"><span class="header-section-number">1.6.4</span> Hybrid Approaches</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">1.7</span> Key Takeaways</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">1.8</span> Exercises</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-08.html">Part III: Architectures</a></li><li class="breadcrumb-item"><a href="./chapter-10.html">Chapter 10: Seminal Models and Architectures</a></li></ol></nav></header>





<section id="chapter-10-seminal-models-and-architectures" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Chapter 10: Seminal Models and Architectures</h1>
<hr>
<p><strong>Previous</strong>: <a href="./chapter-09.html">Chapter 9: Generative Models for Multimodal Data</a> | <strong>Next</strong>: <a href="./chapter-11.html">Chapter 11: Practical Implementation Guide</a> | <strong>Home</strong>: <a href="./index.html">Table of Contents</a></p>
<hr>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>After reading this chapter, you should be able to: - Understand CLIP’s architecture and impact - Understand BLIP-2’s parameter efficiency approach - Understand GPT-4V’s multimodal capabilities - Compare different model architectures - Choose appropriate models for applications</p>
</section>
<section id="clip-learning-transferable-models-from-natural-language-supervision" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="clip-learning-transferable-models-from-natural-language-supervision"><span class="header-section-number">1.2</span> 10.1 CLIP: Learning Transferable Models From Natural Language Supervision</h2>
<section id="revolution-in-vision-understanding" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="revolution-in-vision-understanding"><span class="header-section-number">1.2.1</span> Revolution in Vision Understanding</h3>
<p><strong>Historical context (pre-2021):</strong></p>
<pre><code>Vision models trained on ImageNet:
  1.4 million images
  1000 fixed classes
  Cannot generalize to new concepts

Problem:
  "Can we classify cats?" → Pre-trained model: Yes (class exists)
  "Can we classify dog breeds?" → No retraining, poor accuracy
  "Can we classify objects in photos from 200 years ago?" → Completely fails

Fundamental limitation:
  Models learn specific classes
  Cannot generalize to new concepts
  Must retrain for new tasks</code></pre>
<p><strong>CLIP solution:</strong></p>
<pre><code>Instead of training on labeled classes,
train on language descriptions directly

Key insight:
  Images naturally paired with text on internet
  Text is flexible: can describe anything
  Use this natural supervision!

Dataset: 400 million image-text pairs
Training: Contrastive learning
Result: Zero-shot transfer to any category!</code></pre>
</section>
<section id="clip-architecture" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="clip-architecture"><span class="header-section-number">1.2.2</span> CLIP Architecture</h3>
<p><strong>Components:</strong></p>
<pre><code>Image Encoder:           Text Encoder:
  Vision Transformer      Transformer
  Input: 224×224 image    Input: Text tokens
  Output: 512D vector     Output: 512D vector

                    ↓            ↓

                [L2 Normalize]

                    ↓            ↓

            Similarity Computation
            (Dot product of normalized)
                    ↓
            Contrastive Loss</code></pre>
<p><strong>Training process:</strong></p>
<pre><code>Batch size: 32,768 (massive!)

1. Image-caption pairs sampled
2. Encode all images: 32k × 512
3. Encode all captions: 32k × 512
4. Compute 32k × 32k similarity matrix
5. Apply contrastive loss
   - Diagonal elements (matched pairs) should be high
   - Off-diagonal elements (mismatches) should be low
6. Backprop and update

Requires:
  - Multiple GPUs (distributed training)
  - Efficient operations (all at batch size 32k)
  - 2 weeks of training on TPU clusters</code></pre>
</section>
<section id="zero-shot-transfer" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="zero-shot-transfer"><span class="header-section-number">1.2.3</span> Zero-Shot Transfer</h3>
<p><strong>How it works:</strong></p>
<pre><code>New task: Classify dog breeds

Step 1: Create text templates
  "a photo of a {breed}"

  Breeds: Golden Retriever, Labrador, Poodle, ...

Step 2: Encode all templates
  text_embeddings = text_encoder(templates)

Step 3: For test image
  image_embedding = image_encoder(image)

  similarities = image_embedding · text_embeddings

  Prediction = argmax(similarities)

No training on dog breeds needed!
Never seen dog breed data!
Still achieves good accuracy!

Example results:
  Image: [Golden Retriever photo]

  Similarities:
    "a photo of a Golden Retriever": 0.95 ← Highest
    "a photo of a Labrador": 0.72
    "a photo of a Poodle": 0.68
    ...

  Prediction: Golden Retriever ✓</code></pre>
<p><strong>Why templates matter:</strong></p>
<pre><code>Good template: "a photo of a {}"
  Anchors description to visual domain
  Natural phrasing matches training data

Bad template: "a {}"
  Too ambiguous
  Could mean drawing, word, concept
  Confuses encoder

Optimal performance needs:
  Multiple diverse templates
  Hand-tuning per domain</code></pre>
</section>
<section id="benchmark-results" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="benchmark-results"><span class="header-section-number">1.2.4</span> Benchmark Results</h3>
<p><strong>ImageNet evaluation:</strong></p>
<pre><code>Zero-shot CLIP-ViT-L:  62.8%
ResNet-50 supervised:  76.1%

Gap exists, but context matters:
  CLIP: No labeled ImageNet data
        Trained on raw internet
        Immediately generalizable

  ResNet: Trained on 1.4M labeled ImageNet
          Specific to those 1000 classes
          Needs fine-tuning for new tasks

Transfer comparison:

ImageNet 1% labeled:
  CLIP fine-tuned: 76.3%
  Supervised ResNet (1% labels): 30-40%

  CLIP is 2-3× more data-efficient!

Stanford Cars (fine-tuning):
  CLIP linear probe: 94.1%
  ResNet-50 fine-tuned: 92.8%

  CLIP transfers better!</code></pre>
</section>
<section id="impact-on-field" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="impact-on-field"><span class="header-section-number">1.2.5</span> Impact on Field</h3>
<p><strong>Before CLIP (pre-2021):</strong></p>
<pre><code>Vision = ImageNet classification
Evaluation = Classification accuracy
Transfer = Fine-tuning on new task
Zero-shot = Not really done</code></pre>
<p><strong>After CLIP (post-2021):</strong></p>
<pre><code>Vision = Multimodal understanding
Evaluation = Zero-shot transfer metrics
Transfer = No fine-tuning needed
Zero-shot = Standard approach</code></pre>
<p><strong>Cascading impact:</strong></p>
<pre><code>CLIP (Apr 2021): Language-supervised vision
    ↓
DALL-E (Jan 2021, but validated by CLIP)
    → Text-to-image with language understanding
    ↓
Flamingo (Apr 2022): Vision-language models
    ↓
LLaVA (Apr 2023): Vision + large language models
    ↓
GPT-4V (Sep 2023): Multimodal reasoning

Each step enabled by CLIP's success</code></pre>
</section>
</section>
<section id="blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders"><span class="header-section-number">1.3</span> 10.2 BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders</h2>
<section id="context-and-motivation" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="context-and-motivation"><span class="header-section-number">1.3.1</span> Context and Motivation</h3>
<p><strong>Problem after CLIP:</strong></p>
<pre><code>CLIP effective but:
  - Fine-tuning expensive
  - Need task-specific tuning
  - Limited reasoning capability

Vision-language models:
  - Powerful but slow (billion+ parameters)
  - Require massive compute
  - Not accessible

Question: Can we get SOTA with small model?
Solution: BLIP-2 parameter-efficient approach</code></pre>
</section>
<section id="blip-2-architecture" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="blip-2-architecture"><span class="header-section-number">1.3.2</span> BLIP-2 Architecture</h3>
<p><strong>Key innovation: Frozen encoders + lightweight connector</strong></p>
<pre><code>                ┌─ Frozen Vision Encoder
                │  (pre-trained, not updated)
                │
Image input ────┤
                │
                └─ Lightweight connector
                   (trainable, small)
                         │
                         ↓
                   Shared representation
                         ↓
Language model ────── Q-Former
(frozen)          (trainable, small)
                         │
                Text input</code></pre>
<p><strong>Q-Former (Query Transformer):</strong></p>
<pre><code>Purpose: Bridge between vision and language

Architecture:
  - 12 Transformer layers
  - 8 attention heads
  - ~300M parameters (small!)

  But connects:
    Frozen image encoder (2048D features)
    Frozen language model (2048D embedding)

Query mechanism:
  - Learns learnable query vectors
  - Query vectors attend to image features
  - Extracts information without changing image encoder
  - Output: Fixed number of tokens</code></pre>
<p><strong>Training strategy:</strong></p>
<pre><code>Step 1: Vision-Language Pre-training
  Dataset: 129M image-text pairs
  Loss: Contrastive + caption matching
  Time: 1 week on 80 A100 GPUs

  Result: Q-Former learns to extract visual information

Step 2: Instruction Tuning (Optional)
  Dataset: Instruction-following examples
  Fine-tune Q-Former and language model
  Time: Few hours

  Result: Follows instructions better</code></pre>
</section>
<section id="why-it-works" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="why-it-works"><span class="header-section-number">1.3.3</span> Why It Works</h3>
<p><strong>Efficiency gains:</strong></p>
<pre><code>CLIP approach:
  Train image encoder: 2 weeks
  Train text encoder: 2 weeks
  Aligned: 2 weeks
  Total: 6 weeks
  Parameters: 300M (image) + 150M (text) = 450M trained

BLIP-2 approach:
  Use pre-trained frozen encoders
  Train tiny Q-Former: 3 days
  Total: 3 days
  Parameters: 300M trained (Q-Former)

  100× faster!
  Same performance!</code></pre>
<p><strong>Information flow:</strong></p>
<pre><code>Vision encoder captures:
  - Object detection
  - Spatial understanding
  - Visual patterns

Q-Former bottleneck:
  - Must compress high-level concepts
  - Learns what information matters
  - Efficient transfer to language

Language model:
  - Already trained on huge text corpus
  - Strong reasoning capabilities
  - Leveraged for vision-language tasks</code></pre>
</section>
<section id="blip-2-capabilities" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="blip-2-capabilities"><span class="header-section-number">1.3.4</span> BLIP-2 Capabilities</h3>
<p><strong>Image understanding:</strong></p>
<pre><code>Image: [Cat on couch]

Q: "What's in the image?"
A: "A cat is relaxing on a couch"

Q: "What color is the cat?"
A: "The cat appears to be orange or ginger colored"

Q: "Why might the cat be on the couch?"
A: "Cats often rest on couches because they are comfortable
   and provide a good vantage point for observing surroundings"

Reasoning capability comes from:
  Frozen language model
  Q-Former alignment
  Instruction tuning</code></pre>
<p><strong>Visual question answering:</strong></p>
<pre><code>Image: [Busy street scene]
Question: "How many people can you see?"

Processing:
  1. Extract features from image via frozen encoder
  2. Q-Former compresses to meaningful tokens
  3. Append question
  4. Language model generates answer
  5. "I can see approximately 7-8 people in the image"</code></pre>
<p><strong>Image-text retrieval:</strong></p>
<pre><code>Use Q-Former output as image representation
Match with text embeddings from language model

Image encoder → Q-Former → representation
Text → Language model → representation

Similarity: cos(image_rep, text_rep)
High similarity: Retrieved as match</code></pre>
</section>
<section id="benchmark-results-1" class="level3" data-number="1.3.5">
<h3 data-number="1.3.5" class="anchored" data-anchor-id="benchmark-results-1"><span class="header-section-number">1.3.5</span> Benchmark Results</h3>
<p><strong>Performance comparison:</strong></p>
<pre><code>                CLIP        BLIP-2      BLIP-2 + InstInst
────────────────────────────────────────────────────────
Flickr30K      86.3%       88.6%        90.1%
COCO           65.4%       71.6%        75.8%
VQA v2         82.4%       83.2%        84.5%

BLIP-2 better in almost every metric
With instruction tuning, achieves SOTA
All with frozen encoders!</code></pre>
<p><strong>Efficiency:</strong></p>
<pre><code>Parameter comparison:

CLIP-ViT-L:
  Image: 303M
  Text: 123M
  Total: 426M (all trainable)

BLIP-2:
  Image: 303M (frozen)
  Q-Former: 300M (trainable)
  Language model: 1.3B (frozen)
  Total: 1.6B (but only 300M trained!)

Training time:
  CLIP: 2 weeks
  BLIP-2: 3 days (30× faster!)

Inference memory:
  CLIP: Load both encoders (~400M)
  BLIP-2: Load all three (~1.6B) but frozen ones optimized
          Total inference: comparable</code></pre>
</section>
</section>
<section id="gpt-4v-multimodal-reasoning" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="gpt-4v-multimodal-reasoning"><span class="header-section-number">1.4</span> 10.3 GPT-4V: Multimodal Reasoning</h2>
<section id="revolutionary-capabilities" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="revolutionary-capabilities"><span class="header-section-number">1.4.1</span> Revolutionary Capabilities</h3>
<p><strong>What makes it different:</strong></p>
<pre><code>Before GPT-4V:
  Vision = Classification, detection, captioning
  Reasoning = Mostly on text
  Multimodal = Aligned representations, limited reasoning

GPT-4V:
  Vision = Understanding visual complexity
  Reasoning = Deep reasoning on images
  Multimodal = Joint reasoning with language</code></pre>
</section>
<section id="examples-of-capabilities" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="examples-of-capabilities"><span class="header-section-number">1.4.2</span> Examples of Capabilities</h3>
<p><strong>Example 1: Complex visual reasoning</strong></p>
<pre><code>Image: [Complex chart with multiple time series]

User: "What trend does this chart show?"

GPT-4V: "The chart shows four time series from 2010-2023.
  - Series A (blue): Steady decline from 100 to 40
  - Series B (red): Volatile, peaks 2015, valley 2020
  - Series C (green): Gradual increase
  - Series D (orange): Cyclical pattern

  Overall: Diverging trends suggest different underlying factors"

Not just captioning - actual data analysis!</code></pre>
<p><strong>Example 2: Document understanding</strong></p>
<pre><code>Image: [Scanned business letter]

User: "Extract the invoice number and total amount"

GPT-4V: "Invoice Number: INV-2024-05-12345
         Total Amount: $2,459.87"

Understands document structure
Extracts relevant information
Handles poor quality scans</code></pre>
<p><strong>Example 3: Reasoning about composition</strong></p>
<pre><code>Image: [Painting composition analysis]

User: "Analyze the compositional technique in this painting"

GPT-4V: "This painting uses rule of thirds compositionally:
  - Main subject (woman) positioned at intersection of thirds
  - Horizon line at upper third line
  - Warm lighting on subject, cool lighting background
  - Diagonal lead lines draw eye to subject

  The artist effectively guides viewer attention through
  deliberate placement and color contrast"

Art criticism level analysis!</code></pre>
</section>
<section id="architecture-inferred" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="architecture-inferred"><span class="header-section-number">1.4.3</span> Architecture (Inferred)</h3>
<p><strong>Likely design (exact details not public):</strong></p>
<pre><code>Vision encoder:
  Likely ViT-based or custom
  Processes image at multiple resolutions
  Extracts hierarchical features

Feature extraction:
  Multiple image patches at different scales
  Attention to different regions
  Global and local features

Integration with language model:
  Features converted to tokens
  Inserted into language model token sequence
  Language model processes mixed modality input

Language model:
  GPT-4 core (text foundation)
  Extended to handle vision tokens
  Uses cross-attention to integrate vision
  Can reason about images like language

Processing:
  Image → Tokenize as vision tokens
  Text → Tokenize as text tokens
  Mixed → Process through transformer
  Output: Text reasoning about image</code></pre>
<p><strong>Inference process:</strong></p>
<pre><code>User input: Image + text question

1. Process image
   Convert to vision tokens
   Hierarchical extraction
   Result: ~100-1000 vision tokens

2. Concatenate with text
   [Image tokens] + [Question tokens]
   Single token sequence

3. Process through language model
   Transformer attends to all tokens
   Cross-modal reasoning

4. Generate response
   Autoregressive text generation
   Condition on image + question

Reasoning capability:
  Language model reasons about vision tokens
  Same as reasoning about text
  But tokens encode visual information</code></pre>
</section>
<section id="capabilities-and-limitations" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="capabilities-and-limitations"><span class="header-section-number">1.4.4</span> Capabilities and Limitations</h3>
<p><strong>Strong capabilities:</strong></p>
<pre><code>✓ Complex reasoning about images
✓ Document and form understanding
✓ Visual common sense
✓ Temporal reasoning (video understanding)
✓ Following fine-grained instructions
✓ Reasoning about text in images
✓ Compositional understanding</code></pre>
<p><strong>Limitations:</strong></p>
<pre><code>✗ Spatial relationships (exact positions)
✗ Counting small objects (&gt;10 items unreliable)
✗ Reading all text perfectly (OCR still struggles)
✗ 3D understanding (limited depth reasoning)
✗ Medical diagnosis (not trained for this)
✗ Legal decisions (not legal advice)</code></pre>
</section>
<section id="usage-and-access" class="level3" data-number="1.4.5">
<h3 data-number="1.4.5" class="anchored" data-anchor-id="usage-and-access"><span class="header-section-number">1.4.5</span> Usage and Access</h3>
<p><strong>Availability:</strong></p>
<pre><code>Model: GPT-4V
Access: OpenAI API (paid)
Cost: $0.03 per 1K image tokens
      (roughly $0.01-0.03 per image depending on size)

Alternatives (open-source):
  LLaVA: Free, open-source, weaker but good
  Flamingo: DeepMind, accessible via API
  Claude 3 Vision: Anthropic, competitive
  Gemini Pro Vision: Google, competitive</code></pre>
</section>
<section id="practical-usage-example" class="level3" data-number="1.4.6">
<h3 data-number="1.4.6" class="anchored" data-anchor-id="practical-usage-example"><span class="header-section-number">1.4.6</span> Practical Usage Example</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT4VisionAnalyzer:</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, api_key):</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        openai.api_key <span class="op">=</span> api_key</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> analyze_image(<span class="va">self</span>, image_url, query):</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Analyze image using GPT-4V</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co">            image_url: URL of image</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co">            query: Question or instruction</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="co">            Analysis text</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span><span class="st">"gpt-4-vision-preview"</span>,</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"content"</span>: [</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>                        {</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"type"</span>: <span class="st">"image_url"</span>,</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"image_url"</span>: {<span class="st">"url"</span>: image_url}</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>                        },</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>                        {</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"type"</span>: <span class="st">"text"</span>,</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"text"</span>: query</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>                        }</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>                    ]</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>            max_tokens<span class="op">=</span><span class="dv">1024</span></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> analyze_local_image(<span class="va">self</span>, image_path, query):</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Analyze local image by encoding to base64"""</span></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> base64</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(image_path, <span class="st">"rb"</span>) <span class="im">as</span> image_file:</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>            base64_image <span class="op">=</span> base64.b64encode(image_file.read()).decode(<span class="st">'utf-8'</span>)</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determine image type</span></span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>        image_type <span class="op">=</span> <span class="st">"jpeg"</span> <span class="cf">if</span> image_path.endswith(<span class="st">".jpg"</span>) <span class="cf">else</span> <span class="st">"png"</span></span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span><span class="st">"gpt-4-vision-preview"</span>,</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[</span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"content"</span>: [</span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>                        {</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"type"</span>: <span class="st">"image_url"</span>,</span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"image_url"</span>: {</span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>                                <span class="st">"url"</span>: <span class="ss">f"data:image/</span><span class="sc">{</span>image_type<span class="sc">}</span><span class="ss">;base64,</span><span class="sc">{</span>base64_image<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a>                            }</span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a>                        },</span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a>                        {</span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"type"</span>: <span class="st">"text"</span>,</span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"text"</span>: query</span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a>                        }</span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a>                    ]</span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb31-69"><a href="#cb31-69" aria-hidden="true" tabindex="-1"></a>            max_tokens<span class="op">=</span><span class="dv">1024</span></span>
<span id="cb31-70"><a href="#cb31-70" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb31-71"><a href="#cb31-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-72"><a href="#cb31-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb31-73"><a href="#cb31-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-74"><a href="#cb31-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage</span></span>
<span id="cb31-75"><a href="#cb31-75" aria-hidden="true" tabindex="-1"></a>analyzer <span class="op">=</span> GPT4VisionAnalyzer(<span class="st">"your-api-key"</span>)</span>
<span id="cb31-76"><a href="#cb31-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-77"><a href="#cb31-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Analyze image from URL</span></span>
<span id="cb31-78"><a href="#cb31-78" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> analyzer.analyze_image(</span>
<span id="cb31-79"><a href="#cb31-79" aria-hidden="true" tabindex="-1"></a>    <span class="st">"https://example.com/image.jpg"</span>,</span>
<span id="cb31-80"><a href="#cb31-80" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Describe the scene and identify key objects"</span></span>
<span id="cb31-81"><a href="#cb31-81" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-82"><a href="#cb31-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span>
<span id="cb31-83"><a href="#cb31-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-84"><a href="#cb31-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Analyze local image</span></span>
<span id="cb31-85"><a href="#cb31-85" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> analyzer.analyze_local_image(</span>
<span id="cb31-86"><a href="#cb31-86" aria-hidden="true" tabindex="-1"></a>    <span class="st">"path/to/image.png"</span>,</span>
<span id="cb31-87"><a href="#cb31-87" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What problem does this diagram illustrate?"</span></span>
<span id="cb31-88"><a href="#cb31-88" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-89"><a href="#cb31-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="vision-transformers-vit" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="vision-transformers-vit"><span class="header-section-number">1.5</span> 10.4 Vision Transformers (ViT)</h2>
<section id="architecture-deep-dive" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="architecture-deep-dive"><span class="header-section-number">1.5.1</span> Architecture Deep Dive</h3>
<p><strong>From CNN to ViT:</strong></p>
<pre><code>CNN (Convolutional Neural Network):
  ├─ Inductive bias: Locality (nearby pixels related)
  ├─ Receptive field: Grows with depth
  ├─ Properties: Equivariance to translation
  └─ Requirement: Medium datasets

ViT (Vision Transformer):
  ├─ Inductive bias: Minimal (pure attention)
  ├─ Receptive field: Global from layer 1
  ├─ Properties: No built-in translation equivariance
  └─ Requirement: Large datasets (billions)

Trade-off:
  CNN: Efficient with data, learns locality
  ViT: Requires more data, learns general patterns

  With sufficient data: ViT often wins</code></pre>
<p><strong>Detailed architecture:</strong></p>
<pre><code>Input: 224×224×3 image

Step 1: Patch embedding
  Divide into 16×16 patches
  14×14 = 196 patches
  Each patch: 16×16×3 = 768D
  Project to 768D embedding
  Result: 196×768 tokens

Step 2: Add special tokens
  [CLS] token (for classification)
  [DIST] token (for distillation, optional)
  Result: 197 tokens (or 198)

Step 3: Positional encoding
  Sinusoidal or learnable encoding
  Absolute positions (not relative)
  Same 768D as embeddings
  Result: 197×768 tokens with position info

Step 4: 12-layer transformer encoder
  Layer i:
    ├─ Multi-head self-attention (12 heads)
    │  └─ Each token attends to all 197 tokens
    │
    ├─ Add &amp; Normalize (residual + layer norm)
    │
    ├─ Feed-forward network (3072D intermediate)
    │
    └─ Add &amp; Normalize

  After each layer: Tokens refined by context

  Final layer output: 197×768 tokens

Step 5: Classification
  Extract [CLS] token: 768D
  Linear layer: 768D → num_classes
  Softmax → probabilities</code></pre>
<p><strong>Self-attention in ViT:</strong></p>
<pre><code>Each layer: All tokens attend to all tokens

Complexity: O(n²) where n = 196
  Attention matrix: 196×196
  For 224×224: 49,984 similarities computed

Feasibility:
  Modern GPU: Can handle easily
  Fast enough for training
  Inference: ~100ms per image

Benefit:
  Every patch sees every other patch
  Global context from start
  Long-range dependencies captured</code></pre>
</section>
<section id="vit-variants" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="vit-variants"><span class="header-section-number">1.5.2</span> ViT Variants</h3>
<p><strong>ViT-B (Base):</strong></p>
<pre><code>Layers: 12
Hidden dim: 768
Heads: 12
Parameters: 86M</code></pre>
<p><strong>ViT-L (Large):</strong></p>
<pre><code>Layers: 24
Hidden dim: 1024
Heads: 16
Parameters: 304M</code></pre>
<p><strong>ViT-H (Huge):</strong></p>
<pre><code>Layers: 32
Hidden dim: 1280
Heads: 16
Parameters: 632M</code></pre>
<p><strong>ViT with different patch sizes:</strong></p>
<pre><code>ViT-B/32: 32×32 patches
  196/4 = 49 tokens
  Faster, less detail
  Better for small images

ViT-B/16: 16×16 patches
  196 tokens
  Standard choice
  Good balance

ViT-B/8: 8×8 patches
  14×14 = 196 tokens
  Slower, more detail
  Best quality</code></pre>
</section>
<section id="training-vit" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="training-vit"><span class="header-section-number">1.5.3</span> Training ViT</h3>
<p><strong>Data requirements:</strong></p>
<pre><code>ImageNet-1K (small dataset):
  1.4M images
  ViT fails: 76% accuracy (worse than ResNet)
  Reason: Not enough data to learn structure

ImageNet-21K (medium dataset):
  14M images
  ViT succeeds: 85% accuracy

JFT-300M (large private dataset):
  300M images
  ViT excels: 90%+ accuracy

Pattern:
  ViT-B requires ~10M images minimum
  ViT-L requires ~50M images
  ViT-H requires ~500M images

  Trade-off with amount of pre-training data available</code></pre>
<p><strong>Training details:</strong></p>
<pre><code>Optimization:
  Optimizer: AdamW
  Learning rate: 0.001 (with warmup and decay)
  Batch size: 4096 (distributed across GPUs)
  Epochs: ~90

Regularization:
  Dropout: 0.1
  Stochastic depth: 0.1-0.2
  Layer scale: Trainable scale per layer
  Mixup: Data augmentation (mix images)

Initialization:
  Patch embedding: Random normal
  Transformer weights: Trunc normal
  Positional encoding: Learned (not frozen)</code></pre>
<p><strong>Fine-tuning:</strong></p>
<pre><code>Pre-trained ViT-B-32 from CLIP:
  Trained on 400M image-text pairs
  Good general vision understanding

Fine-tune on ImageNet-1K:
  Freeze most layers
  Train last few layers only
  Learning rate: 0.0001 (small!)
  Epochs: 10-20

  Result: 85% accuracy
  With only 1.4M images!

  Shows power of pre-training</code></pre>
</section>
<section id="why-vit-works" class="level3" data-number="1.5.4">
<h3 data-number="1.5.4" class="anchored" data-anchor-id="why-vit-works"><span class="header-section-number">1.5.4</span> Why ViT Works</h3>
<p><strong>Theoretical insights:</strong></p>
<pre><code>1. Patches are tokens
   Like words in NLP
   Vision is just tokenized differently
   Transformer processes any tokens equally

2. Attention is universal
   Works for images (2D spatial)
   Works for text (1D sequential)
   Works for audio (1D temporal)
   No modality-specific design needed

3. Scaling laws
   Transformers scale better than CNNs
   More data → ViT wins
   More parameters → ViT wins
   Smooth scaling (no sudden jumps)

4. Transfer learning
   Pre-trained representations general
   Work across domains
   Fine-tune quickly to new task</code></pre>
<p><strong>Empirical validation:</strong></p>
<pre><code>Scaling laws (Dosovitski et al.):

Model size vs downstream accuracy:

Large datasets (&gt;50M images):
  ╱ ViT trend
 ╱ CNN trend
╱

ViT converges slower initially
But eventually dominates
On large data: ViT &gt;&gt; CNN

Compute scaling:
  Same compute budget
  ViT often outperforms CNN
  Even on small datasets with proper pre-training</code></pre>
</section>
</section>
<section id="comparison-and-selection-guide" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="comparison-and-selection-guide"><span class="header-section-number">1.6</span> 10.5 Comparison and Selection Guide</h2>
<section id="performance-comparison" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="performance-comparison"><span class="header-section-number">1.6.1</span> Performance Comparison</h3>
<p><strong>Zero-shot classification (ImageNet):</strong></p>
<pre><code>                    Zero-shot    Fine-tune 1%
────────────────────────────────────────────
ResNet-50           ~30%         ~20%
CLIP ViT-B/32       62.8%        76%
CLIP ViT-L/14       68.3%        79%
BLIP-2              ~71%         80%
GPT-4V              ~85%*        ~90%*

*Estimated based on capabilities</code></pre>
<p><strong>Reasoning capability:</strong></p>
<pre><code>                Vision    Language  Reasoning
                Underst   Fluency   Complexity
────────────────────────────────────────────
CLIP            ✓✓        ✗         ✗
ViT             ✓✓✓       ✗         ✗
BLIP-2          ✓✓        ✓✓        ✓✓
GPT-4V          ✓✓✓       ✓✓✓       ✓✓✓</code></pre>
</section>
<section id="choosing-between-models" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="choosing-between-models"><span class="header-section-number">1.6.2</span> Choosing Between Models</h3>
<p><strong>Decision flowchart:</strong></p>
<pre><code>
Is it zero-shot classification?
│
├─ YES → Need language grounding?
│        │
│        ├─ YES → CLIP (fast, simple)
│        │
│        └─ NO → ViT (better accuracy)
│
└─ NO → Need visual reasoning?
        │
        ├─ YES → Need language fluency?
        │        │
        │        ├─ YES → GPT-4V (SOTA but expensive)
        │        │
        │        └─ NO → BLIP-2 (good balance)
        │
        └─ NO → Need efficiency?
                 │
                 ├─ YES → BLIP-2 (fast)
                 │
                 └─ NO → ViT (best accuracy)</code></pre>
</section>
<section id="model-selection-guide" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="model-selection-guide"><span class="header-section-number">1.6.3</span> Model Selection Guide</h3>
<p><strong>For production deployment:</strong></p>
<pre><code>Requirement: Real-time inference
  Choice: CLIP (fast, lightweight)
  Model: CLIP ViT-B/32
  Latency: ~50ms per image
  Accuracy: 62% zero-shot ImageNet

Requirement: High accuracy on custom task
  Choice: ViT fine-tuned
  Model: ViT-L pre-trained on JFT-300M
  Latency: ~100ms per image
  Accuracy: ~90% (with fine-tuning)

Requirement: Complex visual reasoning
  Choice: BLIP-2
  Model: BLIP-2 (Flamingo variant)
  Latency: ~500ms per image
  Accuracy: 85% zero-shot VQA

Requirement: State-of-the-art performance
  Choice: GPT-4V
  Model: GPT-4V via API
  Latency: ~2000ms per image (API call)
  Accuracy: ~95% on most tasks
  Cost: ~$0.03 per image</code></pre>
<p><strong>Trade-off matrix:</strong></p>
<pre><code>Model      Speed  Accuracy  Reasoning  Cost   Accessibility
────────────────────────────────────────────────────────────
CLIP       ★★★    ★★        ★         Low    ✓ Open
ViT        ★★     ★★★       ★         Low    ✓ Open
BLIP-2     ★      ★★★       ★★        Low    ✓ Open
GPT-4V     ★      ★★★★      ★★★★     High   ⚠ API only

Legend:
  Speed: ★★★ = fast, ★ = slow
  Accuracy: ★★★★ = best, ★ = okay
  Reasoning: ★★★★ = excellent, ★ = limited
  Cost: Low = &lt;$1K to run, High = &gt;$100K
  Accessibility: ✓ = open-source, ⚠ = API-only</code></pre>
</section>
<section id="hybrid-approaches" class="level3" data-number="1.6.4">
<h3 data-number="1.6.4" class="anchored" data-anchor-id="hybrid-approaches"><span class="header-section-number">1.6.4</span> Hybrid Approaches</h3>
<p><strong>Combining models:</strong></p>
<pre><code>Pipeline 1: CLIP for routing
  ① Use CLIP to classify general category
  ② Route to specialized model based on category
  ③ Specialized model provides detailed answer

  Benefit: Efficient routing
           Specialized models for domains

Pipeline 2: BLIP-2 with ViT backbone
  ① Use ViT for image encoding
  ② Use BLIP-2 Q-Former for alignment
  ③ Use language model for reasoning

  Benefit: Best of both worlds
           Good accuracy + reasoning

Pipeline 3: Ensemble
  ① Get predictions from multiple models
  ② Combine predictions (voting, averaging)
  ③ Use confidence scores for weighting

  Benefit: Robust predictions
           Uncertainty estimation
           Better than single model</code></pre>
<p><strong>Example implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HybridVisionModel:</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Combine multiple vision models"""</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.clip <span class="op">=</span> CLIPModel()</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vit <span class="op">=</span> ViTModel()</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blip2 <span class="op">=</span> BLIP2Model()</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> classify_with_routing(<span class="va">self</span>, image):</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Route based on CLIP understanding"""</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fast CLIP classification</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        clip_pred <span class="op">=</span> <span class="va">self</span>.clip.predict(image)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Route to specialized model</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> clip_pred[<span class="st">'category'</span>] <span class="op">==</span> <span class="st">'text_heavy'</span>:</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use OCR-optimized model</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.specialized_ocr_model(image)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> clip_pred[<span class="st">'category'</span>] <span class="op">==</span> <span class="st">'scene_complex'</span>:</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use detailed reasoning model</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.blip2.analyze(image)</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use fast ViT</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.vit.predict(image)</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> ensemble_prediction(<span class="va">self</span>, image):</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Combine predictions from multiple models"""</span></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>        clip_pred <span class="op">=</span> <span class="va">self</span>.clip.predict(image)</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>        vit_pred <span class="op">=</span> <span class="va">self</span>.vit.predict(image)</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        blip2_pred <span class="op">=</span> <span class="va">self</span>.blip2.predict(image)</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Weighted ensemble</span></span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> {</span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">'clip'</span>: <span class="fl">0.2</span>,</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>            <span class="st">'vit'</span>: <span class="fl">0.5</span>,</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>            <span class="st">'blip2'</span>: <span class="fl">0.3</span></span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>        ensemble_score <span class="op">=</span> (</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>            weights[<span class="st">'clip'</span>] <span class="op">*</span> clip_pred[<span class="st">'score'</span>] <span class="op">+</span></span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>            weights[<span class="st">'vit'</span>] <span class="op">*</span> vit_pred[<span class="st">'score'</span>] <span class="op">+</span></span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>            weights[<span class="st">'blip2'</span>] <span class="op">*</span> blip2_pred[<span class="st">'score'</span>]</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ensemble_score</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> confidence_aware_selection(<span class="va">self</span>, image):</span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Choose model based on confidence"""</span></span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>        clip_result <span class="op">=</span> <span class="va">self</span>.clip.predict(image)</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># High confidence: Use fast model</span></span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> clip_result[<span class="st">'confidence'</span>] <span class="op">&gt;</span> <span class="fl">0.9</span>:</span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> clip_result</span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Medium confidence: Use stronger model</span></span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> clip_result[<span class="st">'confidence'</span>] <span class="op">&gt;</span> <span class="fl">0.7</span>:</span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.vit.predict(image)</span>
<span id="cb50-60"><a href="#cb50-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-61"><a href="#cb50-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Low confidence: Use most powerful model</span></span>
<span id="cb50-62"><a href="#cb50-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb50-63"><a href="#cb50-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.blip2.analyze(image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="key-takeaways" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">1.7</span> Key Takeaways</h2>
<ul>
<li><strong>CLIP</strong> revolutionized zero-shot transfer with language supervision</li>
<li><strong>BLIP-2</strong> showed parameter-efficient multimodal learning is possible</li>
<li><strong>GPT-4V</strong> demonstrated deep visual reasoning capabilities</li>
<li><strong>ViT</strong> proved transformers work for vision without CNNs</li>
<li><strong>Trade-offs exist</strong> between accuracy, speed, reasoning, and cost</li>
<li><strong>Hybrid approaches</strong> can optimize for specific applications</li>
<li><strong>Model selection</strong> depends on task requirements and constraints</li>
</ul>
</section>
<section id="exercises" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="exercises"><span class="header-section-number">1.8</span> Exercises</h2>
<p><strong>⭐ Beginner:</strong> 1. Use CLIP for zero-shot classification 2. Compare CLIP vs ViT on different datasets 3. Implement text template variations for CLIP</p>
<p><strong>⭐⭐ Intermediate:</strong> 4. Fine-tune BLIP-2 on custom dataset 5. Build ensemble of multiple models 6. Compare inference latency across models</p>
<p><strong>⭐⭐⭐ Advanced:</strong> 7. Implement custom routing based on CLIP understanding 8. Build confidence-aware model selection 9. Optimize inference pipeline for production</p>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/guokai8\.github\.io\/mml_learning\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Kai Guo - Multimodal Learning Guide</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>