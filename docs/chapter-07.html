<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter-07 ‚Äì Multimodal Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-56df71c9454ca07313afc907ff0d97f5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Multimodal Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./preface.html"> 
<span class="menu-text">Preface</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./how-to-use.html"> 
<span class="menu-text">How to Use</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li class="dropdown-header">Part I: Foundations</li>
        <li>
    <a class="dropdown-item" href="./chapter-01.html">
 <span class="dropdown-text">Chapter 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-02.html">
 <span class="dropdown-text">Chapter 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-03.html">
 <span class="dropdown-text">Chapter 3</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part II: Core Techniques</li>
        <li>
    <a class="dropdown-item" href="./chapter-04.html">
 <span class="dropdown-text">Chapter 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-05.html">
 <span class="dropdown-text">Chapter 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-06.html">
 <span class="dropdown-text">Chapter 6</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-07.html">
 <span class="dropdown-text">Chapter 7</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part III: Architectures</li>
        <li>
    <a class="dropdown-item" href="./chapter-08.html">
 <span class="dropdown-text">Chapter 8</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-09.html">
 <span class="dropdown-text">Chapter 9</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-10.html">
 <span class="dropdown-text">Chapter 10</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part IV: Practice</li>
        <li>
    <a class="dropdown-item" href="./chapter-11.html">
 <span class="dropdown-text">Chapter 11</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-12.html">
 <span class="dropdown-text">Chapter 12</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="./appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./README.md">
 <span class="dropdown-text">About</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guokai8/mml_learning"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guokai8@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-04.html">Part II: Core Techniques</a></li><li class="breadcrumb-item"><a href="./chapter-07.html">Chapter 7: Contrastive Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">üìö Multimodal Learning: Theory, Practice, and Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./how-to-use.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Use This Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction to Multimodal Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: Foundations and Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 3: Feature Representation for Each Modality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Core Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Feature Alignment and Bridging Modalities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: Fusion Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Attention Mechanisms in Multimodal Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Chapter 7: Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part III: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Generative Models for Multimodal Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 10: Seminal Models and Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Part IV: Practice</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 11: Practical Implementation Guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 12: Advanced Topics and Future Directions</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comprehensive Appendix and Resources</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-7-contrastive-learning" id="toc-chapter-7-contrastive-learning" class="nav-link active" data-scroll-target="#chapter-7-contrastive-learning"><span class="header-section-number">1</span> Chapter 7: Contrastive Learning</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#the-problem-contrastive-learning-solves" id="toc-the-problem-contrastive-learning-solves" class="nav-link" data-scroll-target="#the-problem-contrastive-learning-solves"><span class="header-section-number">1.2</span> 7.1 The Problem Contrastive Learning Solves</a>
  <ul class="collapse">
  <li><a href="#traditional-supervised-learning" id="toc-traditional-supervised-learning" class="nav-link" data-scroll-target="#traditional-supervised-learning"><span class="header-section-number">1.2.1</span> Traditional Supervised Learning</a></li>
  <li><a href="#self-supervised-learning-intuition" id="toc-self-supervised-learning-intuition" class="nav-link" data-scroll-target="#self-supervised-learning-intuition"><span class="header-section-number">1.2.2</span> Self-Supervised Learning Intuition</a></li>
  <li><a href="#contrastive-learning-idea" id="toc-contrastive-learning-idea" class="nav-link" data-scroll-target="#contrastive-learning-idea"><span class="header-section-number">1.2.3</span> Contrastive Learning Idea</a></li>
  </ul></li>
  <li><a href="#infonce-loss---the-foundation" id="toc-infonce-loss---the-foundation" class="nav-link" data-scroll-target="#infonce-loss---the-foundation"><span class="header-section-number">1.3</span> 7.2 InfoNCE Loss - The Foundation</a>
  <ul class="collapse">
  <li><a href="#understanding-the-loss" id="toc-understanding-the-loss" class="nav-link" data-scroll-target="#understanding-the-loss"><span class="header-section-number">1.3.1</span> Understanding the Loss</a></li>
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation"><span class="header-section-number">1.3.2</span> Mathematical Formulation</a></li>
  <li><a href="#numerical-example" id="toc-numerical-example" class="nav-link" data-scroll-target="#numerical-example"><span class="header-section-number">1.3.3</span> Numerical Example</a></li>
  <li><a href="#why-this-works" id="toc-why-this-works" class="nav-link" data-scroll-target="#why-this-works"><span class="header-section-number">1.3.4</span> Why This Works</a></li>
  <li><a href="#temperature-parameter" id="toc-temperature-parameter" class="nav-link" data-scroll-target="#temperature-parameter"><span class="header-section-number">1.3.5</span> Temperature Parameter</a></li>
  </ul></li>
  <li><a href="#clip---contrastive-learning-success-story" id="toc-clip---contrastive-learning-success-story" class="nav-link" data-scroll-target="#clip---contrastive-learning-success-story"><span class="header-section-number">1.4</span> 7.3 CLIP - Contrastive Learning Success Story</a>
  <ul class="collapse">
  <li><a href="#context-and-impact" id="toc-context-and-impact" class="nav-link" data-scroll-target="#context-and-impact"><span class="header-section-number">1.4.1</span> Context and Impact</a></li>
  <li><a href="#clip-architecture" id="toc-clip-architecture" class="nav-link" data-scroll-target="#clip-architecture"><span class="header-section-number">1.4.2</span> CLIP Architecture</a></li>
  <li><a href="#training-process" id="toc-training-process" class="nav-link" data-scroll-target="#training-process"><span class="header-section-number">1.4.3</span> Training Process</a></li>
  <li><a href="#zero-shot-transfer---revolutionary-capability" id="toc-zero-shot-transfer---revolutionary-capability" class="nav-link" data-scroll-target="#zero-shot-transfer---revolutionary-capability"><span class="header-section-number">1.4.4</span> Zero-Shot Transfer - Revolutionary Capability</a></li>
  <li><a href="#benchmark-results" id="toc-benchmark-results" class="nav-link" data-scroll-target="#benchmark-results"><span class="header-section-number">1.4.5</span> Benchmark Results</a></li>
  <li><a href="#why-clip-is-revolutionary" id="toc-why-clip-is-revolutionary" class="nav-link" data-scroll-target="#why-clip-is-revolutionary"><span class="header-section-number">1.4.6</span> Why CLIP is Revolutionary</a></li>
  <li><a href="#impact-on-field" id="toc-impact-on-field" class="nav-link" data-scroll-target="#impact-on-field"><span class="header-section-number">1.4.7</span> Impact on Field</a></li>
  </ul></li>
  <li><a href="#variants-and-extensions-of-contrastive-learning" id="toc-variants-and-extensions-of-contrastive-learning" class="nav-link" data-scroll-target="#variants-and-extensions-of-contrastive-learning"><span class="header-section-number">1.5</span> 7.4 Variants and Extensions of Contrastive Learning</a>
  <ul class="collapse">
  <li><a href="#method-1-simclr---self-supervised-vision" id="toc-method-1-simclr---self-supervised-vision" class="nav-link" data-scroll-target="#method-1-simclr---self-supervised-vision"><span class="header-section-number">1.5.1</span> Method 1: SimCLR - Self-Supervised Vision</a></li>
  <li><a href="#method-2-moco---momentum-contrast" id="toc-method-2-moco---momentum-contrast" class="nav-link" data-scroll-target="#method-2-moco---momentum-contrast"><span class="header-section-number">1.5.2</span> Method 2: MoCo - Momentum Contrast</a></li>
  <li><a href="#method-3-byol---contrastive-without-negatives" id="toc-method-3-byol---contrastive-without-negatives" class="nav-link" data-scroll-target="#method-3-byol---contrastive-without-negatives"><span class="header-section-number">1.5.3</span> Method 3: BYOL - Contrastive Without Negatives</a></li>
  </ul></li>
  <li><a href="#practical-guide-to-contrastive-learning" id="toc-practical-guide-to-contrastive-learning" class="nav-link" data-scroll-target="#practical-guide-to-contrastive-learning"><span class="header-section-number">1.6</span> 7.5 Practical Guide to Contrastive Learning</a>
  <ul class="collapse">
  <li><a href="#implementing-contrastive-learning" id="toc-implementing-contrastive-learning" class="nav-link" data-scroll-target="#implementing-contrastive-learning"><span class="header-section-number">1.6.1</span> Implementing Contrastive Learning</a></li>
  <li><a href="#choosing-hyperparameters" id="toc-choosing-hyperparameters" class="nav-link" data-scroll-target="#choosing-hyperparameters"><span class="header-section-number">1.6.2</span> Choosing Hyperparameters</a></li>
  <li><a href="#evaluating-contrastive-models" id="toc-evaluating-contrastive-models" class="nav-link" data-scroll-target="#evaluating-contrastive-models"><span class="header-section-number">1.6.3</span> Evaluating Contrastive Models</a></li>
  </ul></li>
  <li><a href="#troubleshooting-contrastive-learning" id="toc-troubleshooting-contrastive-learning" class="nav-link" data-scroll-target="#troubleshooting-contrastive-learning"><span class="header-section-number">1.7</span> 7.6 Troubleshooting Contrastive Learning</a>
  <ul class="collapse">
  <li><a href="#problem-1-loss-not-decreasing" id="toc-problem-1-loss-not-decreasing" class="nav-link" data-scroll-target="#problem-1-loss-not-decreasing"><span class="header-section-number">1.7.1</span> Problem 1: Loss not decreasing</a></li>
  <li><a href="#problem-2-representation-collapse" id="toc-problem-2-representation-collapse" class="nav-link" data-scroll-target="#problem-2-representation-collapse"><span class="header-section-number">1.7.2</span> Problem 2: Representation collapse</a></li>
  <li><a href="#problem-3-slow-convergence" id="toc-problem-3-slow-convergence" class="nav-link" data-scroll-target="#problem-3-slow-convergence"><span class="header-section-number">1.7.3</span> Problem 3: Slow convergence</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">1.8</span> Key Takeaways</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">1.9</span> Exercises</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-04.html">Part II: Core Techniques</a></li><li class="breadcrumb-item"><a href="./chapter-07.html">Chapter 7: Contrastive Learning</a></li></ol></nav></header>





<section id="chapter-7-contrastive-learning" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Chapter 7: Contrastive Learning</h1>
<hr>
<p><strong>Previous</strong>: <a href="./chapter-06.html">Chapter 6: Attention Mechanisms in Multimodal Systems</a> | <strong>Next</strong>: <a href="./chapter-08.html">Chapter 8: Transformer Architecture</a> | <strong>Home</strong>: <a href="./index.html">Table of Contents</a></p>
<hr>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>After reading this chapter, you should be able to: - Understand contrastive learning principles and motivation - Implement InfoNCE loss - Understand CLIP‚Äôs revolutionary approach - Compare different contrastive methods - Apply contrastive learning to your own problems</p>
</section>
<section id="the-problem-contrastive-learning-solves" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="the-problem-contrastive-learning-solves"><span class="header-section-number">1.2</span> 7.1 The Problem Contrastive Learning Solves</h2>
<section id="traditional-supervised-learning" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="traditional-supervised-learning"><span class="header-section-number">1.2.1</span> Traditional Supervised Learning</h3>
<p><strong>Standard approach:</strong></p>
<pre><code>Training data: (input, label) pairs

Task: Image classification
  Input: Image
  Label: "cat" or "dog"

  Process:
  ‚ë† Pass image through network
  ‚ë° Output logits for each class
  ‚ë¢ Cross-entropy loss compares to label
  ‚ë£ Backprop updates weights

Requirements:
  ‚úó Requires labels for everything
  ‚úó Labels are expensive (human annotation)
  ‚úó Limited to labeled dataset size
  ‚úó New task = new labeled data needed</code></pre>
<p><strong>Bottleneck in practice:</strong></p>
<pre><code>Problem: Most data is unlabeled

Example:
  ImageNet: 1.4M labeled images
  Internet: Billions of images daily

  Ratio: ~1 labeled per 1 million unlabeled!

Question: How to leverage the vast unlabeled data?

Traditional supervised learning: Can't use it!
Solution: Contrastive learning</code></pre>
</section>
<section id="self-supervised-learning-intuition" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="self-supervised-learning-intuition"><span class="header-section-number">1.2.2</span> Self-Supervised Learning Intuition</h3>
<p><strong>Key insight:</strong></p>
<pre><code>Don't need explicit labels!
Create labels from data itself using natural structure</code></pre>
<p><strong>Example - Image rotation prediction:</strong></p>
<pre><code>Unlabeled image:
  [Photo of cat]

Create self-supervised task:
  Rotate image 90¬∞

  Rotated image ‚Üí Network ‚Üí Predict rotation

Label is free! (We created it by rotation)

Training:
  ‚ë† Rotate image by random angle (0¬∞, 90¬∞, 180¬∞, 270¬∞)
  ‚ë° Network predicts angle
  ‚ë¢ Loss: Cross-entropy between predicted and actual angle

Result:
  Network learns visual representations
  Without any human labels!

Benefit:
  Can train on billions of unlabeled images
  Representations useful for downstream tasks
  Transfer to real tasks with small labeled dataset</code></pre>
<p><strong>Why this works:</strong></p>
<pre><code>To predict rotation, network must understand:
  - What's the "up" direction? (spatial orientation)
  - What are objects and their structure? (semantics)
  - What's foreground vs background? (attention)

These are useful representations for other tasks!</code></pre>
</section>
<section id="contrastive-learning-idea" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="contrastive-learning-idea"><span class="header-section-number">1.2.3</span> Contrastive Learning Idea</h3>
<p><strong>Core concept:</strong></p>
<pre><code>Supervised learning: "Is this input A or B or C?"
Contrastive learning: "Which B is similar to A?"

Example:
  Supervised:      "Is this a dog?" (Yes/No)
  Contrastive:     "Given this dog photo, which text matches best?
                    A) 'A dog running'
                    B) 'A cat sleeping'
                    C) 'A car parked'"

Contrastive doesn't need explicit labels
Just needs relative similarities!</code></pre>
<p><strong>Why it‚Äôs powerful:</strong></p>
<pre><code>Advantage 1: No labels needed
  ‚úì Use unlabeled data directly
  ‚úì Billions of image-text pairs from web
  ‚úì Much cheaper than labeling

Advantage 2: Richer signal
  Binary classification: Yes/No (1 bit)
  Contrastive: Ranking among many (log‚ÇÇ(N) bits)

  With N=1000 options:
  Ranking gives ~10 bits of information
  vs 1 bit for binary

Advantage 3: Metric learning
  Directly optimize for similarity
  Better representations for retrieval
  Natural distance metrics</code></pre>
</section>
</section>
<section id="infonce-loss---the-foundation" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="infonce-loss---the-foundation"><span class="header-section-number">1.3</span> 7.2 InfoNCE Loss - The Foundation</h2>
<section id="understanding-the-loss" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="understanding-the-loss"><span class="header-section-number">1.3.1</span> Understanding the Loss</h3>
<p><strong>Name breakdown:</strong> - <strong>Info</strong> = Information theory - <strong>NCE</strong> = Noise Contrastive Estimation</p>
<p><strong>Goal:</strong></p>
<pre><code>Make positive pairs similar
Make negative pairs dissimilar

Positive pair: (cat image, "cat" text)
Negative pair: (cat image, "dog" text)</code></pre>
</section>
<section id="mathematical-formulation" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="mathematical-formulation"><span class="header-section-number">1.3.2</span> Mathematical Formulation</h3>
<p><strong>Formula:</strong></p>
<pre><code>L = -log [ exp(sim(q,k+)/œÑ) / (exp(sim(q,k+)/œÑ) + Œ£‚±º exp(sim(q,k‚Åª‚±º)/œÑ)) ]

Breakdown:

q = query (e.g., image)
k+ = positive key (e.g., matching text)
k‚Åª‚±º = negative keys (non-matching texts)
œÑ = temperature (controls sharpness)
sim = similarity function (cosine, dot product)</code></pre>
<p><strong>Step-by-step explanation:</strong></p>
<pre><code>Step 1: Compute similarities
  sim(query, positive) = dot product
  sim(query, negative‚ÇÅ) = dot product
  sim(query, negative‚ÇÇ) = dot product
  ...

  Result: Scores (could be any value)

Step 2: Scale by temperature
  Score / œÑ

  Temperature effect:
    œÑ small (0.01): Scores become extreme
    œÑ normal (0.1): Moderate scaling
    œÑ large (1.0): Minimal scaling

  Why temperature?
    Prevents softmax from being too sharp
    Allows gradient flow during training

Step 3: Exponential
  exp(score / œÑ)

  Result: All positive (e^x &gt; 0 for all x)

  Effect:
    Larger scores ‚Üí larger exponents
    Softmax then emphasizes them

Step 4: Softmax (normalize)
  exp(positive) / (exp(positive) + Œ£ exp(negatives))

  Result: Probability in [0, 1]

  Interpretation:
    Probability that positive is highest ranked
    Perfect: Probability = 1.0
    Random: Probability = 1/(1+num_negatives)

Step 5: Negative log
  -log(probability)

  If probability = 1.0: loss = 0 (perfect!)
  If probability = 0.1: loss = -log(0.1) = 2.3 (bad)
  If probability = 0.5: loss = -log(0.5) = 0.69 (medium)</code></pre>
</section>
<section id="numerical-example" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="numerical-example"><span class="header-section-number">1.3.3</span> Numerical Example</h3>
<p><strong>Setup:</strong></p>
<pre><code>Query: Image of red cat
Positive: Text "a red cat"
Negatives:
  - "a blue dog"
  - "a green parrot"
  - "a car"

Similarities (before temperature):
  sim(query, positive) = 0.8    (high, should be!)
  sim(query, neg1) = 0.2        (low, good)
  sim(query, neg2) = 0.15       (low, good)
  sim(query, neg3) = 0.1        (low, good)

Temperature œÑ = 0.1</code></pre>
<p><strong>Computing loss:</strong></p>
<pre><code>Step 1: Scale by temperature
  0.8 / 0.1 = 8.0
  0.2 / 0.1 = 2.0
  0.15 / 0.1 = 1.5
  0.1 / 0.1 = 1.0

Step 2: Exponentials
  e^8.0 ‚âà 2981
  e^2.0 ‚âà 7.4
  e^1.5 ‚âà 4.5
  e^1.0 ‚âà 2.7

Step 3: Softmax (probability)
  2981 / (2981 + 7.4 + 4.5 + 2.7)
  = 2981 / 2995.6
  ‚âà 0.995   (99.5% probability positive is best!)

Step 4: Loss
  loss = -log(0.995) ‚âà 0.005   (very small! Model doing great)</code></pre>
<p><strong>What if model was bad:</strong></p>
<pre><code>Similarities:
  sim(query, positive) = 0.1    (low! bad!)
  sim(query, neg1) = 0.5        (high! worse)
  sim(query, neg2) = 0.4
  sim(query, neg3) = 0.3

After temperature scaling (œÑ = 0.1):
  0.1 / 0.1 = 1.0     ‚Üí e^1.0 ‚âà 2.7
  0.5 / 0.1 = 5.0     ‚Üí e^5.0 ‚âà 148
  0.4 / 0.1 = 4.0     ‚Üí e^4.0 ‚âà 55
  0.3 / 0.1 = 3.0     ‚Üí e^3.0 ‚âà 20

Softmax:
  2.7 / (2.7 + 148 + 55 + 20)
  = 2.7 / 225.7
  ‚âà 0.012   (1.2% probability - terrible!)

Loss:
  -log(0.012) ‚âà 4.4   (very large! Forces update)</code></pre>
</section>
<section id="why-this-works" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="why-this-works"><span class="header-section-number">1.3.4</span> Why This Works</h3>
<p><strong>Mathematical properties:</strong></p>
<pre><code>1. Bounded between 0 and log(1+N)
   where N = number of negatives

   N=10: Loss ‚àà [0, log(11) ‚âà 2.4]
   N=100: Loss ‚àà [0, log(101) ‚âà 4.6]

   Interpretable scale

2. Gradient is informative

   Perfect case (prob ‚âà 1): gradient ‚âà 0
   Good case (prob ‚âà 0.9): gradient ‚âà small
   Bad case (prob ‚âà 0.1): gradient ‚âà large

   Automatically focuses on hard cases

3. Invariant to scale

   If all similarities multiplied by constant K:
   exp(K*sim) has same relative ordering
   Softmax still works correctly

   Enables using unnormalized similarities</code></pre>
</section>
<section id="temperature-parameter" class="level3" data-number="1.3.5">
<h3 data-number="1.3.5" class="anchored" data-anchor-id="temperature-parameter"><span class="header-section-number">1.3.5</span> Temperature Parameter</h3>
<p><strong>Role of œÑ:</strong></p>
<pre><code>Temperature controls softmax sharpness

œÑ = 0.01 (very cold):
  Softmax becomes nearly one-hot
  exp(5) = 148
  exp(4) = 55
  exp(3) = 20
  Ratio: 148/55 = 2.7x difference

  Large differences between outputs
  Large gradients
  Potential instability

œÑ = 0.1 (standard):
  Moderate softmax
  exp(0.5) = 1.65
  exp(0.4) = 1.49
  exp(0.3) = 1.35
  Ratio: 1.65/1.49 = 1.1x difference

  Balanced gradients
  Stable training
  Common choice

œÑ = 1.0 (very hot):
  Softmax becomes smooth
  exp(0.05) = 1.05
  exp(0.04) = 1.04
  exp(0.03) = 1.03
  Ratio: 1.05/1.04 ‚âà 1.01x difference

  Small differences between outputs
  Small gradients
  Slow learning

œÑ = 10.0 (extremely hot):
  Softmax nearly uniform
  All classes almost equally likely
  Nearly no signal
  Training doesn't work</code></pre>
<p><strong>Effect on learning:</strong></p>
<pre><code>Optimal temperature depends on:
  - Number of negatives
  - Difficulty of task
  - Data quality

Typical range: œÑ ‚àà [0.05, 0.2]

CLIP uses: œÑ ‚âà 0.07 (learned during training)</code></pre>
</section>
</section>
<section id="clip---contrastive-learning-success-story" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="clip---contrastive-learning-success-story"><span class="header-section-number">1.4</span> 7.3 CLIP - Contrastive Learning Success Story</h2>
<section id="context-and-impact" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="context-and-impact"><span class="header-section-number">1.4.1</span> Context and Impact</h3>
<p><strong>Problem statement (2020):</strong></p>
<pre><code>Existing vision models:
  - Trained on ImageNet (1.4M images)
  - Limited to 1000 classes
  - Can't generalize to new concepts
  - Require supervised fine-tuning

Question:
  Can we use web data (unsupervised) for vision?
  Can we match NLP's success with massive unlabeled data?</code></pre>
<p><strong>CLIP solution:</strong></p>
<pre><code>Data: 400M image-caption pairs from web
Task: Learn from natural language supervision
Method: Contrastive learning on image-text pairs

Result: Revolutionary zero-shot transfer</code></pre>
</section>
<section id="clip-architecture" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="clip-architecture"><span class="header-section-number">1.4.2</span> CLIP Architecture</h3>
<p><strong>Components:</strong></p>
<pre><code>Image encoder:           Text encoder:
  Vision Transformer      Transformer (BERT-like)
  Input: 224√ó224 image    Input: Text tokens
  Output: 512D vector     Output: 512D vector

            ‚Üì                     ‚Üì

    [Normalize to unit length]

            ‚Üì                     ‚Üì

    Similarity computation (dot product of normalized)

            ‚Üì

    Contrastive loss</code></pre>
<p><strong>Data collection:</strong></p>
<pre><code>400 million image-caption pairs from internet

Sources:
  - Web pages with images and captions
  - Publicly available image databases
  - Social media posts with text
  - Stock photo sites with descriptions

Quality:
  - Uncurated and diverse
  - Contains noise and biases
  - Reflects web distribution
  - Natural language (not formal labels)</code></pre>
</section>
<section id="training-process" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="training-process"><span class="header-section-number">1.4.3</span> Training Process</h3>
<p><strong>Batch construction:</strong></p>
<pre><code>Batch size: 32,768 (massive!)

Images: [img_1, img_2, ..., img_32k]
Captions: [caption_1, caption_2, ..., caption_32k]

Encode all:
  Image embeddings: 32k √ó 512
  Caption embeddings: 32k √ó 512

Compute similarity matrix (32k √ó 32k):
  sim[i,j] = image_i ¬∑ caption_j

Goal:
  Diagonal elements high (matched pairs)
  Off-diagonal elements low (mismatched pairs)</code></pre>
<p><strong>Loss computation:</strong></p>
<pre><code>For each image:
  Compute InfoNCE loss
  Positive: matching caption
  Negatives: all other 32k-1 captions

For each caption:
  Compute InfoNCE loss
  Positive: matching image
  Negatives: all other 32k-1 images

Total loss = average of all losses

Optimization:
  Adam optimizer
  Learning rate: 5√ó10‚Åª‚Å¥
  Training: ~2 weeks on large clusters</code></pre>
</section>
<section id="zero-shot-transfer---revolutionary-capability" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="zero-shot-transfer---revolutionary-capability"><span class="header-section-number">1.4.4</span> Zero-Shot Transfer - Revolutionary Capability</h3>
<p><strong>Traditional approach:</strong></p>
<pre><code>New task: Classify images of birds (not in ImageNet)

Steps:
  1. Get labeled training data for birds
  2. Fine-tune ImageNet model
  3. Get predictions

Problem: Need labeled bird data!
Cost: Expensive annotation</code></pre>
<p><strong>CLIP zero-shot approach:</strong></p>
<pre><code>New task: Classify images of birds

No training needed!

Steps:
  1. Text prompts: "a photo of a bird"
                   "a photo of a person"
                   "a photo of a car"

  2. Encode each prompt with CLIP text encoder
     ‚Üí 512D vectors

  3. For test image:
     - Encode with CLIP image encoder
     - Compute similarity to each prompt
     - Select highest similarity

  4. Done! Zero-shot classification

Example:
  Image similarity scores:
    "a photo of a bird": 0.92    ‚Üê Highest
    "a photo of a person": 0.15
    "a photo of a car": 0.08

  Prediction: Bird</code></pre>
<p><strong>Why it works:</strong></p>
<pre><code>CLIP trained on 400M diverse image-caption pairs
Learned that:
  - Images with birds cluster with "bird" text
  - Images with people cluster with "person" text
  - Images with cars cluster with "car" text

These mappings generalize to new images!

Transfer learning without fine-tuning:
  - No labeled data needed
  - No training required
  - Immediate deployment</code></pre>
</section>
<section id="benchmark-results" class="level3" data-number="1.4.5">
<h3 data-number="1.4.5" class="anchored" data-anchor-id="benchmark-results"><span class="header-section-number">1.4.5</span> Benchmark Results</h3>
<p><strong>Zero-shot transfer (ImageNet classification):</strong></p>
<pre><code>Traditional supervised:
  ResNet-50: 76.1% accuracy

CLIP zero-shot:
  CLIP-ViT-L/14: 62.8% accuracy

Seems lower, BUT:
  - CLIP trained on NO labeled images
  - Just 400M raw internet data
  - Immediately applicable to any category
  - ResNet trained with 1.4M labeled ImageNet

Adjusted for training data:
  ResNet: 76.1% on specific dataset
  CLIP: 62.8% on ANY dataset (zero-shot)

  CLIP more generalizable!</code></pre>
<p><strong>After fine-tuning on small labeled sets:</strong></p>
<pre><code>ImageNet (1% labeled):
  CLIP: 76.3% accuracy

Comparison:
  - CLIP fine-tuned with 1% labels ‚âà ResNet with 100% labels
  - 100√ó more data-efficient!
  - Shows power of pre-training</code></pre>
<p><strong>Other domains:</strong></p>
<pre><code>Transfer to new datasets:

STL10 (airplane, bird, car, etc.):
  CLIP: 92.9% zero-shot

Food101 (food classification):
  CLIP: 88.3% zero-shot

EuroSAT (satellite imagery):
  CLIP: 58.4% zero-shot

Works across diverse domains!</code></pre>
</section>
<section id="why-clip-is-revolutionary" class="level3" data-number="1.4.6">
<h3 data-number="1.4.6" class="anchored" data-anchor-id="why-clip-is-revolutionary"><span class="header-section-number">1.4.6</span> Why CLIP is Revolutionary</h3>
<p><strong>1. Scale:</strong></p>
<pre><code>400M image-text pairs &gt;&gt; 1.4M ImageNet
Shows power of scale in representation learning
Unlabeled data is abundant!</code></pre>
<p><strong>2. Natural supervision:</strong></p>
<pre><code>Language is natural way to describe images
Not forced to 1000 classes like ImageNet
Flexible descriptors
Can specify any attribute</code></pre>
<p><strong>3. Zero-shot transfer:</strong></p>
<pre><code>No fine-tuning needed
Immediate deployment
No labeled data required
Generalizes across domains</code></pre>
<p><strong>4. Open-ended prediction:</strong></p>
<pre><code>Not limited to predefined classes
Can describe images with any text
"A cat wearing a hat"
"A red car on a mountain"
Any description works!</code></pre>
</section>
<section id="impact-on-field" class="level3" data-number="1.4.7">
<h3 data-number="1.4.7" class="anchored" data-anchor-id="impact-on-field"><span class="header-section-number">1.4.7</span> Impact on Field</h3>
<pre><code>CLIP (April 2021) was watershed moment

Before CLIP:
  - Supervised learning paradigm dominant
  - Limited to ImageNet 1000 classes
  - Required labeled data for new tasks
  - Struggled on out-of-distribution data

After CLIP:
  - Contrastive learning became mainstream
  - Foundation model era began
  - Zero-shot transfer became practical
  - Industry adopted language-grounded vision

Inspired:
  - ALIGN (Google)
  - LiT (Google)
  - COCA (Meta)
  - Flamingo (DeepMind)
  - BLIP (Salesforce)
  - Many others...</code></pre>
</section>
</section>
<section id="variants-and-extensions-of-contrastive-learning" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="variants-and-extensions-of-contrastive-learning"><span class="header-section-number">1.5</span> 7.4 Variants and Extensions of Contrastive Learning</h2>
<section id="method-1-simclr---self-supervised-vision" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="method-1-simclr---self-supervised-vision"><span class="header-section-number">1.5.1</span> Method 1: SimCLR - Self-Supervised Vision</h3>
<p><strong>Motivation:</strong></p>
<pre><code>CLIP uses text for supervision
What if we only have unlabeled images?

Answer: Use image augmentations as "supervision"</code></pre>
<p><strong>Core idea:</strong></p>
<pre><code>Single image:
  [Original cat photo]

Create two augmented versions:
  [Rotated, cropped, color-adjusted]
  [Different rotation, crop, colors]

Treat as positive pair:
  Both should have similar representations
  (Same cat, different augmentations)

Negatives:
  Other images in batch

Loss: Make augmentations similar,
      other images dissimilar</code></pre>
<p><strong>Process:</strong></p>
<pre><code>1. Sample image x from dataset

2. Create two augmented versions:
   x_i = Aug(x)  (augmentation 1)
   x_j = Aug(x)  (augmentation 2)

   Different random augmentations!

3. Encode both through network f:
   h_i = f(x_i)
   h_j = f(x_j)

4. Project to embedding space:
   z_i = g(h_i)
   z_j = g(h_j)

5. Contrastive loss:
   sim(z_i, z_j) should be high
   sim(z_i, z_k) should be low (for k ‚â† i,j)

6. Backprop updates f and g</code></pre>
<p><strong>Key insights:</strong></p>
<pre><code>Why this works:

Assumptions:
  1. Augmentations preserve content
  2. Different images are different

Implications:
  Model learns representations that:
  - Survive augmentations (robust features)
  - Differ between images (discriminative features)
  - Capture semantic content (not style)

Result:
  Representations useful for downstream tasks
  Without any labels!</code></pre>
<p><strong>Augmentations used:</strong></p>
<pre><code>Strong augmentations needed for self-supervised learning:

Random crop:
  (up to 85% crop)
  ‚Üë Forces learning of part representations

Color jittering:
  Brightness, contrast, saturation, hue
  ‚Üë Prevents learning from color only

Gaussian blur:
  Blurs fine details
  ‚Üë Forces learning of structure, not pixels

Random grayscale:
  Removes color information
  ‚Üë Forces learning of shape and texture

Gaussian noise:
  Adds random noise
  ‚Üë Makes features robust

Note: Extreme augmentations avoid (would destroy content)
  - Extreme rotation: Flips meaning
  - Extreme scaling: Makes object invisible
  - Extreme distortion: No longer recognizable</code></pre>
<p><strong>Differences from CLIP:</strong></p>
<pre><code>                SimCLR          CLIP
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Supervision     Image augment   Text
Data            Unlabeled       Image-caption pairs
Requires        Images only     Images + text
Generalization  Moderate        Excellent
Task alignment  Generic vision  Language grounding
Transfer        Good            Excellent
Interpretable   No              Yes (language)

When to use:
  SimCLR: When you only have unlabeled images
  CLIP: When you have image-caption pairs</code></pre>
</section>
<section id="method-2-moco---momentum-contrast" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="method-2-moco---momentum-contrast"><span class="header-section-number">1.5.2</span> Method 2: MoCo - Momentum Contrast</h3>
<p><strong>Problem with SimCLR:</strong></p>
<pre><code>SimCLR requires large batch size:
  - Small batch: Few negatives ‚Üí weak learning signal
  - Large batch: Better negatives ‚Üí better learning

  Batch size 4096 requires massive GPU memory
  And distributed training complexity</code></pre>
<p><strong>MoCo solution:</strong></p>
<pre><code>Use memory bank instead of current batch

Benefits:
  ‚úì Can use smaller batch size
  ‚úì Negatives more diverse (from different times)
  ‚úì More efficient</code></pre>
<p><strong>Architecture:</strong></p>
<pre><code>Online encoder: f_q
  Learns from current batch
  Updated every step

Memory bank: Queue
  Stores recent representations
  Old representations pushed out as new added

Momentum encoder: f_k
  Slowly following online encoder
  f_k = Œ± √ó f_k + (1-Œ±) √ó f_q

  Typically Œ± = 0.999
  Moves slowly (momentum!)

Process:

1. Current batch through online encoder
   ‚Üí query embeddings q

2. Pop old representations from queue
   ‚Üí memory negatives

3. Compute loss using:
   - query from online encoder (positive)
   - memory from momentum encoder (negatives)

4. Push new representations to queue

5. Update momentum encoder (slowly follows online)</code></pre>
<p><strong>Why momentum encoder:</strong></p>
<pre><code>Without it:
  Queue contains representations from old network
  Network keeps changing ‚Üí representations inconsistent
  Training unstable

With momentum encoder:
  Queue contains representations from slow network
  Representations are consistent
  Training stable

Effect:
  Momentum = inertia
  Small updates accumulate
  Smooth trajectory</code></pre>
<p><strong>Performance:</strong></p>
<pre><code>ImageNet pre-training ‚Üí transfer to other tasks

                Top-1 Accuracy
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Supervised      76.5% (ResNet-50)
SimCLR          69.3% (requires large batch)
MoCo v1         60.6% (with 65K negatives)
MoCo v2         71.3% (improved version)
MoCo v3         76.7% (vision transformer)

Note: Self-supervised eventually matched supervised!
      Shows power of approach</code></pre>
</section>
<section id="method-3-byol---contrastive-without-negatives" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="method-3-byol---contrastive-without-negatives"><span class="header-section-number">1.5.3</span> Method 3: BYOL - Contrastive Without Negatives</h3>
<p><strong>Surprising finding (Grill et al., 2020):</strong></p>
<pre><code>Do we even need negative examples?

Traditional contrastive:
  Make positives similar
  Make negatives dissimilar

BYOL:
  Only make positives similar
  No explicit negatives!

Question: How does this work?

Answer: Still has implicit negatives
        (Through model architecture and learning dynamics)</code></pre>
<p><strong>Architecture:</strong></p>
<pre><code>Online network:
  Encoder f + Projector g
  Input: image ‚Üí output: representation
  Updated every step

Target network:
  Copy of online network
  Parameter updates: EMA (exponential moving average)
  target_param = Œ± √ó target_param + (1-Œ±) √ó online_param

Predictor h:
  Additional MLP on top of online network
  NOT on target network (asymmetry!)

Loss:
  For two augmentations of same image:
  loss = ||h(online(aug1)) - target(aug2)||¬≤

  Make online and target predictions close
  Using MSE loss (not contrastive!)

  Also symmetrically:
  loss += ||h(online(aug2)) - target(aug1)||¬≤</code></pre>
<p><strong>Why this works (still debated!):</strong></p>
<pre><code>Possible explanations:

1. Implicit negatives through optimization
   - Mini-batch gradient descent creates diversity
   - Network can't collapse to constant
   - Similar to negative mining

2. Momentum encoder provides stability
   - Target network changes slowly
   - Creates effective "negatives" through difference

3. Predictor prevents mode collapse
   - Without predictor: Would learn trivial solution
   - With predictor: Breaks symmetry
   - Forces meaningful learning

Empirical results:
  BYOL works surprisingly well!
  Without explicit negatives!
  Counterintuitive but effective</code></pre>
<p><strong>Advantages:</strong></p>
<pre><code>‚úì Doesn't need negative pairs
‚úì Don't need image-text pairs (image-only sufficient)
‚úì Works with small batches
‚úì Stable training
‚úì Strong performance (competitive with SimCLR)</code></pre>
<p><strong>Disadvantages:</strong></p>
<pre><code>‚úó Why it works still not fully understood
‚úó Less interpretable
‚úó More complex architecture
‚úó Harder to debug when it fails</code></pre>
</section>
</section>
<section id="practical-guide-to-contrastive-learning" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="practical-guide-to-contrastive-learning"><span class="header-section-number">1.6</span> 7.5 Practical Guide to Contrastive Learning</h2>
<section id="implementing-contrastive-learning" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="implementing-contrastive-learning"><span class="header-section-number">1.6.1</span> Implementing Contrastive Learning</h3>
<p><strong>Basic template:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ContrastiveLearningModel(nn.Module):</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, projection_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.projector <span class="op">=</span> nn.Linear(encoder.output_dim, projection_dim)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encode</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Project</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.projector(h)</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> F.normalize(z, p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ContrastiveLoss(nn.Module):</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, temperature<span class="op">=</span><span class="fl">0.07</span>):</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.temperature <span class="op">=</span> temperature</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z_i, z_j):</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute NT-Xent loss</span></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a><span class="co">        z_i, z_j: (batch_size, embedding_dim) tensors</span></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> z_i.shape[<span class="dv">0</span>]</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concatenate: positive pairs are diagonal</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> torch.cat([z_i, z_j], dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># (2*batch, dim)</span></span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Similarity matrix</span></span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> torch.mm(z, z.t()) <span class="op">/</span> <span class="va">self</span>.temperature</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create labels: diagonal elements are positives</span></span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.arange(batch_size, device<span class="op">=</span>z.device)</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.cat([labels, labels])</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Positive pairs at positions (i, batch+i) and (batch+i, i)</span></span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute loss: each sample should match its pair</span></span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loss for all positions</span></span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(similarity, labels)</span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_contrastive(model, data_loader, optimizer, device, epochs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ContrastiveLoss(temperature<span class="op">=</span><span class="fl">0.07</span>)</span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-60"><a href="#cb50-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images <span class="kw">in</span> data_loader:</span>
<span id="cb50-61"><a href="#cb50-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get two augmented versions</span></span>
<span id="cb50-62"><a href="#cb50-62" aria-hidden="true" tabindex="-1"></a>            x_i <span class="op">=</span> augment(images)</span>
<span id="cb50-63"><a href="#cb50-63" aria-hidden="true" tabindex="-1"></a>            x_j <span class="op">=</span> augment(images)</span>
<span id="cb50-64"><a href="#cb50-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-65"><a href="#cb50-65" aria-hidden="true" tabindex="-1"></a>            x_i <span class="op">=</span> x_i.to(device)</span>
<span id="cb50-66"><a href="#cb50-66" aria-hidden="true" tabindex="-1"></a>            x_j <span class="op">=</span> x_j.to(device)</span>
<span id="cb50-67"><a href="#cb50-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-68"><a href="#cb50-68" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb50-69"><a href="#cb50-69" aria-hidden="true" tabindex="-1"></a>            z_i <span class="op">=</span> model(x_i)</span>
<span id="cb50-70"><a href="#cb50-70" aria-hidden="true" tabindex="-1"></a>            z_j <span class="op">=</span> model(x_j)</span>
<span id="cb50-71"><a href="#cb50-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-72"><a href="#cb50-72" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute loss</span></span>
<span id="cb50-73"><a href="#cb50-73" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(z_i, z_j)</span>
<span id="cb50-74"><a href="#cb50-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-75"><a href="#cb50-75" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Backward pass</span></span>
<span id="cb50-76"><a href="#cb50-76" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb50-77"><a href="#cb50-77" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb50-78"><a href="#cb50-78" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb50-79"><a href="#cb50-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-80"><a href="#cb50-80" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb50-81"><a href="#cb50-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-82"><a href="#cb50-82" aria-hidden="true" tabindex="-1"></a>        avg_loss <span class="op">=</span> total_loss <span class="op">/</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb50-83"><a href="#cb50-83" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: Loss = </span><span class="sc">{</span>avg_loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="choosing-hyperparameters" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="choosing-hyperparameters"><span class="header-section-number">1.6.2</span> Choosing Hyperparameters</h3>
<p><strong>Temperature:</strong></p>
<pre><code>Range: [0.05, 0.2]

Diagnostic:
  Training loss plateaus at high value?
    ‚Üí Temperature too low (sharp, unstable)
    ‚Üí Increase œÑ

  Training loss decreases but very slowly?
    ‚Üí Temperature too high (smooth, weak signal)
    ‚Üí Decrease œÑ

Rule of thumb:
  Start with œÑ = 0.1
  Adjust based on loss curve</code></pre>
<p><strong>Batch size:</strong></p>
<pre><code>Larger batch = more negatives = better signal

Typical choices:
  Small GPU: 256-512
  Medium GPU: 1024-2048
  Large GPU: 4096+
  Multi-GPU: 32K+ (like CLIP)

Trade-off:
  Larger batch: Better learning, slower per epoch
  Smaller batch: Worse learning, faster per epoch</code></pre>
<p><strong>Projection dimension:</strong></p>
<pre><code>Embedding dimension (before projection): 1024-2048 (from encoder)
Projection dimension: 128-512

Common choices:
  256D (standard)
  128D (more compression)
  512D (less compression)

Effect:
  Smaller: Faster computation, less memory
  Larger: More expressive, risk of overfitting</code></pre>
<p><strong>Number of negatives:</strong></p>
<pre><code>Within batch:
  Batch size 256 ‚Üí 255 negatives per sample

Memory bank (MoCo):
  Queue size 65536 ‚Üí 65535 negatives

More negatives ‚Üí better learning signal
But more computation
Typical: 255-65K negatives</code></pre>
</section>
<section id="evaluating-contrastive-models" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="evaluating-contrastive-models"><span class="header-section-number">1.6.3</span> Evaluating Contrastive Models</h3>
<p><strong>Method 1: Linear evaluation protocol</strong></p>
<pre><code>1. Train contrastive model on unlabeled data
   ‚Üí Get representations

2. Freeze encoder
   ‚Üí Don't update weights

3. Train linear classifier on representations
   ‚Üí Small labeled dataset

4. Evaluate on test set

Metric: Accuracy of linear classifier
Insight: If representations good ‚Üí linear classifier accurate

Example:
  CIFAR-10 (50K training images)
  Contrastive pre-training: All 50K unlabeled
  Linear eval: 5K labeled for training, 10K for testing

  Result: 96% accuracy
  Interpretation: Representations capture meaningful patterns</code></pre>
<p><strong>Method 2: Transfer learning evaluation</strong></p>
<pre><code>1. Train contrastive model on source dataset
2. Fine-tune on target task
3. Compare to:
   - Supervised baseline
   - Random initialization
   - Other pre-training methods

Metric: Downstream task accuracy
Insight: Better representations ‚Üí better transfer</code></pre>
<p><strong>Method 3: Downstream task performance</strong></p>
<pre><code>Pre-training dataset: ImageNet (unlabeled contrastive)
Downstream tasks:
  1. ImageNet-100 classification (supervised fine-tune)
  2. CIFAR-10 classification
  3. STL10 classification
  4. Transfer to object detection
  5. Transfer to segmentation

Results show generalization across tasks</code></pre>
</section>
</section>
<section id="troubleshooting-contrastive-learning" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="troubleshooting-contrastive-learning"><span class="header-section-number">1.7</span> 7.6 Troubleshooting Contrastive Learning</h2>
<section id="problem-1-loss-not-decreasing" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1" class="anchored" data-anchor-id="problem-1-loss-not-decreasing"><span class="header-section-number">1.7.1</span> Problem 1: Loss not decreasing</h3>
<p><strong>Potential causes:</strong></p>
<pre><code>‚ë† Temperature too low
   Effect: Softmax too sharp
   Solution: Increase œÑ (e.g., 0.1 ‚Üí 0.2)

‚ë° Learning rate too small
   Effect: Updates too tiny
   Solution: Increase learning rate

‚ë¢ Batch size too small
   Effect:

-----

</code></pre>
<p>Effect: Weak learning signal Solution: Increase batch size if possible</p>
<p>‚ë£ Bad initialization Effect: Starting in bad local minimum Solution: Use proper weight initialization</p>
<p>‚ë§ Augmentations too weak Effect: Positive pairs too similar anyway Solution: Increase augmentation strength</p>
<p>‚ë• Augmentations too strong Effect: Positive pairs become different objects Solution: Decrease augmentation strength</p>
<pre><code>
**Debugging steps:**

```python
# 1. Check loss values
print(f"Initial loss: {loss.item()}")
# Should decrease over time
# If increasing or constant: something wrong

# 2. Check similarity matrix
similarity = torch.mm(z, z.t())
print(f"Max similarity: {similarity.max():.3f}")
print(f"Min similarity: {similarity.min():.3f}")
# Should: Max ‚âà 1, Min ‚âà -1 for normalized vectors

# 3. Check gradients
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm():.3f}")
# Should be reasonable values (not 0, not inf)

# 4. Check temperature effect
temperatures = [0.01, 0.05, 0.1, 0.2, 0.5]
for tau in temperatures:
    loss = compute_loss(embeddings, tau)
    print(f"œÑ={tau}: loss={loss:.3f}")
# Should have sweet spot, not too high/low everywhere</code></pre>
</section>
<section id="problem-2-representation-collapse" class="level3" data-number="1.7.2">
<h3 data-number="1.7.2" class="anchored" data-anchor-id="problem-2-representation-collapse"><span class="header-section-number">1.7.2</span> Problem 2: Representation collapse</h3>
<p><strong>What is it:</strong></p>
<pre><code>Model learns to make all representations nearly identical

Example:
  All images ‚Üí representation [0.5, 0.5, 0.5, ...]
  All images ‚Üí representation [0.51, 0.49, 0.50, ...]

  Trivial solution: "All same = all similar"
  Loss can be artificially low!
  But representations useless for downstream tasks</code></pre>
<p><strong>Symptoms:</strong></p>
<pre><code>‚úì Loss decreasing nicely
‚úó Linear evaluation performance poor
‚úó Representations clustered at single point
‚úó Variance of representations near zero</code></pre>
<p><strong>Causes and solutions:</strong></p>
<pre><code>Cause 1: No negatives (only positives)
  Solution: Ensure you have negatives in batch

Cause 2: Batch too small
  Solution: Increase batch size

Cause 3: No regularization
  Solution: Add normalization (L2 normalization helps)

Cause 4: Poor augmentations
  Solution: Ensure augmentations are meaningful
  (Reproduce the issue with weak augmentations)</code></pre>
<p><strong>Prevention:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Monitor variance</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> monitor_collapse(z):</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Check if representations are collapsing"""</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Variance across batch</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    variance <span class="op">=</span> torch.var(z, dim<span class="op">=</span><span class="dv">0</span>).mean()</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Std across batch</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> torch.std(z, dim<span class="op">=</span><span class="dv">0</span>).mean()</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Variance: </span><span class="sc">{</span>variance<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Std: </span><span class="sc">{</span>std<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> variance <span class="op">&lt;</span> <span class="fl">0.001</span>:</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"WARNING: Representations collapsing!"</span>)</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a><span class="co"># During training</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z_i, z_j <span class="kw">in</span> batches:</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> monitor_collapse(z_i):</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Take corrective action</span></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust learning rate, batch size, etc.</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="problem-3-slow-convergence" class="level3" data-number="1.7.3">
<h3 data-number="1.7.3" class="anchored" data-anchor-id="problem-3-slow-convergence"><span class="header-section-number">1.7.3</span> Problem 3: Slow convergence</h3>
<p><strong>Causes:</strong></p>
<pre><code>‚ë† Learning rate too small
   ‚Üí Gradients don't produce meaningful updates
   ‚Üí Training takes forever

‚ë° Too few negatives
   ‚Üí Weak learning signal
   ‚Üí Takes many steps to learn

‚ë¢ Bad data augmentation
   ‚Üí Positive pairs too similar/different
   ‚Üí Model confused about what to learn

‚ë£ Model too complex
   ‚Üí Slow to train
   ‚Üí Consider simpler architecture</code></pre>
<p><strong>Solutions:</strong></p>
<pre><code>1. Learning rate warmup
   Gradually increase LR from 0 to target
   Helps with stability

   Schedule:
   LR(t) = target_lr * min(1, t / warmup_steps)

2. Learning rate scheduling
   Reduce LR as training progresses
   Helps fine-tuning

   CosineAnnealingLR: Common choice

3. Increase batch size
   If hardware permits
   Each sample gets more negatives
   Stronger learning signal

4. Use momentum
   Keep moving average of gradients
   Smooths noisy gradient signal</code></pre>
</section>
</section>
<section id="key-takeaways" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">1.8</span> Key Takeaways</h2>
<ul>
<li><strong>Contrastive learning</strong> learns from similarity/dissimilarity without labels</li>
<li><strong>InfoNCE loss</strong> is the foundation: maximize positive similarity relative to negatives</li>
<li><strong>CLIP</strong> revolutionized the field with language-grounded vision at scale</li>
<li><strong>Temperature</strong> controls softmax sharpness and learning signal</li>
<li><strong>Self-supervised variants</strong> (SimCLR, MoCo, BYOL) enable learning from unlabeled data</li>
<li><strong>Large batch size</strong> provides more negatives and stronger signal</li>
<li><strong>Hyperparameter tuning</strong> (temperature, batch size, augmentation) is crucial</li>
<li><strong>Representation collapse</strong> is a real risk to monitor</li>
</ul>
</section>
<section id="exercises" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="exercises"><span class="header-section-number">1.9</span> Exercises</h2>
<p><strong>‚≠ê Beginner:</strong> 1. Implement InfoNCE loss from scratch 2. Compute temperature effects on loss 3. Understand positive/negative pairs in a batch</p>
<p><strong>‚≠ê‚≠ê Intermediate:</strong> 4. Build image-text contrastive model on small dataset 5. Implement temperature scheduling 6. Compare different similarity metrics</p>
<p><strong>‚≠ê‚≠ê‚≠ê Advanced:</strong> 7. Implement SimCLR with proper augmentations 8. Build MoCo with momentum encoder 9. Debug and fix representation collapse</p>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/guokai8\.github\.io\/mml_learning\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>¬© 2024 Kai Guo - Multimodal Learning Guide</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>