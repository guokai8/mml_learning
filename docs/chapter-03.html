<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter-03 – Multimodal Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-56df71c9454ca07313afc907ff0d97f5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Multimodal Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./preface.html"> 
<span class="menu-text">Preface</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./how-to-use.html"> 
<span class="menu-text">How to Use</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li class="dropdown-header">Part I: Foundations</li>
        <li>
    <a class="dropdown-item" href="./chapter-01.html">
 <span class="dropdown-text">Chapter 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-02.html">
 <span class="dropdown-text">Chapter 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-03.html">
 <span class="dropdown-text">Chapter 3</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part II: Core Techniques</li>
        <li>
    <a class="dropdown-item" href="./chapter-04.html">
 <span class="dropdown-text">Chapter 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-05.html">
 <span class="dropdown-text">Chapter 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-06.html">
 <span class="dropdown-text">Chapter 6</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-07.html">
 <span class="dropdown-text">Chapter 7</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part III: Architectures</li>
        <li>
    <a class="dropdown-item" href="./chapter-08.html">
 <span class="dropdown-text">Chapter 8</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-09.html">
 <span class="dropdown-text">Chapter 9</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-10.html">
 <span class="dropdown-text">Chapter 10</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part IV: Practice</li>
        <li>
    <a class="dropdown-item" href="./chapter-11.html">
 <span class="dropdown-text">Chapter 11</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-12.html">
 <span class="dropdown-text">Chapter 12</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="./appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./README.md">
 <span class="dropdown-text">About</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guokai8/mml_learning"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guokai8@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01.html">Part I: Foundations</a></li><li class="breadcrumb-item"><a href="./chapter-03.html">Chapter 3: Feature Representation for Each Modality</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📚 Multimodal Learning: Theory, Practice, and Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./how-to-use.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Use This Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction to Multimodal Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: Foundations and Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Chapter 3: Feature Representation for Each Modality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Core Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Feature Alignment and Bridging Modalities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: Fusion Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Attention Mechanisms in Multimodal Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part III: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Generative Models for Multimodal Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 10: Seminal Models and Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Part IV: Practice</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 11: Practical Implementation Guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 12: Advanced Topics and Future Directions</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comprehensive Appendix and Resources</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-3-feature-representation-for-each-modality" id="toc-chapter-3-feature-representation-for-each-modality" class="nav-link active" data-scroll-target="#chapter-3-feature-representation-for-each-modality"><span class="header-section-number">1</span> Chapter 3: Feature Representation for Each Modality</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#text-representation-evolution-and-methods" id="toc-text-representation-evolution-and-methods" class="nav-link" data-scroll-target="#text-representation-evolution-and-methods"><span class="header-section-number">1.2</span> 3.1 Text Representation: Evolution and Methods</a>
  <ul class="collapse">
  <li><a href="#historical-evolution" id="toc-historical-evolution" class="nav-link" data-scroll-target="#historical-evolution"><span class="header-section-number">1.2.1</span> Historical Evolution</a></li>
  <li><a href="#method-1-bag-of-words-bow" id="toc-method-1-bag-of-words-bow" class="nav-link" data-scroll-target="#method-1-bag-of-words-bow"><span class="header-section-number">1.2.2</span> Method 1: Bag-of-Words (BoW)</a></li>
  <li><a href="#method-2-tf-idf-term-frequency-inverse-document-frequency" id="toc-method-2-tf-idf-term-frequency-inverse-document-frequency" class="nav-link" data-scroll-target="#method-2-tf-idf-term-frequency-inverse-document-frequency"><span class="header-section-number">1.2.3</span> Method 2: TF-IDF (Term Frequency-Inverse Document Frequency)</a></li>
  <li><a href="#method-3-word2vec---learning-word-meaning" id="toc-method-3-word2vec---learning-word-meaning" class="nav-link" data-scroll-target="#method-3-word2vec---learning-word-meaning"><span class="header-section-number">1.2.4</span> Method 3: Word2Vec - Learning Word Meaning</a></li>
  <li><a href="#method-4-bert---context-aware-embeddings" id="toc-method-4-bert---context-aware-embeddings" class="nav-link" data-scroll-target="#method-4-bert---context-aware-embeddings"><span class="header-section-number">1.2.5</span> Method 4: BERT - Context-Aware Embeddings</a></li>
  <li><a href="#method-5-large-language-models-llms" id="toc-method-5-large-language-models-llms" class="nav-link" data-scroll-target="#method-5-large-language-models-llms"><span class="header-section-number">1.2.6</span> Method 5: Large Language Models (LLMs)</a></li>
  </ul></li>
  <li><a href="#image-representation-from-pixels-to-concepts" id="toc-image-representation-from-pixels-to-concepts" class="nav-link" data-scroll-target="#image-representation-from-pixels-to-concepts"><span class="header-section-number">1.3</span> 3.2 Image Representation: From Pixels to Concepts</a>
  <ul class="collapse">
  <li><a href="#historical-evolution-1" id="toc-historical-evolution-1" class="nav-link" data-scroll-target="#historical-evolution-1"><span class="header-section-number">1.3.1</span> Historical Evolution</a></li>
  <li><a href="#method-1-hand-crafted-features" id="toc-method-1-hand-crafted-features" class="nav-link" data-scroll-target="#method-1-hand-crafted-features"><span class="header-section-number">1.3.2</span> Method 1: Hand-Crafted Features</a></li>
  <li><a href="#method-2-cnns---automatic-feature-learning" id="toc-method-2-cnns---automatic-feature-learning" class="nav-link" data-scroll-target="#method-2-cnns---automatic-feature-learning"><span class="header-section-number">1.3.3</span> Method 2: CNNs - Automatic Feature Learning</a></li>
  <li><a href="#method-3-vision-transformers-vit" id="toc-method-3-vision-transformers-vit" class="nav-link" data-scroll-target="#method-3-vision-transformers-vit"><span class="header-section-number">1.3.4</span> Method 3: Vision Transformers (ViT)</a></li>
  </ul></li>
  <li><a href="#audio-representation-from-waveforms-to-features" id="toc-audio-representation-from-waveforms-to-features" class="nav-link" data-scroll-target="#audio-representation-from-waveforms-to-features"><span class="header-section-number">1.4</span> 3.3 Audio Representation: From Waveforms to Features</a>
  <ul class="collapse">
  <li><a href="#method-1-mfcc-mel-frequency-cepstral-coefficients" id="toc-method-1-mfcc-mel-frequency-cepstral-coefficients" class="nav-link" data-scroll-target="#method-1-mfcc-mel-frequency-cepstral-coefficients"><span class="header-section-number">1.4.1</span> Method 1: MFCC (Mel-Frequency Cepstral Coefficients)</a></li>
  <li><a href="#method-2-spectrogram" id="toc-method-2-spectrogram" class="nav-link" data-scroll-target="#method-2-spectrogram"><span class="header-section-number">1.4.2</span> Method 2: Spectrogram</a></li>
  <li><a href="#method-3-wav2vec2---self-supervised-learning" id="toc-method-3-wav2vec2---self-supervised-learning" class="nav-link" data-scroll-target="#method-3-wav2vec2---self-supervised-learning"><span class="header-section-number">1.4.3</span> Method 3: Wav2Vec2 - Self-Supervised Learning</a></li>
  </ul></li>
  <li><a href="#comparison-and-selection-guide" id="toc-comparison-and-selection-guide" class="nav-link" data-scroll-target="#comparison-and-selection-guide"><span class="header-section-number">1.5</span> 3.4 Comparison and Selection Guide</a>
  <ul class="collapse">
  <li><a href="#dimension-and-computational-cost" id="toc-dimension-and-computational-cost" class="nav-link" data-scroll-target="#dimension-and-computational-cost"><span class="header-section-number">1.5.1</span> Dimension and Computational Cost</a></li>
  <li><a href="#modality-comparison-summary" id="toc-modality-comparison-summary" class="nav-link" data-scroll-target="#modality-comparison-summary"><span class="header-section-number">1.5.2</span> Modality Comparison Summary</a></li>
  <li><a href="#choosing-representation" id="toc-choosing-representation" class="nav-link" data-scroll-target="#choosing-representation"><span class="header-section-number">1.5.3</span> Choosing Representation</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">1.6</span> Key Takeaways</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">1.7</span> Exercises</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01.html">Part I: Foundations</a></li><li class="breadcrumb-item"><a href="./chapter-03.html">Chapter 3: Feature Representation for Each Modality</a></li></ol></nav></header>





<section id="chapter-3-feature-representation-for-each-modality" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Chapter 3: Feature Representation for Each Modality</h1>
<hr>
<p><strong>Previous</strong>: <a href="./chapter-02.html">Chapter 2: Foundations and Core Concepts</a> | <strong>Next</strong>: <a href="./chapter-04.html">Chapter 4: Feature Alignment and Bridging Modalities</a> | <strong>Home</strong>: <a href="./index.html">Table of Contents</a></p>
<hr>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>After reading this chapter, you should be able to: - Understand text representation methods from BoW to BERT - Explain CNNs and Vision Transformers for images - Describe MFCC and self-supervised learning for audio - Compare different modality representations - Choose appropriate representations for specific tasks</p>
</section>
<section id="text-representation-evolution-and-methods" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="text-representation-evolution-and-methods"><span class="header-section-number">1.2</span> 3.1 Text Representation: Evolution and Methods</h2>
<section id="historical-evolution" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="historical-evolution"><span class="header-section-number">1.2.1</span> Historical Evolution</h3>
<pre><code>Timeline of text representation:

1950s-1990s:    Manual feature engineering
  ↓
1990s-2000s:    Bag-of-Words, TF-IDF
  ↓
2000s-2010s:    Word embeddings (Word2Vec, GloVe)
  ↓
2013-2018:      RNN, LSTM, GRU with embeddings
  ↓
2017+:          Transformer-based (BERT, GPT)
  ↓
2022+:          Large language models (GPT-3, LLaMA)
  ↓
2024+:          Multimodal LLMs</code></pre>
</section>
<section id="method-1-bag-of-words-bow" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="method-1-bag-of-words-bow"><span class="header-section-number">1.2.2</span> Method 1: Bag-of-Words (BoW)</h3>
<p><strong>Concept:</strong> Treat text as unordered collection of words, ignoring sequence and grammar.</p>
<p><strong>Process:</strong></p>
<pre><code>Input:     "The cat sat on the mat"
             ↓
Tokenize:  ["the", "cat", "sat", "on", "the", "mat"]
             ↓
Count:     {"the": 2, "cat": 1, "sat": 1, "on": 1, "mat": 1}
             ↓
Vectorize: [2, 1, 1, 1, 1]  (in vocabulary order)</code></pre>
<p><strong>Formal definition:</strong></p>
<pre><code>For vocabulary V = {w_1, w_2, ..., w_N}
Text represented as: x = [c_1, c_2, ..., c_N]
where c_i = count of word w_i in text

Dimension = vocabulary size (can be 10,000-50,000)</code></pre>
<p><strong>Example - Classification:</strong></p>
<pre><code>Training data:
  Text 1: "I love this movie" → Label: Positive
  Text 2: "This movie is bad" → Label: Negative

BoW vectors:
  Text 1: {love: 1, movie: 1, positive words}
  Text 2: {bad: 1, movie: 1, negative words}

Classifier learns:
  "love" → +positive contribution
  "bad" → +negative contribution</code></pre>
<p><strong>Advantages:</strong> ✓ Simple and fast ✓ Interpretable ✓ Works surprisingly well for many tasks</p>
<p><strong>Disadvantages:</strong> ✗ Loses word order (“dog bit man” = “man bit dog”) ✗ No semantic relationships (“happy” vs “joyful” treated as completely different) ✗ All words equally important (doesn’t distinguish important from common words) ✗ Very high dimensionality</p>
<p><strong>When to use:</strong> - Spam detection - Topic modeling - Simple text classification - When simplicity and speed are priorities</p>
</section>
<section id="method-2-tf-idf-term-frequency-inverse-document-frequency" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="method-2-tf-idf-term-frequency-inverse-document-frequency"><span class="header-section-number">1.2.3</span> Method 2: TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
<p><strong>Motivation:</strong> BoW treats all words equally. But some words are more informative than others.</p>
<p><strong>Concept:</strong></p>
<pre><code>Importance = (word frequency in document) × (rarity across corpus)

Words appearing everywhere ("the", "is") get low weight
Words appearing rarely but specifically ("CEO", "algorithm") get high weight</code></pre>
<p><strong>Formal definition:</strong></p>
<pre><code>TF (Term Frequency):
  TF(t,d) = count(t in d) / total_words(d)
  Normalized frequency of term t in document d

IDF (Inverse Document Frequency):
  IDF(t) = log(total_documents / documents_containing_t)
  How rare is this term across all documents?

TF-IDF:
  TF-IDF(t,d) = TF(t,d) × IDF(t)</code></pre>
<p><strong>Example calculation:</strong></p>
<pre><code>Corpus: 1,000 documents
Term "cat": appears in 100 documents, 5 times in document D

TF = 5 / total_words_in_D = 0.05
IDF = log(1000/100) = log(10) = 1.0
TF-IDF = 0.05 × 1.0 = 0.05

Compare to:
Term "the": appears in 900 documents, 50 times in document D

TF = 50 / total_words_in_D = 0.50
IDF = log(1000/900) = log(1.11) ≈ 0.1
TF-IDF = 0.50 × 0.1 = 0.05

Wait, same score! That's the point - importance normalized.</code></pre>
<p><strong>Benefits over BoW:</strong> ✓ Handles different document lengths better ✓ Downweights common words ✓ Emphasizes distinctive terms</p>
<p><strong>Disadvantages:</strong> ✗ Still ignores word order ✗ No semantic understanding ✗ Requires corpus statistics ✗ Doesn’t handle synonyms</p>
<p><strong>When to use:</strong> - Information retrieval and search - TF-IDF is foundation of many search engines - Document classification - When you have many documents and limited compute</p>
</section>
<section id="method-3-word2vec---learning-word-meaning" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="method-3-word2vec---learning-word-meaning"><span class="header-section-number">1.2.4</span> Method 3: Word2Vec - Learning Word Meaning</h3>
<p><strong>Revolutionary idea (Mikolov et al., 2013):</strong> “Words with similar contexts have similar meanings”</p>
<p><strong>Learning through prediction:</strong></p>
<pre><code>Idea: If we can predict context words from a word,
      we've learned what that word means.

Process:

Text: "The dog barked loudly at the mailman"
              ↓
Focus on "barked", predict context:
  Context: {dog, loudly, at, the}
  Prediction task: Given "barked", predict these

Loss: How well did we predict?
  If good prediction → "barked" representation is good
  If poor → Update "barked" vector

After training on millions of sentences:
  "barked" vector captures:
  - Associated with actions
  - Related to animals
  - Past tense
  - Physical events</code></pre>
<p><strong>Key discovery:</strong></p>
<pre><code>Vector arithmetic works!

king - man + woman ≈ queen

Explanation:
- "king" and "queen" appear in similar contexts (monarchy)
- "man" and "woman" capture gender dimension
- Vector subtraction removes gender from "king"
- Vector addition applies gender to result
- Result: "queen"

This algebraic structure wasn't hand-designed!
It emerged from learning word contexts.</code></pre>
<p><strong>Technical details - Two approaches:</strong></p>
<p><strong>Skip-gram:</strong></p>
<pre><code>Input: Target word "barked"
Task: Predict context words {dog, loudly, at, the}

Model: Two embedding matrices
  Input embedding: What is "barked"?
  Output embedding: What patterns lead to context?

Optimization:
  Maximize: P(context | barked)
  Network learns useful representations</code></pre>
<p><strong>CBOW (Continuous Bag of Words):</strong></p>
<pre><code>Input: Context words {the, dog, barked, loudly}
Task: Predict center word

Reverse of skip-gram
Can be faster to train</code></pre>
<p><strong>Properties:</strong> - Fixed embedding per word (doesn’t handle polysemy) - 300D vectors typical - Can be trained on unlabeled data - Transferable to downstream tasks</p>
<p><strong>Example - Semantic relationships:</strong></p>
<pre><code>cos_sim(king, queen) ≈ 0.7   (high, related)
cos_sim(king, man) ≈ 0.65     (high, overlapping)
cos_sim(queen, woman) ≈ 0.68  (high, overlapping)
cos_sim(king, dog) ≈ 0.2      (low, unrelated)

Structure emerges in embedding space!</code></pre>
<p><strong>Limitations:</strong> ✗ One vector per word (ignores context and polysemy) ✗ “Bank” (financial) and “bank” (river) have identical vectors ✗ Same word might mean different things in different contexts ✗ Doesn’t capture longer-range dependencies</p>
<p><strong>When to use:</strong> - Quick baseline for text tasks - When you need interpretable word relationships - Transfer learning where only word similarity needed - When computational resources are limited</p>
</section>
<section id="method-4-bert---context-aware-embeddings" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="method-4-bert---context-aware-embeddings"><span class="header-section-number">1.2.5</span> Method 4: BERT - Context-Aware Embeddings</h3>
<p><strong>Motivation:</strong></p>
<p>Word2Vec limitation - context blindness:</p>
<pre><code>Sentence 1: "I went to the bank to deposit money"
Sentence 2: "I sat on the bank of the river"

Word2Vec:
  "bank" in both sentences → IDENTICAL vector
  Problem: Different meanings!

What we need:
  Context-aware "bank" for finance sentence
  Different context-aware "bank" for river sentence</code></pre>
<p><strong>BERT Innovation (Devlin et al., 2018):</strong> “Use entire sentence context to generate embeddings”</p>
<p><strong>Architecture overview:</strong></p>
<pre><code>Input text: "The cat sat on the mat"
             ↓
Tokenization (using WordPiece):
  [CLS] The cat sat on the mat [SEP]
             ↓
Embedding:
  - Token embedding (which word)
  - Position embedding (where in sequence)
  - Segment embedding (which sentence)
             ↓
Transformer encoder (12 layers):
  Each layer:
    - Self-attention (how relevant is each token to others)
    - Feed-forward network
    - Normalization
             ↓
Output: 12 vectors of 768D each
  Each token has representation influenced by entire sequence</code></pre>
<p><strong>Key innovation - Bidirectional context:</strong></p>
<pre><code>Traditional RNN: Left-to-right only
  Input: "The cat sat..."
         Process: The → cat → sat
         When processing "sat", don't know what comes after

BERT: Bidirectional
  Input: "The cat sat on the mat"
         Process: Entire sequence simultaneously
         All positions see all other positions
         Through self-attention in first layer</code></pre>
<p><strong>Training procedure - Masked Language Modeling:</strong></p>
<pre><code>Goal: Learn good representations for any language task

Method: Predict masked words

Original:      "The [MASK] sat on the mat"
Task:          Predict the masked word
Expected:      "cat"

Training:
  ① Randomly mask 15% of tokens
  ② Model predicts masked tokens
  ③ Loss = cross-entropy between predicted and actual
  ④ Update all parameters

Result:
  Model learns representations that contain
  information about what words should appear
  = learns semantic and syntactic patterns</code></pre>
<p><strong>Using BERT embeddings:</strong></p>
<pre><code>For sentence classification:
  ① Process sentence through BERT
  ② Extract [CLS] token (special classification token)
  ③ [CLS] vector = sentence representation (768D)
  ④ Add linear classifier on top
  ⑤ Train classifier on downstream task

For token classification (e.g., NER):
  ① Process sentence through BERT
  ② Extract all token vectors (each is 768D)
  ③ Each token has context-aware representation
  ④ Add classifier for each token
  ⑤ Predict label for each token

Benefit:
  - No task-specific feature engineering needed
  - Transfer learning from massive pre-training
  - Strong performance on small datasets</code></pre>
<p><strong>Concrete example - Polysemy handling:</strong></p>
<pre><code>Sentence 1: "I went to the bank to deposit money"
  "bank" → BERT embedding with finance context

Sentence 2: "I sat on the bank of the river"
  "bank" → BERT embedding with geography context

Different embeddings!
BERT captures context from surrounding words</code></pre>
<p><strong>Properties:</strong> - Context-dependent embeddings - 768D vectors (BERT-base) - Larger versions available (BERT-large: 1024D) - Pre-trained on 3.3B words - Extremely effective for transfer learning</p>
<p><strong>Advantages over Word2Vec:</strong> ✓ Handles polysemy (same word, different contexts) ✓ Bidirectional context ✓ Pre-trained on massive corpus ✓ Strong transfer learning ✓ Achieves SOTA on many tasks</p>
<p><strong>Disadvantages:</strong> ✗ Computationally expensive ✗ Slower inference than Word2Vec ✗ Requires more compute resources ✗ Less interpretable (768D vectors hard to understand)</p>
<p><strong>When to use:</strong> - Text classification (sentiment, topic) - Named entity recognition - Question answering - Semantic similarity - When accuracy more important than speed - When GPU resources available</p>
</section>
<section id="method-5-large-language-models-llms" class="level3" data-number="1.2.6">
<h3 data-number="1.2.6" class="anchored" data-anchor-id="method-5-large-language-models-llms"><span class="header-section-number">1.2.6</span> Method 5: Large Language Models (LLMs)</h3>
<p><strong>Further evolution - GPT family:</strong></p>
<pre><code>BERT (2018):        Encoder-only, bidirectional
GPT (2018):         Decoder-only, left-to-right
GPT-2 (2019):       1.5B parameters
GPT-3 (2020):       175B parameters - in-context learning
GPT-4 (2023):       ~1.76T parameters - multimodal</code></pre>
<p><strong>LLM representations:</strong></p>
<pre><code>GPT-3 embeddings:
  Layer 1:    Basic patterns
  Layer 16:   Mid-level concepts
  Layer 32:   High-level semantics
  Layer 48 (final): Task-specific representations

Properties:
  - 12,288D vectors (very high-dimensional)
  - Captures vast knowledge
  - Can be used as semantic features
  - More interpretable than BERT in some ways</code></pre>
<p><strong>Using LLM embeddings for multimodal tasks:</strong></p>
<pre><code>Instead of using fixed word embeddings,
use representations from large language models

Benefit:
  - Captures world knowledge from pre-training
  - Understands complex semantics
  - Better for rare/unusual concepts
  - Can be adapted to specific domains

Cost:
  - Expensive API calls (if using services like OpenAI)
  - Privacy concerns (data sent to external servers)
  - Latency (requires API round-trip)</code></pre>
<p><strong>Comparison of text representations:</strong></p>
<pre><code>Method          Dimension   Context-aware   Speed   Pre-training
────────────────────────────────────────────────────────────────
BoW             10K-50K     No              Fast    None needed
TF-IDF          10K-50K     No              Fast    Corpus stats
Word2Vec        300         No              Fast    Large corpus
GloVe           300         No              Fast    Large corpus
FastText        300         No              Fast    Large corpus
ELMo            1024        Yes             Slow    Large corpus
BERT            768         Yes             Medium  Huge corpus
RoBERTa         768         Yes             Medium  Huge corpus
GPT-2           1600        Yes             Slow    Huge corpus
GPT-3           12288       Yes             Very slow API</code></pre>
</section>
</section>
<section id="image-representation-from-pixels-to-concepts" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="image-representation-from-pixels-to-concepts"><span class="header-section-number">1.3</span> 3.2 Image Representation: From Pixels to Concepts</h2>
<section id="historical-evolution-1" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="historical-evolution-1"><span class="header-section-number">1.3.1</span> Historical Evolution</h3>
<pre><code>Timeline:

1980s-1990s:    Edge detection (Canny, Sobel)
  ↓
1990s-2000s:    Hand-crafted features (SIFT, HOG)
  ↓
2012:           AlexNet - Deep learning breakthrough
  ↓
2014:           VGGNet, GoogleNet
  ↓
2015:           ResNet - Skip connections, very deep networks
  ↓
2020:           Vision Transformer - Attention-based vision
  ↓
2024:           Large multimodal models processing images</code></pre>
</section>
<section id="method-1-hand-crafted-features" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="method-1-hand-crafted-features"><span class="header-section-number">1.3.2</span> Method 1: Hand-Crafted Features</h3>
<p><strong>SIFT (Scale-Invariant Feature Transform)</strong></p>
<pre><code>Problem solved:
  "Find the same building in photos taken at different times,
   different angles, different zoom levels"

SIFT features are invariant to:
  - Translation (where object is in image)
  - Scaling (zoom level)
  - Rotation (camera angle)
  - Illumination (lighting changes)

Process:
  1. Find keypoints (interest points)
     - Corners, edges, distinctive regions

  2. Describe neighborhoods around keypoints
     - Direction and magnitude of gradients
     - Histogram of edge orientations

  3. Result: Keypoint descriptor (128D vector)
     - Invariant to many transformations
     - Can match same keypoint across images

Example:
  Building in Photo 1 (summer, noon, straight angle)
  Same building in Photo 2 (winter, sunset, aerial view)

  SIFT can find matching keypoints!
  Enables: Panorama stitching, 3D reconstruction</code></pre>
<p><strong>HOG (Histogram of Oriented Gradients)</strong></p>
<pre><code>Key insight:
  Human shape recognition relies on edge directions
  (Horizontal edges on top = head, vertical on sides = body)

Process:
  1. Divide image into cells (8×8 pixels)

  2. For each cell:
     - Compute edge direction at each pixel
     - Create histogram of edge directions

  3. Result: Concatenate all histograms
     - Captures shape and edge structure
     - Dimension: ~3,780 for 64×128 image

Application:
  Pedestrian detection
  - HOG captures distinctive human silhouette
  - Works well because human shape is distinctive
  - Fast computation (no deep learning needed)

  Limitation:
  - Only works for rigid objects (humans, faces)
  - Fails for abstract categories</code></pre>
<p><strong>Bag-of-Visual-Words</strong></p>
<pre><code>Idea: Apply Bag-of-Words concept to images

Process:
  1. Extract SIFT features from image
     → Get 100-1000 keypoint descriptors per image

  2. Cluster descriptors (k-means)
     → Create "visual vocabulary" (e.g., 1000 clusters)
     → Each cluster = one "visual word"

  3. Histogram of visual words
     → Count which words appear in image
     → Result: Bag-of-words vector

  4. Classify or compare based on histogram

Example:
  Image 1 has: {30 "corner edges", 20 "smooth curves", ...}
  Image 2 has: {5 "corner edges", 45 "smooth curves", ...}

  More curve words → Perhaps a cat
  More corner words → Perhaps a building</code></pre>
<p><strong>Advantages of hand-crafted features:</strong> ✓ Interpretable (understand what they measure) ✓ Fast computation ✓ Works with small datasets ✓ Explicit mathematical basis</p>
<p><strong>Disadvantages:</strong> ✗ Requires domain expertise to design ✗ Limited to specific feature types ✗ Poor generalization to new domains ✗ Cannot capture complex semantic patterns ✗ Manually chosen → not optimized for task</p>
<p><strong>When to use:</strong> - When you understand the specific patterns to detect - Limited computational resources - Small datasets - Tasks where hand-crafted features are well-suited (e.g., pedestrian detection)</p>
</section>
<section id="method-2-cnns---automatic-feature-learning" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="method-2-cnns---automatic-feature-learning"><span class="header-section-number">1.3.3</span> Method 2: CNNs - Automatic Feature Learning</h3>
<p><strong>The Breakthrough (AlexNet, 2012):</strong></p>
<pre><code>Revolutionary insight:
  "Stop hand-crafting features!
   Let neural networks learn what's important."

Results:
  ImageNet competition:
  - 2011 (hand-crafted): 25.8% error
  - 2012 (AlexNet): 15.3% error  ← 38% error reduction!
  - 2015 (ResNet): 3.6% error   ← Human-level performance</code></pre>
<p><strong>Hierarchical Feature Learning:</strong></p>
<pre><code>Raw image (224×224×3 pixels)
        ↓
Layer 1-2: Low-level features
  - Edge detection
  - Simple curves
  - Corners
  └─→ What: Detects local patterns
      Why: Edges are building blocks
      Output: 64 feature maps (32×32)

Layer 3-4: Mid-level features
  - Textures
  - Shapes
  - Parts
  └─→ What: Combines local patterns
      Why: Shapes emerge from edges
      Output: 256 feature maps (16×16)

Layer 5: High-level features
  - Objects
  - Semantic concepts
  - Scene context
  └─→ What: Object detectors
      Why: Objects are concepts
      Output: 512 feature maps (8×8)

Global pooling &amp; Dense layers:
  - Aggregate spatial info
  - Predict class probabilities
  └─→ Output: Class predictions</code></pre>
<p><strong>Why CNNs work:</strong></p>
<pre><code>1. Inductive bias toward images
   - Local connectivity: Nearby pixels related
   - Shared weights: Same pattern recognized anywhere
   - Translation invariance: "Cat is a cat" whether left/right

2. Hierarchical composition
   - Edges → Shapes → Objects
   - Matches how we see

3. Parameter sharing
   - Filters reused across space
   - Reduces parameters vs fully connected
   - Enables learning on larger images</code></pre>
<p><strong>Key architecture - ResNet (Residual Networks):</strong></p>
<pre><code>Problem with deep networks:
  Deeper = more parameters = better?
  But: Very deep networks are hard to train!

  Cause: Gradient vanishing
    Backprop through 100 layers:
    gradient = g₁ × g₂ × g₃ × ... × g₁₀₀

    If each gᵢ = 0.9:
    0.9¹⁰⁰ ≈ 0.0000027  (essentially zero!)

    Can't learn early layers

Solution: Skip connections (residual connections)

Normal layer: y = f(x)
Residual layer: y = x + f(x)

Benefit:
  Even if f(x) learns nothing (f(x)=0),
  y = x still flows information through

  Gradient paths:
  Without skip: gradient = ∂f/∂x × ∂f/∂x × ...
  With skip: gradient = ... + 1 + 1 + ...

  The "+1" terms prevent vanishing!</code></pre>
<p><strong>ResNet architecture example (ResNet-50):</strong></p>
<pre><code>Input: Image (224×224×3)
  ↓
Conv 7×7, stride 2
→ (112×112×64)
  ↓
MaxPool 3×3, stride 2
→ (56×56×64)
  ↓
Residual Block 1: [16 conv blocks]
→ (56×56×256)
  ↓
Residual Block 2: [33 conv blocks]
→ (28×28×512)
  ↓
Residual Block 3: [36 conv blocks]
→ (14×14×1024)
  ↓
Residual Block 4: [3 conv blocks]
→ (7×7×2048)
  ↓
Average Pool
→ (2048,)
  ↓
Linear layer (1000 classes)
→ Predictions

Total parameters: 25.5M
Depth: 50 layers
Performance: 76% ImageNet top-1 accuracy</code></pre>
<p><strong>Properties:</strong> - 2048D global feature vector (before classification) - Pre-trained on ImageNet (1.4M images) - Can fine-tune on downstream tasks - Very stable training (skip connections)</p>
<p><strong>Advantages:</strong> ✓ Learns task-relevant features ✓ Transfers well to other tasks ✓ Stable training (deep networks possible) ✓ Interpretable to some extent (visualize activations) ✓ Efficient inference</p>
<p><strong>Disadvantages:</strong> ✗ Black-box decisions (what does each dimension mean?) ✗ Requires large labeled datasets to train from scratch ✗ Inherits biases from ImageNet</p>
<p><strong>When to use:</strong> - Most modern computer vision tasks - Transfer learning (fine-tune on new task) - When you want strong off-the-shelf features - Production systems (mature, optimized, proven)</p>
</section>
<section id="method-3-vision-transformers-vit" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="method-3-vision-transformers-vit"><span class="header-section-number">1.3.4</span> Method 3: Vision Transformers (ViT)</h3>
<p><strong>Paradigm shift (Dosovitskiy et al., 2020):</strong></p>
<pre><code>Traditional thinking:
  "Images need CNNs!"
  Reason: Spatial structure, translational equivariance

ViT question:
  "What if we just use Transformers like NLP?"
  Insight: Pure attention can learn spatial patterns

Result:
  Vision Transformer outperforms ResNet
  When trained on large datasets!</code></pre>
<p><strong>Architecture:</strong></p>
<pre><code>Input image (224×224×3)
        ↓
Divide into patches (16×16)
        ↓
14×14 = 196 patches
        ↓
Each patch: 16×16×3 = 768D
        ↓
Linear projection
        ↓
196 vectors of 768D
        ↓
Add positional encoding
(so model knows spatial position)
        ↓
Add [CLS] token
(like BERT for images)
        ↓
Transformer encoder (12 layers)
        ↓
Extract [CLS] token
        ↓
768D image representation</code></pre>
<p><strong>How it works:</strong></p>
<pre><code>Key insight: Patches are like words

In NLP:
  Word tokens → Transformer → Semantic relationships

In ViT:
  Image patches → Transformer → Spatial relationships

Layer 1:
  Each patch attends to all other patches
  Learns: Which patches are related?

Layer 2-12:
  Progressively integrate information
  Layer 6: Coarse spatial understanding
  Layer 12: Fine-grained semantic understanding</code></pre>
<p><strong>Why this works:</strong></p>
<ol type="1">
<li><p><strong>Global receptive field from Layer 1</strong></p>
<p>CNN needs many layers to see globally ViT sees all patches from first layer Enables faster learning of global patterns</p></li>
<li><p><strong>Flexible to patches</strong></p>
<p>Can use any patch size Trade-off:</p>
<ul>
<li>Larger patches (32×32): Fewer tokens, less detail</li>
<li>Smaller patches (8×8): More tokens, finer detail</li>
</ul></li>
<li><p><strong>Scales with data</strong></p>
<p>CNNs strong with small data (inductive biases) ViT weak with small data, strong with large</p>
<p>Modern datasets massive → ViT wins</p></li>
</ol>
<p><strong>Example - ViT-Base vs ResNet-50:</strong></p>
<pre><code>                ViT-Base       ResNet-50
────────────────────────────────────
Parameters      86M            25.5M
ImageNet acc    77.9%          76%
Training data   1.4M+JFT      1.4M
Pre-training    224×224        1000×1000
Fine-tuning     Excellent      Good

Interpretation:
  ViT needs more data to train
  But then performs better
  Especially when transferring to new tasks</code></pre>
<p><strong>Advantages:</strong> ✓ Better scaling properties ✓ Transfers better to downstream tasks ✓ Simpler architecture (no CNN-specific tricks needed) ✓ More interpretable (attention patterns show what matters) ✓ Unified with NLP (same architecture for both)</p>
<p><strong>Disadvantages:</strong> ✗ Worse with small datasets ✗ Requires more computation than CNN equivalents ✗ Training unstable (needs careful tuning) ✗ Slower inference in some hardware</p>
<p><strong>When to use:</strong> - Large-scale applications - Transfer learning to new visual tasks - When computational resources abundant - When interpretability matters (attention visualization) - New research (faster progress with transformers)</p>
<p><strong>Attention visualization:</strong></p>
<pre><code>For each query patch, show which patches it attends to

Example - Query at cat's head position:

Attention heatmap:
[   0    0    0  ]
[   0   0.9   0.8]  (high attention to nearby patches)
[   0    0.6   0  ]

Shows:
- Model focuses on cat head region
- Attends to surrounding patches (context)
- Ignores background regions</code></pre>
</section>
</section>
<section id="audio-representation-from-waveforms-to-features" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="audio-representation-from-waveforms-to-features"><span class="header-section-number">1.4</span> 3.3 Audio Representation: From Waveforms to Features</h2>
<section id="method-1-mfcc-mel-frequency-cepstral-coefficients" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="method-1-mfcc-mel-frequency-cepstral-coefficients"><span class="header-section-number">1.4.1</span> Method 1: MFCC (Mel-Frequency Cepstral Coefficients)</h3>
<p><strong>Principle:</strong> “Extract features that match human hearing, not physics”</p>
<p><strong>Why needed:</strong></p>
<pre><code>Raw audio at 16kHz:
  1 second = 16,000 samples
  10 seconds = 160,000 samples

Problem:
  Too many numbers to process
  Not perceptually relevant (e.g., 16kHz vs 16.1kHz)

Solution:
  Extract ~39 MFCCs per frame (25ms)
  Much more compact and perceptually meaningful</code></pre>
<p><strong>Extraction process step-by-step:</strong></p>
<pre><code>① Raw waveform
   Sample audio: 16kHz, mono
   Duration: 10 seconds

② Pre-emphasis
   Amplify high frequencies
   Reason: High frequencies carry important information
   Filter: y[n] = x[n] - 0.95*x[n-1]

③ Frame division
   Split into overlapping frames
   Frame length: 25ms = 400 samples
   Hop size: 10ms
   Result: ~980 frames for 10-second audio

④ Window each frame
   Apply Hamming window: reduces edge artifacts

⑤ Fourier Transform (FFT)
   Convert time domain → frequency domain
   For each frame: 400 samples → 200 frequency bins

⑥ Mel-scale warping
   Map frequency to Mel scale (human perception)

   Linear frequency: 125Hz, 250Hz, 500Hz, 1000Hz, 2000Hz
   Mel frequency:     0Mel,   250Mel, 500Mel, 1000Mel, 1700Mel

   Why?
   Humans more sensitive to low frequencies
   High frequencies sound similar to each other
   (1000Hz difference matters less at 10,000Hz)

⑦ Logarithm
   Human loudness perception is logarithmic
   log(power) more perceptually uniform than power

⑧ Discrete Cosine Transform (DCT)
   Decorrelate the Mel-scale powers
   Result: Typically 13-39 coefficients

Result: MFCC vector
  Dimensions: 39 (or 13, 26 depending on config)
  One vector per 10ms
  Represents spectral shape at that time</code></pre>
<p><strong>Visualization:</strong></p>
<pre><code>Raw waveform:          Spectrogram:           MFCCs:
Amplitude              Frequency vs Time      Features vs Time
   ↑                      High ▲               ↑
   │ ~~~~               ▓▓▓▓▓│▓▓▓          ▓▓▓│▓▓▓
   │~  ~  ~  ~~       ▓▓▓  │▓▓▓          ▓▓ │▓▓
   │ ~ ~~  ~ ~       ▓▓   │▓           ▓  │▓
   └──────────→      ▓▓    │            ▓  │
   Time (s)         Low ▼  └─────────→ Coeff│
                         Time (s)         └─→
                                        Dim 1-39</code></pre>
<p><strong>Example - Speech recognition:</strong></p>
<pre><code>Audio: "Hello"
        ↓
MFCC extraction (39D per frame)
        ↓
10 frames of audio (each 10ms):
  Frame 1: [0.2, -0.1, 0.5, ..., 0.3] (39D)
  Frame 2: [0.21, -0.08, 0.52, ..., 0.31] (39D)
  ...
  Frame 10: [0.15, -0.12, 0.45, ..., 0.25] (39D)
        ↓
Sequence of MFCCs: 10×39 matrix
        ↓
Feed to speech recognition model
        ↓
Output: Text "Hello"</code></pre>
<p><strong>Properties:</strong> - Fixed dimensionality (39D) - Perceptually meaningful - Low computational cost - Standard for speech tasks</p>
<p><strong>Advantages:</strong> ✓ Fast to compute ✓ Well-understood (40+ years research) ✓ Works well for speech (the main audio task) ✓ Low dimensionality ✓ Perceptually meaningful</p>
<p><strong>Disadvantages:</strong> ✗ Not learnable (fixed formula) ✗ May discard useful information ✗ Optimized for speech, not music ✗ Doesn’t handle music well</p>
<p><strong>When to use:</strong> - Speech recognition - Speaker identification - Emotion recognition from speech - Music genre classification (acceptable) - Limited compute resources</p>
</section>
<section id="method-2-spectrogram" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="method-2-spectrogram"><span class="header-section-number">1.4.2</span> Method 2: Spectrogram</h3>
<p><strong>Alternative to MFCC:</strong> Keep all frequency information, don’t apply Mel-scale or DCT.</p>
<p><strong>Process:</strong></p>
<pre><code>① Raw audio
② Frame division
③ FFT
④ Magnitude spectrum
⑤ Spectrogram: stacked magnitude spectra over time

Result: 2D matrix
  Dimensions: Time × Frequency
  Values: Power at each time-frequency bin

Example: 10-second audio at 16kHz
  Time: 980 frames
  Frequency: 513 bins
  Size: 980×513</code></pre>
<p><strong>Visualization:</strong></p>
<pre><code>Spectrogram of "Hello":

Frequency
(Hz)    |▓▓ ▓▓▓▓    ▓▓    |
        |▓▓▓▓▓▓▓  ▓▓▓▓▓▓ | High freq
        |  ▓▓▓▓▓▓▓▓▓▓▓▓  |
  8000  |─────────────────|
        | ▓▓▓▓ ▓▓▓▓▓  ▓▓  |
        |▓▓▓▓ ▓▓▓▓▓▓▓▓▓   |
        |▓▓ ▓ ▓▓▓▓▓ ▓▓    | Low freq
    0   |___________________|
        0    2    4    6    8    10
              Time (seconds)

Darker = higher power
Different time positions → different audio</code></pre>
<p><strong>Advantages over MFCC:</strong> ✓ More information preserved ✓ Raw frequency content visible ✓ Can apply deep learning directly ✓ Works for any audio (not just speech)</p>
<p><strong>Disadvantages:</strong> ✗ High dimensionality (harder to process) ✗ Not perceptually normalized ✗ Less standard for speech</p>
<p><strong>When to use:</strong> - Music processing and generation - Sound event detection - When using deep learning (CNN/Transformer) - When frequency content important</p>
</section>
<section id="method-3-wav2vec2---self-supervised-learning" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="method-3-wav2vec2---self-supervised-learning"><span class="header-section-number">1.4.3</span> Method 3: Wav2Vec2 - Self-Supervised Learning</h3>
<p><strong>Modern approach (Meta AI, 2020):</strong></p>
<pre><code>Problem:
  Need thousands of hours transcribed audio for ASR
  Transcription is expensive

Solution:
  Learn from UNLABELED audio
  Use self-supervised learning</code></pre>
<p><strong>Training mechanism:</strong></p>
<pre><code>Phase 1: Pretraining (on unlabeled data)

  ① Feature extraction (CNN)
     Raw waveform → discrete codes

     Intuition: Compress speech to meaningful units

  ② Contrastive loss
     Predict masked codes from context
     Similar to BERT for speech

  Result: Model learns speech patterns
          Without any transcriptions!

Phase 2: Fine-tuning (with small labeled dataset)

  ① Load pretrained model
  ② Add task-specific head (classification)
  ③ Train on labeled examples

  Benefit: Needs much less labeled data!</code></pre>
<p><strong>Quantization step:</strong></p>
<pre><code>Why quantize speech?

Raw features: Continuous values
Problem: Too flexible, model can memorize

Quantized features: Discrete codes (e.g., 1-512)
Benefit:
  - Reduces search space
  - Forces learning of essential patterns
  - Similar to VQ-VAE for images

Example:
  Raw feature: [0.234, -0.512, 0.891, ...]
  ↓ (vector quantization)
  Nearest code ID: 147

  Code vector: Learned codebook entry 147</code></pre>
<p><strong>Architecture:</strong></p>
<pre><code>Raw waveform (16kHz)
        ↓
CNN feature extraction
        ↓
Quantization to codes
        ↓
Transformer encoder (contextual understanding)
        ↓
768D representation per frame</code></pre>
<p><strong>Training details:</strong></p>
<pre><code>Objective:
  Predict masked codes from surrounding codes

  Input: [code_1, [MASK], code_3, [MASK], code_5]
  Task: Predict masked codes

  Loss: Contrastive - predict correct code among negatives

Result:
  Encoder learns to represent speech meaningfully
  Ready for downstream tasks</code></pre>
<p><strong>Fine-tuning for tasks:</strong></p>
<pre><code>Task 1: Speech Recognition (ASR)
  Add: Linear layer for character/phoneme classification
  Train: On (audio, transcription) pairs

  Data needed: 10-100 hours labeled
  Without pretraining: 10,000+ hours needed!

Task 2: Speaker Identification
  Add: Linear layer for speaker classification
  Train: On (audio, speaker_id) pairs

Task 3: Emotion Recognition
  Add: Linear layer for emotion classification
  Train: On (audio, emotion) pairs</code></pre>
<p><strong>Empirical results:</strong></p>
<pre><code>Without Wav2Vec2 pretraining:
  ASR with 100 hours data: 25% WER (Word Error Rate)

With Wav2Vec2 pretraining:
  ASR with 100 hours data: 10% WER
  ASR with 10 hours data: 12% WER

Improvement:
  50% error reduction with same data
  Or 10× less labeled data for same performance</code></pre>
<p><strong>Properties:</strong> - 768D representation per frame - Learned from unlabeled data - Transferable across tasks - Works for any audio</p>
<p><strong>Advantages:</strong> ✓ Leverages massive unlabeled data ✓ Strong transfer learning ✓ Handles diverse audio types ✓ Better than MFCC for complex tasks</p>
<p><strong>Disadvantages:</strong> ✗ Complex training procedure ✗ Requires large unlabeled dataset for pretraining ✗ Longer inference than MFCC</p>
<p><strong>When to use:</strong> - Speech recognition (SOTA approach) - Multi-speaker systems - Low-resource languages - When accuracy is critical</p>
</section>
</section>
<section id="comparison-and-selection-guide" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="comparison-and-selection-guide"><span class="header-section-number">1.5</span> 3.4 Comparison and Selection Guide</h2>
<section id="dimension-and-computational-cost" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="dimension-and-computational-cost"><span class="header-section-number">1.5.1</span> Dimension and Computational Cost</h3>
<pre><code>                Dimension   Speed       Training Data
────────────────────────────────────────────────────
MFCC            39          Very fast   Hundreds hours
Spectrogram     513         Fast        Thousands hours
Wav2Vec2        768         Slow        Millions hours unlabeled

Hand-crafted    1000-5000   Fast        Medium
SIFT            128/keypoint Fast       Medium
HOG             3780        Fast        Medium

ResNet50        2048        Medium      1.4M images
ViT-Base        768         Medium      14M images
BERT            768         Medium      3.3B words
GPT-3           12288       Slow        Huge</code></pre>
</section>
<section id="modality-comparison-summary" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="modality-comparison-summary"><span class="header-section-number">1.5.2</span> Modality Comparison Summary</h3>
<pre><code>                Text            Image           Audio
────────────────────────────────────────────────────
Modern rep.     BERT/GPT        ResNet/ViT      Wav2Vec2
Dimension       768             2048/768        768
Interpretable   Somewhat        Little          Very little
Speed           Medium          Fast            Medium
Pre-training    Easy (text web) Requires labels Can be unsupervised
Transfer        Excellent       Good            Good
Multimodal fit  Good            Excellent       Good</code></pre>
</section>
<section id="choosing-representation" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="choosing-representation"><span class="header-section-number">1.5.3</span> Choosing Representation</h3>
<p><strong>Decision flowchart:</strong></p>
<pre><code>Is computational budget limited?
  YES → Use hand-crafted or MFCC
  NO → Continue
       ↓
Is this a production system?
  YES → Use proven methods (ResNet, BERT)
  NO → Continue
       ↓
Do you have massive labeled data?
  YES → Consider training from scratch
  NO → Use pre-trained features
       ↓
Do you have unlabeled data?
  YES → Consider self-supervised (Wav2Vec2)
  NO → Use supervised pre-trained models</code></pre>
</section>
</section>
<section id="key-takeaways" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">1.6</span> Key Takeaways</h2>
<ul>
<li><strong>Text:</strong> Evolution from BoW to BERT shows power of context</li>
<li><strong>Images:</strong> CNNs dominate but ViT shows promising future</li>
<li><strong>Audio:</strong> MFCC traditional, Wav2Vec2 is modern frontier</li>
<li><strong>Pre-training is key:</strong> Leveraging unlabeled data essential</li>
<li><strong>Different modalities need different approaches</strong></li>
<li><strong>Trade-offs exist:</strong> accuracy vs speed, interpretability vs performance</li>
</ul>
</section>
<section id="exercises" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="exercises"><span class="header-section-number">1.7</span> Exercises</h2>
<p><strong>⭐ Beginner:</strong> 1. Implement TF-IDF from scratch 2. Extract MFCC features from an audio file 3. Visualize a spectrogram</p>
<p><strong>⭐⭐ Intermediate:</strong> 4. Compare MFCC vs spectrogram representations 5. Fine-tune BERT on text classification 6. Extract ResNet features and cluster images</p>
<p><strong>⭐⭐⭐ Advanced:</strong> 7. Implement self-attention for images (simplified ViT) 8. Build Wav2Vec2 from scratch (simplified) 9. Compare different dimensionality reduction techniques</p>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/guokai8\.github\.io\/mml_learning\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Kai Guo - Multimodal Learning Guide</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>