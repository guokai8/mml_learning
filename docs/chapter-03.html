<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter-03 ‚Äì Multimodal Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-56df71c9454ca07313afc907ff0d97f5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Multimodal Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./preface.html"> 
<span class="menu-text">Preface</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./how-to-use.html"> 
<span class="menu-text">How to Use</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li class="dropdown-header">Part I: Foundations</li>
        <li>
    <a class="dropdown-item" href="./chapter-01.html">
 <span class="dropdown-text">Chapter 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-02.html">
 <span class="dropdown-text">Chapter 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-03.html">
 <span class="dropdown-text">Chapter 3</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part II: Core Techniques</li>
        <li>
    <a class="dropdown-item" href="./chapter-04.html">
 <span class="dropdown-text">Chapter 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-05.html">
 <span class="dropdown-text">Chapter 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-06.html">
 <span class="dropdown-text">Chapter 6</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-07.html">
 <span class="dropdown-text">Chapter 7</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part III: Architectures</li>
        <li>
    <a class="dropdown-item" href="./chapter-08.html">
 <span class="dropdown-text">Chapter 8</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-09.html">
 <span class="dropdown-text">Chapter 9</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-10.html">
 <span class="dropdown-text">Chapter 10</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part IV: Practice</li>
        <li>
    <a class="dropdown-item" href="./chapter-11.html">
 <span class="dropdown-text">Chapter 11</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-12.html">
 <span class="dropdown-text">Chapter 12</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="./appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./README.md">
 <span class="dropdown-text">About</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guokai8/mml_learning"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guokai8@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01.html">Part I: Foundations</a></li><li class="breadcrumb-item"><a href="./chapter-03.html">Chapter 3: Feature Representation for Each Modality</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">üìö Multimodal Learning: Theory, Practice, and Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./how-to-use.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Use This Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction to Multimodal Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: Foundations and Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Chapter 3: Feature Representation for Each Modality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Core Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Feature Alignment and Bridging Modalities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: Fusion Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Attention Mechanisms in Multimodal Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part III: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Generative Models for Multimodal Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 10: Seminal Models and Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Part IV: Practice</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 11: Practical Implementation Guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 12: Advanced Topics and Future Directions</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comprehensive Appendix and Resources</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-3-feature-representation-for-each-modality" id="toc-chapter-3-feature-representation-for-each-modality" class="nav-link active" data-scroll-target="#chapter-3-feature-representation-for-each-modality"><span class="header-section-number">1</span> Chapter 3: Feature Representation for Each Modality</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#text-representation" id="toc-text-representation" class="nav-link" data-scroll-target="#text-representation"><span class="header-section-number">1.2</span> 3.1 Text Representation</a>
  <ul class="collapse">
  <li><a href="#bag-of-words-bow---the-foundation" id="toc-bag-of-words-bow---the-foundation" class="nav-link" data-scroll-target="#bag-of-words-bow---the-foundation"><span class="header-section-number">1.2.1</span> Bag of Words (BoW) - The Foundation</a></li>
  <li><a href="#tf-idf-term-frequency-inverse-document-frequency" id="toc-tf-idf-term-frequency-inverse-document-frequency" class="nav-link" data-scroll-target="#tf-idf-term-frequency-inverse-document-frequency"><span class="header-section-number">1.2.2</span> TF-IDF (Term Frequency-Inverse Document Frequency)</a></li>
  <li><a href="#word-embeddings---semantic-vectors" id="toc-word-embeddings---semantic-vectors" class="nav-link" data-scroll-target="#word-embeddings---semantic-vectors"><span class="header-section-number">1.2.3</span> Word Embeddings - Semantic Vectors</a></li>
  </ul></li>
  <li><a href="#image-representation" id="toc-image-representation" class="nav-link" data-scroll-target="#image-representation"><span class="header-section-number">1.3</span> 3.2 Image Representation</a>
  <ul class="collapse">
  <li><a href="#classical-approaches-pre-deep-learning" id="toc-classical-approaches-pre-deep-learning" class="nav-link" data-scroll-target="#classical-approaches-pre-deep-learning"><span class="header-section-number">1.3.1</span> Classical Approaches (Pre-Deep Learning)</a></li>
  <li><a href="#deep-learning-approaches" id="toc-deep-learning-approaches" class="nav-link" data-scroll-target="#deep-learning-approaches"><span class="header-section-number">1.3.2</span> Deep Learning Approaches</a></li>
  <li><a href="#vision-transformer-vit---modern-alternative" id="toc-vision-transformer-vit---modern-alternative" class="nav-link" data-scroll-target="#vision-transformer-vit---modern-alternative"><span class="header-section-number">1.3.3</span> Vision Transformer (ViT) - Modern Alternative</a></li>
  </ul></li>
  <li><a href="#audio-representation" id="toc-audio-representation" class="nav-link" data-scroll-target="#audio-representation"><span class="header-section-number">1.4</span> 3.3 Audio Representation</a>
  <ul class="collapse">
  <li><a href="#traditional-signal-processing" id="toc-traditional-signal-processing" class="nav-link" data-scroll-target="#traditional-signal-processing"><span class="header-section-number">1.4.1</span> Traditional Signal Processing</a></li>
  <li><a href="#modern-deep-learning-approaches" id="toc-modern-deep-learning-approaches" class="nav-link" data-scroll-target="#modern-deep-learning-approaches"><span class="header-section-number">1.4.2</span> Modern Deep Learning Approaches</a></li>
  </ul></li>
  <li><a href="#practical-implementation-examples" id="toc-practical-implementation-examples" class="nav-link" data-scroll-target="#practical-implementation-examples"><span class="header-section-number">1.5</span> 3.4 Practical Implementation Examples</a>
  <ul class="collapse">
  <li><a href="#text-feature-extraction" id="toc-text-feature-extraction" class="nav-link" data-scroll-target="#text-feature-extraction"><span class="header-section-number">1.5.1</span> Text Feature Extraction</a></li>
  <li><a href="#image-feature-extraction" id="toc-image-feature-extraction" class="nav-link" data-scroll-target="#image-feature-extraction"><span class="header-section-number">1.5.2</span> Image Feature Extraction</a></li>
  <li><a href="#audio-feature-extraction" id="toc-audio-feature-extraction" class="nav-link" data-scroll-target="#audio-feature-extraction"><span class="header-section-number">1.5.3</span> Audio Feature Extraction</a></li>
  </ul></li>
  <li><a href="#debugging-feature-extraction" id="toc-debugging-feature-extraction" class="nav-link" data-scroll-target="#debugging-feature-extraction"><span class="header-section-number">1.6</span> 3.5 Debugging Feature Extraction</a>
  <ul class="collapse">
  <li><a href="#common-issues-and-solutions" id="toc-common-issues-and-solutions" class="nav-link" data-scroll-target="#common-issues-and-solutions"><span class="header-section-number">1.6.1</span> Common Issues and Solutions</a></li>
  </ul></li>
  <li><a href="#exercises-and-projects" id="toc-exercises-and-projects" class="nav-link" data-scroll-target="#exercises-and-projects"><span class="header-section-number">1.7</span> 3.6 Exercises and Projects</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">1.8</span> Key Takeaways</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">1.9</span> Further Reading</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01.html">Part I: Foundations</a></li><li class="breadcrumb-item"><a href="./chapter-03.html">Chapter 3: Feature Representation for Each Modality</a></li></ol></nav></header>





<section id="chapter-3-feature-representation-for-each-modality" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Chapter 3: Feature Representation for Each Modality</h1>
<hr>
<p><strong>Previous</strong>: <a href="./chapter-02.html">Chapter 2: Foundations and Core Concepts</a> | <strong>Next</strong>: <a href="./chapter-04.html">Chapter 4: Feature Alignment and Bridging Modalities</a> | <strong>Home</strong>: <a href="./index.html">Table of Contents</a></p>
<hr>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>After reading this chapter, you should be able to: - Extract features from text using various methods - Understand CNN architectures for image processing<br>
- Process audio signals for machine learning - Choose appropriate feature extraction for different modalities - Debug common issues in feature extraction pipelines</p>
</section>
<section id="text-representation" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="text-representation"><span class="header-section-number">1.2</span> 3.1 Text Representation</h2>
<section id="bag-of-words-bow---the-foundation" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="bag-of-words-bow---the-foundation"><span class="header-section-number">1.2.1</span> Bag of Words (BoW) - The Foundation</h3>
<p><strong>Basic concept:</strong> Represent text as word frequency counts</p>
<p><strong>Example:</strong></p>
<pre><code>Text: "The cat sat on the mat"
Vocabulary: [the, cat, sat, on, mat, dog, run, ...]

BoW representation:
[2, 1, 1, 1, 1, 0, 0, ...]
‚Üë  ‚Üë  ‚Üë  ‚Üë  ‚Üë  ‚Üë  ‚Üë
"the" appears 2 times
"cat" appears 1 time  
"sat" appears 1 time
"on" appears 1 time
"mat" appears 1 time
"dog" appears 0 times
"run" appears 0 times</code></pre>
<p><strong>Advantages:</strong> ‚úì Simple to understand and implement ‚úì Fast computation ‚úì Good baseline for many tasks ‚úì Interpretable (can see which words matter)</p>
<p><strong>Disadvantages:</strong> ‚úó Loses word order (‚Äúdog bit man‚Äù = ‚Äúman bit dog‚Äù) ‚úó No semantic relationships (‚Äúhappy‚Äù vs ‚Äújoyful‚Äù treated as completely different) ‚úó All words equally important (doesn‚Äôt distinguish important from common words) ‚úó Very high dimensionality</p>
<p><strong>When to use:</strong> - Spam detection - Topic modeling - Simple text classification - Document similarity (basic)</p>
</section>
<section id="tf-idf-term-frequency-inverse-document-frequency" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="tf-idf-term-frequency-inverse-document-frequency"><span class="header-section-number">1.2.2</span> TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
<p><strong>Improvement over BoW:</strong> Weight words by importance</p>
<p><strong>Mathematical formulation:</strong></p>
<pre><code>For term t in document d within corpus D:

TF(t,d) = count(t,d) / |d|
where:
  count(t,d) = number of times term t appears in document d
  |d| = total number of terms in document d

IDF(t,D) = log(|D| / |{d ‚àà D : t ‚àà d}|)
where:
  |D| = total number of documents in corpus
  |{d ‚àà D : t ‚àà d}| = number of documents containing term t

TF-IDF(t,d,D) = TF(t,d) √ó IDF(t,D)</code></pre>
<p><strong>Intuition:</strong></p>
<pre><code>TF (Term Frequency):
  Higher if word appears more in this document
  "This document is about cats. Cats are amazing. I love cats."
  ‚Üí "cats" gets high TF score

IDF (Inverse Document Frequency):  
  Higher if word appears in fewer documents overall
  Common words like "the", "is", "a" appear everywhere ‚Üí low IDF
  Specific words like "photosynthesis" appear rarely ‚Üí high IDF

Combined TF-IDF:
  High score = word is frequent in this document AND rare overall
  ‚Üí Indicates this word is important for characterizing this document</code></pre>
<p><strong>Example calculation:</strong></p>
<pre><code>Corpus: 1000 documents
Document: "The cat sat on the mat. The cat was happy."

For word "cat":
  TF = 2/8 = 0.25 (appears 2 times out of 8 total words)
  IDF = log(1000/50) = log(20) ‚âà 3.0 (assuming "cat" appears in 50 documents)
  TF-IDF = 0.25 √ó 3.0 = 0.75

For word "the":
  TF = 3/8 = 0.375 (appears 3 times)  
  IDF = log(1000/900) = log(1.11) ‚âà 0.1 (appears in most documents)
  TF-IDF = 0.375 √ó 0.1 = 0.0375

Result: "cat" gets much higher score than "the"</code></pre>
</section>
<section id="word-embeddings---semantic-vectors" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="word-embeddings---semantic-vectors"><span class="header-section-number">1.2.3</span> Word Embeddings - Semantic Vectors</h3>
<p><strong>Key insight:</strong> Words with similar meanings should have similar representations</p>
<section id="word2vec-2013" class="level4" data-number="1.2.3.1">
<h4 data-number="1.2.3.1" class="anchored" data-anchor-id="word2vec-2013"><span class="header-section-number">1.2.3.1</span> Word2Vec (2013)</h4>
<p><strong>Core idea:</strong> Learn embeddings from word co-occurrence patterns</p>
<p><strong>Two algorithms:</strong> 1. <strong>Skip-gram:</strong> Given center word, predict context words 2. <strong>CBOW (Continuous Bag of Words):</strong> Given context words, predict center word</p>
<p><strong>Training example (Skip-gram):</strong></p>
<pre><code>Sentence: "The cat sat on the mat"
Window size: 2

Training pairs:
Input ‚Üí Output
"cat" ‚Üí "The"     (context word 2 positions left)
"cat" ‚Üí "sat"     (context word 1 position right)  
"sat" ‚Üí "cat"     (context word 1 position left)
"sat" ‚Üí "on"      (context word 1 position right)
...

Model learns: words appearing in similar contexts get similar embeddings</code></pre>
<p><strong>Remarkable property - Semantic arithmetic:</strong></p>
<pre><code>**king** - **man** + **woman** ‚âà **queen**

Explanation:
- "king" and "queen" appear in similar contexts (monarchy)
- "man" and "woman" capture gender dimension
- Vector subtraction removes gender from "king"
- Vector addition applies gender to result
- Result: "queen"

This algebraic structure wasn't hand-designed!
It emerged from learning co-occurrence patterns.</code></pre>
<p><strong>Typical dimensions:</strong> 300D <strong>Training corpus:</strong> Billions of words from Wikipedia, news, web text</p>
</section>
<section id="bert-2018---contextual-embeddings" class="level4" data-number="1.2.3.2">
<h4 data-number="1.2.3.2" class="anchored" data-anchor-id="bert-2018---contextual-embeddings"><span class="header-section-number">1.2.3.2</span> BERT (2018) - Contextual Embeddings</h4>
<p><strong>Key improvement:</strong> Same word gets different embeddings in different contexts</p>
<p><strong>Problem Word2Vec couldn‚Äôt solve:</strong></p>
<pre><code>Sentence 1: "I went to the bank to deposit money"
Sentence 2: "I sat by the river bank to watch sunset"

Word2Vec: "bank" gets SAME embedding in both sentences
BERT: "bank" gets DIFFERENT embeddings based on context</code></pre>
<p><strong>Architecture:</strong> Transformer encoder with bidirectional attention</p>
<p><strong>Training:</strong> Masked Language Modeling</p>
<pre><code>Input: "The cat [MASK] on the mat"
Task: Predict "[MASK]" = "sat"

BERT learns to use context from BOTH sides:
- Left context: "The cat"  
- Right context: "on the mat"
- Combined context suggests "sat" is most likely</code></pre>
<p><strong>Embedding extraction:</strong></p>
<pre><code>Input: Tokenize text into [CLS] + words + [SEP]
Process: 12 transformer layers (BERT-base) or 24 layers (BERT-large)  
Output: Contextual embedding for each token

Common approaches:
1. Use [CLS] token embedding (768D) for sentence representation
2. Average word embeddings for sentence representation
3. Use specific word embeddings for word-level tasks</code></pre>
</section>
<section id="modern-large-language-models-2020" class="level4" data-number="1.2.3.3">
<h4 data-number="1.2.3.3" class="anchored" data-anchor-id="modern-large-language-models-2020"><span class="header-section-number">1.2.3.3</span> Modern Large Language Models (2020+)</h4>
<p><strong>GPT series:</strong></p>
<pre><code>GPT-1 (2018): 117M parameters, decoder-only
GPT-2 (2019): 1.5B parameters, "too dangerous to release"
GPT-3 (2020): 175B parameters, few-shot learning
GPT-4 (2023): Estimated 1T+ parameters, multimodal

Properties:
  - 12,288D vectors (very high-dimensional)
  - Captures vast knowledge
  - Can be used as semantic features
  - More interpretable than BERT in some ways</code></pre>
</section>
</section>
</section>
<section id="image-representation" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="image-representation"><span class="header-section-number">1.3</span> 3.2 Image Representation</h2>
<section id="classical-approaches-pre-deep-learning" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="classical-approaches-pre-deep-learning"><span class="header-section-number">1.3.1</span> Classical Approaches (Pre-Deep Learning)</h3>
<section id="sift-scale-invariant-feature-transform" class="level4" data-number="1.3.1.1">
<h4 data-number="1.3.1.1" class="anchored" data-anchor-id="sift-scale-invariant-feature-transform"><span class="header-section-number">1.3.1.1</span> SIFT (Scale-Invariant Feature Transform)</h4>
<p><strong>Purpose:</strong> Detect and describe local features in images that are invariant to scale, rotation, and illumination</p>
<p><strong>Process:</strong></p>
<pre><code>1. Find keypoints (interest points)
   - Corners, edges, distinctive regions

2. Describe neighborhoods around keypoints
   - Direction and magnitude of gradients
   - Histogram of edge orientations

3. Result: Keypoint descriptor (128D vector)
   - Invariant to many transformations
   - Can match same keypoint across images</code></pre>
<p><strong>Example application:</strong></p>
<pre><code>Image 1: Photo of building from front
Image 2: Photo of same building from side, different lighting

SIFT can find corresponding points:
- Corner of window in both images
- Door handle in both images  
- Logo on building in both images

Use cases:
- Image stitching (panoramas)
- Object recognition
- 3D reconstruction</code></pre>
<p><strong>Advantages:</strong> ‚úì Mathematically well-understood ‚úì Invariant to common transformations ‚úì Works without training data ‚úì Interpretable features</p>
<p><strong>Disadvantages:</strong> ‚úó Hand-crafted (not learned from data) ‚úó Limited to certain types of features ‚úó Not end-to-end optimizable ‚úó Slower than modern CNN features</p>
</section>
</section>
<section id="deep-learning-approaches" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="deep-learning-approaches"><span class="header-section-number">1.3.2</span> Deep Learning Approaches</h3>
<section id="convolutional-neural-networks-cnns" class="level4" data-number="1.3.2.1">
<h4 data-number="1.3.2.1" class="anchored" data-anchor-id="convolutional-neural-networks-cnns"><span class="header-section-number">1.3.2.1</span> Convolutional Neural Networks (CNNs)</h4>
<p><strong>Key insight:</strong> Learn hierarchical features automatically from data</p>
<p><strong>Convolution operation:</strong></p>
<pre><code>Mathematical definition:
(I * K)[i,j] = Œ£Œ£ I[i+m, j+n] √ó K[m,n]
               m n

where:
I = input image/feature map
K = kernel/filter  
* = convolution operator

Interpretation:
- Slide kernel across image
- Compute dot product at each position
- Result: Feature map showing kernel responses</code></pre>
<p><strong>Example convolution:</strong></p>
<pre><code>Input (5√ó5):          Kernel (3√ó3):
[1 2 3 4 5]          [1  0 -1]
[2 3 4 5 6]          [1  0 -1]  
[3 4 5 6 7]          [1  0 -1]
[4 5 6 7 8]
[5 6 7 8 9]

Output (3√ó3):
[0  0  0]     # Each value computed as dot product
[0  0  0]     # of kernel with corresponding image region
[0  0  0]</code></pre>
<p><strong>Feature hierarchy:</strong></p>
<pre><code>Layer 1 (early): Edge detectors
  - Vertical edges: [-1 0 1; -1 0 1; -1 0 1]
  - Horizontal edges: [-1 -1 -1; 0 0 0; 1 1 1]
  - Diagonal edges: [1 0 -1; 0 0 0; -1 0 1]

Layer 2: Simple patterns
  - Corners (combination of edges)
  - Curves  
  - Textures

Layer 3: Object parts
  - Eyes, noses (for faces)
  - Wheels, windows (for cars)
  - Leaves, branches (for trees)

Layer 4: Full objects
  - Complete faces
  - Full cars
  - Entire trees</code></pre>
</section>
<section id="resnet-residual-networks" class="level4" data-number="1.3.2.2">
<h4 data-number="1.3.2.2" class="anchored" data-anchor-id="resnet-residual-networks"><span class="header-section-number">1.3.2.2</span> ResNet (Residual Networks)</h4>
<p><strong>Motivation:</strong> Very deep networks are hard to train</p>
<p><strong>The problem:</strong></p>
<pre><code>Intuition: Deeper = more parameters = better?
But: Very deep networks are hard to train!

Cause: Gradient vanishing during backpropagation
Backprop through L layers:

‚àÇLoss/‚àÇŒ∏‚ÇÅ = ‚àÇLoss/‚àÇh_L √ó ‚àÇh_L/‚àÇh_{L-1} √ó ‚àÇh_{L-1}/‚àÇh_{L-2} √ó ... √ó ‚àÇh‚ÇÇ/‚àÇh‚ÇÅ √ó ‚àÇh‚ÇÅ/‚àÇŒ∏‚ÇÅ

Chain rule multiplication: If each ‚àÇh_i/‚àÇh_{i-1} ‚âà g &lt; 1:
Final gradient ‚âà g^L √ó (initial gradient)

Example with L=100 layers and g=0.9:
0.9¬π‚Å∞‚Å∞ ‚âà 0.0000027 (essentially zero!)

Result: Early layers receive almost no gradient signal</code></pre>
<p><strong>Solution: Skip connections (residual connections)</strong></p>
<p><strong>Architecture change:</strong></p>
<pre><code>Traditional layer: h_{i+1} = f(h_i)
Residual layer: h_{i+1} = h_i + f(h_i)

where f(h_i) is typically:
Conv ‚Üí BatchNorm ‚Üí ReLU ‚Üí Conv ‚Üí BatchNorm</code></pre>
<p><strong>Why this helps:</strong></p>
<pre><code>Benefit:
Even if f(h_i) learns nothing (f(h_i)=0),
h_{i+1} = h_i still flows information through

Gradient paths (using chain rule correctly):
Without skip connections:
  ‚àÇh_{i+1}/‚àÇh_i = f'(h_i)

With skip connections:  
  ‚àÇh_{i+1}/‚àÇh_i = ‚àÇ(h_i + f(h_i))/‚àÇh_i = 1 + f'(h_i)

The "+1" term provides direct gradient pathway!

Through L layers:
Without skip: gradient ‚àù ‚àè·µ¢ f'(h_i) (product of derivatives &lt; 1)
With skip: gradient includes terms with ‚àè·µ¢ (1 + f'(h_i)) (always ‚â• 1)

The identity mappings prevent gradient vanishing!</code></pre>
<p><strong>ResNet architecture example (ResNet-50):</strong></p>
<pre><code>Input: Image (224√ó224√ó3)
  ‚Üì
Conv 7√ó7, stride 2
‚Üí (112√ó112√ó64)
  ‚Üì
MaxPool 3√ó3, stride 2  
‚Üí (56√ó56√ó64)
  ‚Üì
Stage 1: 3 residual blocks
‚Üí (56√ó56√ó256)
  ‚Üì
Stage 2: 4 residual blocks  
‚Üí (28√ó28√ó512)
  ‚Üì
Stage 3: 6 residual blocks
‚Üí (14√ó14√ó1024)
  ‚Üì
Stage 4: 3 residual blocks
‚Üí (7√ó7√ó2048)
  ‚Üì
Global Average Pool
‚Üí (2048,)
  ‚Üì
Fully Connected
‚Üí (num_classes,)</code></pre>
<p><strong>Properties:</strong></p>
<pre><code>ResNet-50 output:
- 2048-dimensional feature vector
- Captures high-level semantic content
- Pre-trained on ImageNet (1.2M images, 1000 classes)
- Transfer learning: Works well for new tasks</code></pre>
<p><strong>Advantages:</strong> ‚úì Much deeper networks possible (50, 101, 152 layers) ‚úì Better performance than shallow networks ‚úì Stable training (deep networks possible) ‚úì Interpretable to some extent (visualize activations) ‚úì Efficient inference</p>
<p><strong>Disadvantages:</strong> ‚úó Black-box decisions (what does each dimension mean?) ‚úó Requires large labeled datasets to train from scratch ‚úó Inherits biases from ImageNet</p>
<p><strong>When to use:</strong> - Most modern computer vision tasks - Transfer learning base - Feature extraction for multimodal systems</p>
</section>
</section>
<section id="vision-transformer-vit---modern-alternative" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="vision-transformer-vit---modern-alternative"><span class="header-section-number">1.3.3</span> Vision Transformer (ViT) - Modern Alternative</h3>
<p><strong>Key idea:</strong> Treat image patches as sequence tokens, apply transformer</p>
<p><strong>Process:</strong></p>
<pre><code>1. Split image into patches (e.g., 16√ó16 patches)
   224√ó224 image ‚Üí 14√ó14 = 196 patches

2. Linear projection of each patch
   16√ó16√ó3 = 768D ‚Üí Linear layer ‚Üí 768D embedding

3. Add positional embeddings
   Patch embeddings + position info

4. Transformer encoder  
   Self-attention across patches

5. Classification token [CLS]
   Final representation for whole image</code></pre>
<p><strong>Comparison with CNNs:</strong></p>
<pre><code>CNNs:                    ViTs:
- Inductive bias         - Less inductive bias
- Local connectivity     - Global attention
- Translation equivariance - Learned spatial relationships  
- Smaller datasets OK    - Needs large datasets
- More efficient         - More computation</code></pre>
</section>
</section>
<section id="audio-representation" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="audio-representation"><span class="header-section-number">1.4</span> 3.3 Audio Representation</h2>
<section id="traditional-signal-processing" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="traditional-signal-processing"><span class="header-section-number">1.4.1</span> Traditional Signal Processing</h3>
<section id="mel-frequency-cepstral-coefficients-mfcc" class="level4" data-number="1.4.1.1">
<h4 data-number="1.4.1.1" class="anchored" data-anchor-id="mel-frequency-cepstral-coefficients-mfcc"><span class="header-section-number">1.4.1.1</span> Mel-frequency Cepstral Coefficients (MFCC)</h4>
<p><strong>Purpose:</strong> Extract perceptually meaningful features from audio</p>
<p><strong>Process:</strong></p>
<pre><code>1. Pre-emphasis filter
   Boost high frequencies
   
2. Windowing  
   Split audio into overlapping frames (25ms windows, 10ms step)
   
3. FFT (Fast Fourier Transform)
   Time domain ‚Üí Frequency domain
   
4. Mel filter bank
   Human auditory perception-based frequency spacing
   
5. Logarithm
   Compress dynamic range
   
6. DCT (Discrete Cosine Transform)
   Decorrelate features
   
Output: Typically 13 MFCC coefficients per frame</code></pre>
<p><strong>Visual representation:</strong></p>
<pre><code>Audio waveform:
Time: 0----1----2----3----4----5 seconds
Amplitude: ‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº‚àº
        ‚Üì
MFCC features (13 √ó num_frames):
Frame: 1    2    3    4    5   ...
c‚ÇÅ:   [2.1  1.8  2.3  1.9  2.0]
c‚ÇÇ:   [0.5  0.3  0.7  0.4  0.6]  
c‚ÇÉ:   [-0.2 0.1 -0.3  0.0 -0.1]
...
c‚ÇÅ‚ÇÉ:  [0.8  0.9  0.7  1.0  0.8]
        ‚Üì
Final representation per utterance:
Statistical summary (mean, std) ‚Üí 26D vector
Or sequence of frame vectors for RNN processing</code></pre>
<p><strong>Example use case:</strong></p>
<pre><code>Input: Audio "Hello"
        ‚Üì
MFCC extraction
        ‚Üì
Output: Text "Hello"</code></pre>
<p><strong>Properties:</strong> - Fixed dimensionality (39D total: 13 MFCC + 13 Œî + 13 ŒîŒî) - Perceptually meaningful - Low computational cost - Standard for speech tasks</p>
<p><strong>Advantages:</strong> ‚úì Fast to compute ‚úì Well-understood (40+ years research) ‚úì Works well for speech (the main audio task) ‚úì Low dimensionality ‚úì Perceptually meaningful</p>
<p><strong>Disadvantages:</strong> ‚úó Not learnable (fixed formula) ‚úó May discard useful information ‚úó Designed specifically for speech ‚úó Not optimal for music or environmental sounds</p>
</section>
<section id="spectrograms" class="level4" data-number="1.4.1.2">
<h4 data-number="1.4.1.2" class="anchored" data-anchor-id="spectrograms"><span class="header-section-number">1.4.1.2</span> Spectrograms</h4>
<p><strong>Purpose:</strong> Visualize frequency content over time</p>
<p><strong>Types:</strong></p>
<pre><code>1. Linear spectrogram:
   FFT magnitudes plotted over time
   Y-axis: Frequency (0 to Nyquist)
   X-axis: Time
   Color: Magnitude

2. Log spectrogram:
   Log-scale frequency axis
   Better for human perception

3. Mel spectrogram:
   Mel-scale frequency axis
   Even better perceptual modeling</code></pre>
<p><strong>Advantages:</strong> ‚úì Complete frequency information preserved ‚úì Raw frequency content visible ‚úì Can apply deep learning directly ‚úì Works for any audio (not just speech)</p>
<p><strong>Disadvantages:</strong> ‚úó High dimensionality (harder to process) ‚úó Not perceptually normalized ‚úó Less standard for speech</p>
<p><strong>When to use:</strong> - Music processing and generation - Environmental sound classification - Any audio task where full frequency content matters</p>
</section>
</section>
<section id="modern-deep-learning-approaches" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="modern-deep-learning-approaches"><span class="header-section-number">1.4.2</span> Modern Deep Learning Approaches</h3>
<section id="wav2vec-2.0" class="level4" data-number="1.4.2.1">
<h4 data-number="1.4.2.1" class="anchored" data-anchor-id="wav2vec-2.0"><span class="header-section-number">1.4.2.1</span> Wav2Vec 2.0</h4>
<p><strong>Purpose:</strong> Learn audio representations from raw waveforms</p>
<p><strong>Architecture:</strong></p>
<pre><code>Raw audio waveform
        ‚Üì
CNN encoder (6 layers)
        ‚Üì  
Quantization module
        ‚Üì
Transformer (12 layers)
        ‚Üì
Contextualized representations (768D per timestep)</code></pre>
<p><strong>Training:</strong> Self-supervised contrastive learning</p>
<pre><code>1. Mask portions of audio  
2. Learn to predict masked regions
3. Use contrastive loss (similar to BERT for text)

Result: Rich audio representations without labeled data</code></pre>
<p><strong>Advantages:</strong> ‚úì Learned from data (not hand-crafted) ‚úì Works across different audio domains ‚úì State-of-the-art for many audio tasks ‚úì Can fine-tune for specific tasks</p>
<p><strong>Disadvantages:</strong> ‚úó Requires large amounts of training data ‚úó Computationally expensive ‚úó Black-box (hard to interpret)</p>
</section>
</section>
</section>
<section id="practical-implementation-examples" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="practical-implementation-examples"><span class="header-section-number">1.5</span> 3.4 Practical Implementation Examples</h2>
<section id="text-feature-extraction" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="text-feature-extraction"><span class="header-section-number">1.5.1</span> Text Feature Extraction</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TextFeatureExtractor:</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, text):</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract BERT features from text"""</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tokenize</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>, </span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract features</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>inputs)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use [CLS] token embedding as sentence representation</span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        sentence_embedding <span class="op">=</span> outputs.last_hidden_state[:, <span class="dv">0</span>, :]  <span class="co"># Shape: (1, 768)</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sentence_embedding.numpy()</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage</span></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> TextFeatureExtractor()</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> extractor.extract_features(<span class="st">"A cute cat sitting on a mat"</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Text features shape: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># (1, 768)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="image-feature-extraction" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="image-feature-extraction"><span class="header-section-number">1.5.2</span> Image Feature Extraction</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageFeatureExtractor:</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load pre-trained ResNet</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.fc <span class="op">=</span> torch.nn.Identity()  <span class="co"># Remove final classifier</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard ImageNet preprocessing</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.preprocess <span class="op">=</span> transforms.Compose([</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], </span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>                               std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]),</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_features(<span class="va">self</span>, image_path):</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract ResNet features from image"""</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load and preprocess image</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(image_path).convert(<span class="st">'RGB'</span>)</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>        image_tensor <span class="op">=</span> <span class="va">self</span>.preprocess(image).unsqueeze(<span class="dv">0</span>)  <span class="co"># Add batch dimension</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract features</span></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>            features <span class="op">=</span> <span class="va">self</span>.model(image_tensor)  <span class="co"># Shape: (1, 2048)</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> features.numpy()</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage  </span></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> ImageFeatureExtractor()</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> extractor.extract_features(<span class="st">"cat.jpg"</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image features shape: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># (1, 2048)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="audio-feature-extraction" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="audio-feature-extraction"><span class="header-section-number">1.5.3</span> Audio Feature Extraction</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> librosa</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Wav2Vec2Processor, Wav2Vec2Model</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AudioFeatureExtractor:</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Traditional MFCC</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sample_rate <span class="op">=</span> <span class="dv">16000</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Modern Wav2Vec2</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.processor <span class="op">=</span> Wav2Vec2Processor.from_pretrained(<span class="st">"facebook/wav2vec2-base"</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> Wav2Vec2Model.from_pretrained(<span class="st">"facebook/wav2vec2-base"</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_mfcc(<span class="va">self</span>, audio_path):</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract MFCC features"""</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load audio</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        audio, sr <span class="op">=</span> librosa.load(audio_path, sr<span class="op">=</span><span class="va">self</span>.sample_rate)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract MFCC (13 coefficients)</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        mfcc <span class="op">=</span> librosa.feature.mfcc(y<span class="op">=</span>audio, sr<span class="op">=</span>sr, n_mfcc<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Take statistical summary</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>        mfcc_mean <span class="op">=</span> np.mean(mfcc, axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (13,)</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        mfcc_std <span class="op">=</span> np.std(mfcc, axis<span class="op">=</span><span class="dv">1</span>)    <span class="co"># (13,)</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.concatenate([mfcc_mean, mfcc_std])  <span class="co"># (26,)</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_wav2vec(<span class="va">self</span>, audio_path):</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract Wav2Vec2 features"""</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load audio</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        audio, sr <span class="op">=</span> librosa.load(audio_path, sr<span class="op">=</span><span class="dv">16000</span>)</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process audio</span></span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.processor(audio, sampling_rate<span class="op">=</span><span class="dv">16000</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract features</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>inputs)</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Average over time dimension</span></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> outputs.last_hidden_state.mean(dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Shape: (1, 768)</span></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> features.numpy()</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage</span></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> AudioFeatureExtractor()</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>mfcc_features <span class="op">=</span> extractor.extract_mfcc(<span class="st">"hello.wav"</span>)</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>wav2vec_features <span class="op">=</span> extractor.extract_wav2vec(<span class="st">"hello.wav"</span>)</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MFCC features shape: </span><span class="sc">{</span>mfcc_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)      <span class="co"># (26,)</span></span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Wav2Vec features shape: </span><span class="sc">{</span>wav2vec_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>) <span class="co"># (1, 768)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="debugging-feature-extraction" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="debugging-feature-extraction"><span class="header-section-number">1.6</span> 3.5 Debugging Feature Extraction</h2>
<section id="common-issues-and-solutions" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="common-issues-and-solutions"><span class="header-section-number">1.6.1</span> Common Issues and Solutions</h3>
<p><strong>Issue 1: Features are all zeros or very small</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> debug_features(features, name<span class="op">=</span><span class="st">"features"</span>):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> statistics:"</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Shape: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Min: </span><span class="sc">{</span>features<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Max: </span><span class="sc">{</span>features<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Mean: </span><span class="sc">{</span>features<span class="sc">.</span>mean()<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Std: </span><span class="sc">{</span>features<span class="sc">.</span>std()<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Zeros: </span><span class="sc">{</span>(features <span class="op">==</span> <span class="dv">0</span>)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> / </span><span class="sc">{</span>features<span class="sc">.</span>size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.std() <span class="op">&lt;</span> <span class="fl">1e-6</span>:</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  WARNING: Very low variance - check preprocessing!"</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.isnan(features).<span class="bu">any</span>():</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  WARNING: NaN values detected!"</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.isinf(features).<span class="bu">any</span>():</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  WARNING: Infinite values detected!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Issue 2: Inconsistent feature scales across modalities</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_features(features, method<span class="op">=</span><span class="st">'l2'</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Normalize features for consistent scale"""</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> method <span class="op">==</span> <span class="st">'l2'</span>:</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># L2 normalization (unit length)</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        norm <span class="op">=</span> np.linalg.norm(features, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> features <span class="op">/</span> (norm <span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'zscore'</span>:</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Z-score normalization  </span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> features.mean(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> features.std(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (features <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'minmax'</span>:</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Min-max normalization</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        min_val <span class="op">=</span> features.<span class="bu">min</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        max_val <span class="op">=</span> features.<span class="bu">max</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (features <span class="op">-</span> min_val) <span class="op">/</span> (max_val <span class="op">-</span> min_val <span class="op">+</span> <span class="fl">1e-8</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Issue 3: Memory issues with large feature matrices</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_feature_extraction(file_paths, extractor, batch_size<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Process files in batches to avoid memory issues"""</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> []</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(file_paths), batch_size):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        batch_paths <span class="op">=</span> file_paths[i:i<span class="op">+</span>batch_size]</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        batch_features <span class="op">=</span> []</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> path <span class="kw">in</span> batch_paths:</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>            feat <span class="op">=</span> extractor.extract_features(path)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>            batch_features.append(feat)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stack batch and free memory</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        batch_features <span class="op">=</span> np.vstack(batch_features)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        features.append(batch_features)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Progress indicator</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Processed </span><span class="sc">{</span><span class="bu">min</span>(i<span class="op">+</span>batch_size, <span class="bu">len</span>(file_paths))<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(file_paths)<span class="sc">}</span><span class="ss"> files"</span>)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.vstack(features)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="exercises-and-projects" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="exercises-and-projects"><span class="header-section-number">1.7</span> 3.6 Exercises and Projects</h2>
<p><strong>‚≠ê Beginner:</strong> 1. Implement BoW and TF-IDF from scratch 2. Extract MFCC features from audio files 3. Visualize CNN filter responses on images 4. Compare different text representations on sentiment analysis</p>
<p><strong>‚≠ê‚≠ê Intermediate:</strong> 5. Fine-tune BERT on domain-specific text 6. Extract ResNet features and cluster images</p>
<p><strong>‚≠ê‚≠ê‚≠ê Advanced:</strong> 7. Implement self-attention for images (simplified ViT) 8. Build Wav2Vec2 from scratch (simplified) 9. Compare different dimensionality reduction techniques</p>
<hr>
</section>
<section id="key-takeaways" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">1.8</span> Key Takeaways</h2>
<ul>
<li><strong>Text representations</strong> evolved from simple BoW to contextual embeddings (BERT, GPT)</li>
<li><strong>Image features</strong> benefit from hierarchical processing (CNNs) and skip connections (ResNet)</li>
<li><strong>Audio processing</strong> uses both traditional signal processing (MFCC) and modern deep learning (Wav2Vec2)</li>
<li><strong>Feature quality</strong> is crucial for downstream multimodal tasks</li>
<li><strong>Normalization</strong> is essential when combining features from different modalities</li>
<li><strong>Debugging tools</strong> help identify and fix common feature extraction issues</li>
</ul>
</section>
<section id="further-reading" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">1.9</span> Further Reading</h2>
<p><strong>Text Representations:</strong> - Mikolov, T., et al.&nbsp;(2013). Efficient Estimation of Word Representations in Vector Space. <em>arXiv:1301.3781</em> - Devlin, J., et al.&nbsp;(2018). BERT: Pre-training of Deep Bidirectional Transformers. <em>arXiv:1810.04805</em></p>
<p><strong>Computer Vision:</strong> - He, K., et al.&nbsp;(2016). Deep Residual Learning for Image Recognition. <em>CVPR 2016</em> - Dosovitskiy, A., et al.&nbsp;(2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. <em>ICLR 2021</em></p>
<p><strong>Audio Processing:</strong> - Baevski, A., et al.&nbsp;(2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. <em>NeurIPS 2020</em></p>
<hr>
<p><strong>Previous</strong>: <a href="./chapter-02.html">Chapter 2: Foundations and Core Concepts</a> | <strong>Next</strong>: <a href="./chapter-04.html">Chapter 4: Feature Alignment and Bridging Modalities</a> | <strong>Home</strong>: <a href="./index.html">Table of Contents</a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>¬© 2024 Kai Guo - Multimodal Learning Guide</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>