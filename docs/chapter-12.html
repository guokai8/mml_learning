<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter-12 – Multimodal Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-56df71c9454ca07313afc907ff0d97f5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-1bebf2fac2c66d78ee8e4a0e5b34d43e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Multimodal Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./preface.html"> 
<span class="menu-text">Preface</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./how-to-use.html"> 
<span class="menu-text">How to Use</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li class="dropdown-header">Part I: Foundations</li>
        <li>
    <a class="dropdown-item" href="./chapter-01.html">
 <span class="dropdown-text">Chapter 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-02.html">
 <span class="dropdown-text">Chapter 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-03.html">
 <span class="dropdown-text">Chapter 3</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part II: Core Techniques</li>
        <li>
    <a class="dropdown-item" href="./chapter-04.html">
 <span class="dropdown-text">Chapter 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-05.html">
 <span class="dropdown-text">Chapter 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-06.html">
 <span class="dropdown-text">Chapter 6</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-07.html">
 <span class="dropdown-text">Chapter 7</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part III: Architectures</li>
        <li>
    <a class="dropdown-item" href="./chapter-08.html">
 <span class="dropdown-text">Chapter 8</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-09.html">
 <span class="dropdown-text">Chapter 9</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-10.html">
 <span class="dropdown-text">Chapter 10</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Part IV: Practice</li>
        <li>
    <a class="dropdown-item" href="./chapter-11.html">
 <span class="dropdown-text">Chapter 11</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./chapter-12.html">
 <span class="dropdown-text">Chapter 12</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="./appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./README.md">
 <span class="dropdown-text">About</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guokai8/mml_learning"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guokai8@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-11.html">Part IV: Practice</a></li><li class="breadcrumb-item"><a href="./chapter-12.html">Chapter 12: Advanced Topics and Future Directions</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📚 Multimodal Learning: Theory, Practice, and Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./how-to-use.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to Use This Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction to Multimodal Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: Foundations and Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 3: Feature Representation for Each Modality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Core Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Feature Alignment and Bridging Modalities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: Fusion Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Attention Mechanisms in Multimodal Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Contrastive Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Part III: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Generative Models for Multimodal Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 10: Seminal Models and Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Part IV: Practice</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 11: Practical Implementation Guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-12.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Chapter 12: Advanced Topics and Future Directions</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comprehensive Appendix and Resources</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-12-advanced-topics-and-future-directions" id="toc-chapter-12-advanced-topics-and-future-directions" class="nav-link active" data-scroll-target="#chapter-12-advanced-topics-and-future-directions"><span class="header-section-number">1</span> Chapter 12: Advanced Topics and Future Directions</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#open-research-problems" id="toc-open-research-problems" class="nav-link" data-scroll-target="#open-research-problems"><span class="header-section-number">1.2</span> 12.1 Open Research Problems</a>
  <ul class="collapse">
  <li><a href="#problem-1-efficient-multimodal-learning" id="toc-problem-1-efficient-multimodal-learning" class="nav-link" data-scroll-target="#problem-1-efficient-multimodal-learning"><span class="header-section-number">1.2.1</span> Problem 1: Efficient Multimodal Learning</a></li>
  <li><a href="#problem-2-long-context-understanding" id="toc-problem-2-long-context-understanding" class="nav-link" data-scroll-target="#problem-2-long-context-understanding"><span class="header-section-number">1.2.2</span> Problem 2: Long-Context Understanding</a></li>
  <li><a href="#problem-3-multimodal-reasoning-and-compositionality" id="toc-problem-3-multimodal-reasoning-and-compositionality" class="nav-link" data-scroll-target="#problem-3-multimodal-reasoning-and-compositionality"><span class="header-section-number">1.2.3</span> Problem 3: Multimodal Reasoning and Compositionality</a></li>
  <li><a href="#problem-4-cross-modal-transfer-and-few-shot-learning" id="toc-problem-4-cross-modal-transfer-and-few-shot-learning" class="nav-link" data-scroll-target="#problem-4-cross-modal-transfer-and-few-shot-learning"><span class="header-section-number">1.2.4</span> Problem 4: Cross-Modal Transfer and Few-Shot Learning</a></li>
  <li><a href="#problem-5-interpretability-and-explainability" id="toc-problem-5-interpretability-and-explainability" class="nav-link" data-scroll-target="#problem-5-interpretability-and-explainability"><span class="header-section-number">1.2.5</span> Problem 5: Interpretability and Explainability</a></li>
  </ul></li>
  <li><a href="#emerging-trends" id="toc-emerging-trends" class="nav-link" data-scroll-target="#emerging-trends"><span class="header-section-number">1.3</span> 12.2 Emerging Trends</a>
  <ul class="collapse">
  <li><a href="#trend-1-foundation-models" id="toc-trend-1-foundation-models" class="nav-link" data-scroll-target="#trend-1-foundation-models"><span class="header-section-number">1.3.1</span> Trend 1: Foundation Models</a></li>
  <li><a href="#trend-2-efficient-and-smaller-models" id="toc-trend-2-efficient-and-smaller-models" class="nav-link" data-scroll-target="#trend-2-efficient-and-smaller-models"><span class="header-section-number">1.3.2</span> Trend 2: Efficient and Smaller Models</a></li>
  <li><a href="#trend-3-retrieval-augmented-generation-rag" id="toc-trend-3-retrieval-augmented-generation-rag" class="nav-link" data-scroll-target="#trend-3-retrieval-augmented-generation-rag"><span class="header-section-number">1.3.3</span> Trend 3: Retrieval-Augmented Generation (RAG)</a></li>
  <li><a href="#trend-4-multimodal-agents" id="toc-trend-4-multimodal-agents" class="nav-link" data-scroll-target="#trend-4-multimodal-agents"><span class="header-section-number">1.3.4</span> Trend 4: Multimodal Agents</a></li>
  <li><a href="#trend-5-video-understanding" id="toc-trend-5-video-understanding" class="nav-link" data-scroll-target="#trend-5-video-understanding"><span class="header-section-number">1.3.5</span> Trend 5: Video Understanding</a></li>
  </ul></li>
  <li><a href="#ethical-considerations" id="toc-ethical-considerations" class="nav-link" data-scroll-target="#ethical-considerations"><span class="header-section-number">1.4</span> 12.3 Ethical Considerations</a>
  <ul class="collapse">
  <li><a href="#challenge-1-bias-and-fairness" id="toc-challenge-1-bias-and-fairness" class="nav-link" data-scroll-target="#challenge-1-bias-and-fairness"><span class="header-section-number">1.4.1</span> Challenge 1: Bias and Fairness</a></li>
  <li><a href="#challenge-2-privacy" id="toc-challenge-2-privacy" class="nav-link" data-scroll-target="#challenge-2-privacy"><span class="header-section-number">1.4.2</span> Challenge 2: Privacy</a></li>
  <li><a href="#challenge-3-environmental-impact" id="toc-challenge-3-environmental-impact" class="nav-link" data-scroll-target="#challenge-3-environmental-impact"><span class="header-section-number">1.4.3</span> Challenge 3: Environmental Impact</a></li>
  <li><a href="#challenge-4-misinformation-and-deepfakes" id="toc-challenge-4-misinformation-and-deepfakes" class="nav-link" data-scroll-target="#challenge-4-misinformation-and-deepfakes"><span class="header-section-number">1.4.4</span> Challenge 4: Misinformation and Deepfakes</a></li>
  </ul></li>
  <li><a href="#learning-path-and-continuous-development" id="toc-learning-path-and-continuous-development" class="nav-link" data-scroll-target="#learning-path-and-continuous-development"><span class="header-section-number">1.5</span> 12.4 Learning Path and Continuous Development</a>
  <ul class="collapse">
  <li><a href="#recommended-learning-sequence" id="toc-recommended-learning-sequence" class="nav-link" data-scroll-target="#recommended-learning-sequence"><span class="header-section-number">1.5.1</span> Recommended Learning Sequence</a></li>
  <li><a href="#resources-for-continuous-learning" id="toc-resources-for-continuous-learning" class="nav-link" data-scroll-target="#resources-for-continuous-learning"><span class="header-section-number">1.5.2</span> Resources for Continuous Learning</a></li>
  </ul></li>
  <li><a href="#contributing-to-the-field" id="toc-contributing-to-the-field" class="nav-link" data-scroll-target="#contributing-to-the-field"><span class="header-section-number">1.6</span> 12.5 Contributing to the Field</a>
  <ul class="collapse">
  <li><a href="#how-to-contribute" id="toc-how-to-contribute" class="nav-link" data-scroll-target="#how-to-contribute"><span class="header-section-number">1.6.1</span> How to Contribute</a></li>
  </ul></li>
  <li><a href="#career-opportunities" id="toc-career-opportunities" class="nav-link" data-scroll-target="#career-opportunities"><span class="header-section-number">1.7</span> 12.6 Career Opportunities</a>
  <ul class="collapse">
  <li><a href="#academic-path" id="toc-academic-path" class="nav-link" data-scroll-target="#academic-path"><span class="header-section-number">1.7.1</span> Academic Path</a></li>
  <li><a href="#industry-path" id="toc-industry-path" class="nav-link" data-scroll-target="#industry-path"><span class="header-section-number">1.7.2</span> Industry Path</a></li>
  <li><a href="#current-job-market" id="toc-current-job-market" class="nav-link" data-scroll-target="#current-job-market"><span class="header-section-number">1.7.3</span> Current Job Market</a></li>
  </ul></li>
  <li><a href="#final-reflections" id="toc-final-reflections" class="nav-link" data-scroll-target="#final-reflections"><span class="header-section-number">1.8</span> Final Reflections</a>
  <ul class="collapse">
  <li><a href="#why-multimodal-learning-matters" id="toc-why-multimodal-learning-matters" class="nav-link" data-scroll-target="#why-multimodal-learning-matters"><span class="header-section-number">1.8.1</span> Why Multimodal Learning Matters</a></li>
  <li><a href="#challenges-ahead" id="toc-challenges-ahead" class="nav-link" data-scroll-target="#challenges-ahead"><span class="header-section-number">1.8.2</span> Challenges Ahead</a></li>
  <li><a href="#call-to-action" id="toc-call-to-action" class="nav-link" data-scroll-target="#call-to-action"><span class="header-section-number">1.8.3</span> Call to Action</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">1.9</span> Key Takeaways</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-11.html">Part IV: Practice</a></li><li class="breadcrumb-item"><a href="./chapter-12.html">Chapter 12: Advanced Topics and Future Directions</a></li></ol></nav></header>





<section id="chapter-12-advanced-topics-and-future-directions" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Chapter 12: Advanced Topics and Future Directions</h1>
<hr>
<p><strong>Previous</strong>: <a href="./chapter-11.html">Chapter 11: Practical Implementation Guide</a> | <strong>Next</strong>: <a href="./appendix.html">Comprehensive Appendix and Resources</a> | <strong>Home</strong>: <a href="./index.html">Table of Contents</a></p>
<hr>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>After reading this chapter, you should be able to: - Understand current research frontiers - Recognize emerging trends in multimodal learning - Address ethical considerations - Plan continuous learning in this field - Contribute to the research community</p>
</section>
<section id="open-research-problems" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="open-research-problems"><span class="header-section-number">1.2</span> 12.1 Open Research Problems</h2>
<section id="problem-1-efficient-multimodal-learning" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="problem-1-efficient-multimodal-learning"><span class="header-section-number">1.2.1</span> Problem 1: Efficient Multimodal Learning</h3>
<p><strong>Challenge:</strong></p>
<pre><code>Current state:
  GPT-4V: Billions of parameters
  CLIP: Hundreds of millions
  Inference: Seconds per image
  Cost: Expensive (API charges)

  Problem: Not accessible to most researchers/companies

Goal:
  Models with &lt;1B parameters
  Inference in &lt;100ms
  Deployable on edge devices
  Open-source and free</code></pre>
<p><strong>Research directions:</strong></p>
<pre><code>1. Neural Architecture Search (NAS)
   Find optimal architectures automatically
   Specialized for each modality combination
   Example: MobileVit for efficient vision

2. Parameter sharing
   Reuse weights across modalities
   Reduce redundancy
   Challenge: Maintaining performance

3. Pruning and compression
   Remove unnecessary connections
   Quantization to lower bits
   Distillation to small models

4. Adapter modules
   Small trainable modules
   Efficient fine-tuning
   Leverage pre-trained models

Current attempts:
  - Efficient CLIP variants
  - Mobile-friendly BLIP-2
  - DistilBERT for text

Benchmark progress needed!</code></pre>
</section>
<section id="problem-2-long-context-understanding" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="problem-2-long-context-understanding"><span class="header-section-number">1.2.2</span> Problem 2: Long-Context Understanding</h3>
<p><strong>Challenge:</strong></p>
<pre><code>Current bottleneck: Quadratic attention complexity

Tasks requiring long context:
  ① Long document understanding (&gt;10K tokens)
  ② Video understanding (1000+ frames)
  ③ Multi-image reasoning (100+ images)
  ④ Temporal reasoning (sequences across time)

Current approaches:
  ✓ Sparse attention: O(n log n) or O(n * window)
  ✓ Linear attention: O(n * d²)
  ✓ Retrieval augmentation: Retrieve then attend
  ✗ Still not perfect

Open questions:
  - Can we get true O(n) complexity?
  - How to handle very long context in practice?
  - Information decay over long sequences?</code></pre>
<p><strong>Research directions:</strong></p>
<pre><code>1. Structured attention
   Hierarchical attention
   Multi-scale representations
   Tree or graph structures

2. Hybrid architectures
   Combine different attention types
   Local + global attention
   Coarse + fine grained

3. Retrieval-augmented generation
   Retrieve relevant context
   Only attend to retrieved
   Reduces effective sequence length

4. Efficient transformers (research area)
   Linformer, Performer, BigBird
   Each with different trade-offs

Current issue:
  Trade-off between:
    Computational efficiency
    Performance quality
    Practical usability

Solving any would be impactful!</code></pre>
</section>
<section id="problem-3-multimodal-reasoning-and-compositionality" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="problem-3-multimodal-reasoning-and-compositionality"><span class="header-section-number">1.2.3</span> Problem 3: Multimodal Reasoning and Compositionality</h3>
<p><strong>Challenge:</strong></p>
<pre><code>Current state:
  Models good at pattern matching
  Models less good at reasoning
  Example:
    ✓ "Red object" - Can find red objects
    ✗ "Count objects that are both red and round" - Harder

Problem:
  Real-world tasks require compositional reasoning
  E.g., visual question answering, scene understanding

Current approaches:
  ✗ End-to-end neural networks struggle
  ✓ Neuro-symbolic approaches more interpretable</code></pre>
<p><strong>Research directions:</strong></p>
<pre><code>1. Neuro-symbolic AI
   Combine neural networks with symbolic reasoning
   Neural for perception, symbolic for logic
   Example: Scene graphs + reasoning rules

2. Disentangled representations
   Separate factors of variation
   Easy to compose and recombine
   Example: Color, shape, size as separate dimensions

3. Program synthesis
   Learn to generate programs that solve tasks
  Example: "red AND round" → specific detection program

4. Modular networks
   Separate modules for different concepts
   Combine modules for complex reasoning
   Example: Module for "color", "shape", etc.

Benchmark improvements needed:
  GQA (Compositional VQA)
  CLEVR (Scene understanding)
  Referential games (Grounding)</code></pre>
</section>
<section id="problem-4-cross-modal-transfer-and-few-shot-learning" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="problem-4-cross-modal-transfer-and-few-shot-learning"><span class="header-section-number">1.2.4</span> Problem 4: Cross-Modal Transfer and Few-Shot Learning</h3>
<p><strong>Challenge:</strong></p>
<pre><code>Current limitation:
  CLIP trained on 400M pairs
  Can't do this for every domain
  Medical, legal, scientific domains need specialized models

Goal:
  Learn with few examples
  Transfer across modalities
  Adapt to new domains quickly

Examples:
  ① Medical imaging + text → Detect tumors with 10 examples
  ② Scientific papers + figures → Understand new concepts
  ③ Low-resource languages → Understand with few examples</code></pre>
<p><strong>Research directions:</strong></p>
<pre><code>1. Few-shot learning techniques
   Meta-learning (learning to learn)
   Prototypical networks
   Matching networks

   Current issue: Requires good representations
                  Which we're trying to learn!

2. Domain adaptation
   Learn from source domain
   Adapt to target domain
   Minimize distribution shift

   Example: ImageNet → Medical images

3. Self-supervised pre-training
   Learn representations without labels
   Then few-shot fine-tune
   Currently best approach

4. Data augmentation in multimodal space
   Generate synthetic pairs
   Mix real and synthetic
   Expand effective dataset size

Benchmark progress:
  miniImageNet
  CIFAR-FS
  BIRDSNAP (few-shot classification)</code></pre>
</section>
<section id="problem-5-interpretability-and-explainability" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="problem-5-interpretability-and-explainability"><span class="header-section-number">1.2.5</span> Problem 5: Interpretability and Explainability</h3>
<p><strong>Challenge:</strong></p>
<pre><code>Models as black boxes:
  What does CLIP learn about images?
  How does GPT-4V make decisions?
  Why does model fail?

Problem:
  High stakes domains (medical, legal)
  Need to understand model reasoning
  Need to debug failures

Current approaches:
  Attention visualization: Shows what model attends to
  Saliency maps: Shows important input regions
  Feature attribution: Shows which features matter

  Limitation: Still not complete understanding</code></pre>
<p><strong>Research directions:</strong></p>
<pre><code>1. Mechanistic interpretability
   Understand internal computations
   How do features emerge?
   What do neurons represent?

   Tools: Activation patching, causal interventions

2. Concept-based explanations
   Instead of pixels, explain in concept space
   "Model uses 'redness' concept"
   More human-understandable

3. Counterfactual explanations
   "What would need to change for different output?"
   Example: "If ball were larger, prediction would change"
   Actionable insights

4. Probing classifiers
   Train auxiliary classifiers on representations
   See what information is encoded
   Reveal hidden structure

Current gaps:
  No unified framework
  Hard to scale to large models
  Trade-off: Accuracy vs Interpretability</code></pre>
</section>
</section>
<section id="emerging-trends" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="emerging-trends"><span class="header-section-number">1.3</span> 12.2 Emerging Trends</h2>
<section id="trend-1-foundation-models" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="trend-1-foundation-models"><span class="header-section-number">1.3.1</span> Trend 1: Foundation Models</h3>
<p><strong>What it is:</strong></p>
<pre><code>Large models trained on massive unlabeled data
Can be adapted to many downstream tasks
Examples: GPT-4, Claude, LLaMA, Flamingo

Characteristics:
  ✓ Trained on diverse, large-scale data
  ✓ Few-shot and zero-shot capable
  ✓ Good at reasoning and understanding
  ✓ Can be fine-tuned efficiently

  ✗ Expensive to train (billions of dollars)
  ✗ Requires massive compute clusters
  ✗ Environmental concerns (energy usage)</code></pre>
<p><strong>Multimodal foundation models:</strong></p>
<pre><code>Recent examples:
  - GPT-4V (OpenAI)
  - Claude 3 (Anthropic)
  - Gemini (Google)
  - Falcon (TII)
  - Flamingo (DeepMind)

Trend: Unified models for multiple modalities
  Not separate image and text models
  Single model handling vision, text, audio, video

Next frontier: Truly general multimodal models</code></pre>
</section>
<section id="trend-2-efficient-and-smaller-models" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="trend-2-efficient-and-smaller-models"><span class="header-section-number">1.3.2</span> Trend 2: Efficient and Smaller Models</h3>
<p><strong>Motivation:</strong></p>
<pre><code>Foundation models are huge
But most applications don't need full power
Trade-offs:
  Accuracy vs efficiency
  Quality vs cost
  Performance vs latency

Movement: "Small is beautiful"
  More efficient methods
  Smaller models matching large model performance
  Accessible to researchers without mega-budgets</code></pre>
<p><strong>Examples:</strong></p>
<pre><code>DistilBERT: 40% smaller, 60% faster, 97% performance
MobileViT: Vision transformer for mobile
TinyLLaMA: 1.1B parameter LLM
Phi-2: 2.7B but outperforms 7B models

Methods:
  1. Knowledge distillation
     Student learns from teacher

  2. Pruning
     Remove unimportant connections

  3. Quantization
     Reduce precision (INT8 instead of FP32)

  4. Architecture search
     Find efficient architectures

Future:
  Small multimodal models for edge devices
  On-device processing without cloud</code></pre>
</section>
<section id="trend-3-retrieval-augmented-generation-rag" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="trend-3-retrieval-augmented-generation-rag"><span class="header-section-number">1.3.3</span> Trend 3: Retrieval-Augmented Generation (RAG)</h3>
<p><strong>Problem it solves:</strong></p>
<pre><code>Current LLMs:
  Knowledge limited to training data
  Can't access new information
  No fact verification

Solution: Augment with retrieval
  When needed, retrieve relevant documents
  Condition generation on retrieved context
  More accurate and factual</code></pre>
<p><strong>Multimodal RAG:</strong></p>
<pre><code>Example: Image-text-document RAG

Query: Image of disease X + question "What treatment?"

Process:
  1. Encode image → query embedding
  2. Retrieve relevant medical papers/images
  3. Retrieve relevant text descriptions
  4. Combine: Image + papers + text → context
  5. Generate: Use context for answer generation

Benefits:
  ✓ Grounds answers in specific documents
  ✓ Can cite sources
  ✓ More recent information
  ✓ Reduced hallucination

Challenges:
  Efficient retrieval with billions of documents
  Combining multiple modality retrievals
  Ranking and selecting best documents</code></pre>
</section>
<section id="trend-4-multimodal-agents" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="trend-4-multimodal-agents"><span class="header-section-number">1.3.4</span> Trend 4: Multimodal Agents</h3>
<p><strong>What it is:</strong></p>
<pre><code>AI agents that can:
  ① See (vision)
  ② Understand (language)
  ③ Plan (reasoning)
  ④ Act (take actions)
  ⑤ Reflect (learn from mistakes)

Examples:
  - Robots that see and understand instructions
  - Agents that read documents and take actions
  - Systems that analyze images and generate reports

Building blocks:
  LLM: Central reasoning engine
  Vision: Image understanding
  Language: Text understanding
  Tools: Can call external functions
  Memory: Persistent state</code></pre>
<p><strong>Example architecture:</strong></p>
<pre><code>User: "Find images of cats in this folder, resize to 256x256,
       upload to cloud storage"

Agent processes:
  1. Plan
     Break down into steps
     "List files → filter images → identify cats →
      check if cat → resize → upload"

  2. Execute
     Step 1: List files
            → ["img1.jpg", "img2.txt", "img3.png"]

     Step 2: Filter image types
            → ["img1.jpg", "img3.png"]

     Step 3: Vision model checks if cat
            → img1.jpg: "yes", img3.png: "no"

     Step 4: Resize
            → img1.jpg → 256×256 version

     Step 5: Upload
            → Upload to storage

     Result: "Done! Resized and uploaded 1 cat image"

  3. Reflect
     Did it work? Any errors? Learn for next time</code></pre>
</section>
<section id="trend-5-video-understanding" class="level3" data-number="1.3.5">
<h3 data-number="1.3.5" class="anchored" data-anchor-id="trend-5-video-understanding"><span class="header-section-number">1.3.5</span> Trend 5: Video Understanding</h3>
<p><strong>Challenge:</strong></p>
<pre><code>Video = Images over time
But not just applying image model frame-by-frame
Temporal relationships matter

Current state:
  Good: Action recognition (what's happening?)
  Poor: Temporal reasoning (cause-effect, predictions)

Goal:
  Understand complex temporal patterns
  Reason about future
  Explain temporal relationships</code></pre>
<p><strong>Research directions:</strong></p>
<pre><code>1. Temporal action localization
   When does action start/end?
   Multiple actions in video?

2. Temporal reasoning
   "Before A happened, B was occurring"
   Cause-effect relationships

3. Video captioning
   Describe entire video (not just frames)
   Capture dynamics, not just static content

4. Future prediction
   Given past frames, predict future
   What will happen next?
   What if X occurs?

Benchmarks:
  ActivityNet
  Kinetics
  HACS

Current models:
  SlowFast (two-stream)
  TimeSformer (pure transformer)
  ViViT (video vision transformer)</code></pre>
</section>
</section>
<section id="ethical-considerations" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="ethical-considerations"><span class="header-section-number">1.4</span> 12.3 Ethical Considerations</h2>
<section id="challenge-1-bias-and-fairness" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="challenge-1-bias-and-fairness"><span class="header-section-number">1.4.1</span> Challenge 1: Bias and Fairness</h3>
<p><strong>The problem:</strong></p>
<pre><code>ML systems trained on real-world data
Real-world data contains human biases
Result: Biased AI systems

Examples:
  ① Image recognition better on light skin tones
  ② Hiring systems biased against minorities
  ③ Medical systems not generalizing across populations
  ④ Language models reflecting stereotypes

Impact:
  Discrimination against groups
  Reinforces societal inequalities
  Legal/regulatory consequences</code></pre>
<p><strong>Addressing bias:</strong></p>
<pre><code>Technical solutions:

1. Dataset curation
   Balanced representation of groups
   Avoid stereotypical associations
   Diverse data collection

2. Augmentation
   Deliberately generate diverse examples
   Color jittering for different skin tones
   Language paraphrasing for dialects

3. Debiasing techniques
   Remove correlation with sensitive attributes
   Adversarial training
   Fairness constraints

4. Evaluation
   Measure performance across groups
   Don't just optimize average
   Check for disparate impact

Metric example:
  Accuracy across demographics:
    Group A: 95%
    Group B: 70%  ← Unfair!

  Should minimize: max(group_A_error - group_B_error)

Limitations:
  Technical fixes can't solve social problems
  Need responsible deployment practices
  Policy and regulation important</code></pre>
</section>
<section id="challenge-2-privacy" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="challenge-2-privacy"><span class="header-section-number">1.4.2</span> Challenge 2: Privacy</h3>
<p><strong>The problem:</strong></p>
<pre><code>Training data often contains sensitive information
Example: Medical images with patient identifiers

Risks:
  ① Privacy breach if data stolen
  ② Model memorization of sensitive details
  ③ Model inversion attacks (recover training data)
  ④ Identification of individuals in training set</code></pre>
<p><strong>Technical solutions:</strong></p>
<pre><code>1. Differential privacy
   Add noise to data/gradients
   Mathematically guarantees privacy
   Trade-off: Model performance vs privacy

   Implementation:
   - DP-SGD: Noisy stochastic gradient descent
   - Privacy budget: How much privacy vs utility

2. Federated learning
   Train on distributed devices
   Never centralize raw data
   Only share model updates

   Process:
   Device 1: Train on local data → send gradients
   Device 2: Train on local data → send gradients
   Server: Average gradients → new model

   Device never sends raw data

3. Data anonymization
   Remove identifiers
   Aggregate sensitive attributes
   Difficulty: Re-identification attacks

4. Encryption
   Homomorphic encryption: Compute on encrypted data
   Secure multi-party computation

   Limitation: Computationally expensive</code></pre>
</section>
<section id="challenge-3-environmental-impact" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="challenge-3-environmental-impact"><span class="header-section-number">1.4.3</span> Challenge 3: Environmental Impact</h3>
<p><strong>The problem:</strong></p>
<pre><code>Large model training is energy-intensive

Example: Training GPT-3
  Estimated energy: 1,287 MWh
  Carbon: ~552 metric tons CO₂
  Cost: ~$4.6 million

Inference at scale:
  Millions of queries daily
  Cumulative energy significant

Environmental concerns:
  ① Climate change impact
  ② Energy grid strain
  ③ Resource waste</code></pre>
<p><strong>Solutions:</strong></p>
<pre><code>1. Efficient architectures
   Smaller models need less energy
   Methods covered earlier: Distillation, quantization, pruning

2. Efficient training
   Mixed precision (FP32 → FP16 or lower)
   Gradient checkpointing
   Better optimization algorithms

3. Green computing
   Use renewable energy data centers
   Optimize cooling
   Hardware efficiency

4. Compute awareness
   Only train when necessary
   Reuse models instead of retraining
   Share pre-trained models

Example carbon calc:
  Original model: 550 tons CO₂
  Distilled model: 20 tons CO₂ to train
  Deployed 1 billion times

  Amortized: 0.00000002 tons per inference
  Responsible AI requires thinking at scale</code></pre>
</section>
<section id="challenge-4-misinformation-and-deepfakes" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" class="anchored" data-anchor-id="challenge-4-misinformation-and-deepfakes"><span class="header-section-number">1.4.4</span> Challenge 4: Misinformation and Deepfakes</h3>
<p><strong>The problem:</strong></p>
<pre><code>Generative models can create:
  ① Deepfake videos
  ② Synthetic but realistic images
  ③ False information at scale
  ④ Manipulated media

Examples:
  Deepfake politician videos
  Fake evidence in legal cases
  Stock market manipulation through false news
  Celebrity impersonation

Challenges:
  Detection hard (adversarial arms race)
  Detection itself can become tool for abuse
  Rapid spread before fact-checking</code></pre>
<p><strong>Addressing misinformation:</strong></p>
<pre><code>Technical approaches:

1. Detection of fakes
   Artifacts in generated content
   Statistical inconsistencies
   Provenance tracking

   Limitation: Arms race with generation

2. Watermarking
   Embed invisible markers in generated content
   Prove content origin

   Challenge: Removing watermarks

3. Authenticity verification
   Cryptographic signatures
   Blockchain tracking
   Chain of custody

4. Responsible release
   Don't release tools enabling deception
   API restrictions
   Monitoring for abuse

Non-technical approaches:
  Media literacy
  Fact-checking infrastructure
  Transparent AI companies
  Regulation and oversight</code></pre>
</section>
</section>
<section id="learning-path-and-continuous-development" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="learning-path-and-continuous-development"><span class="header-section-number">1.5</span> 12.4 Learning Path and Continuous Development</h2>
<section id="recommended-learning-sequence" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="recommended-learning-sequence"><span class="header-section-number">1.5.1</span> Recommended Learning Sequence</h3>
<p><strong>Phase 1: Mastery (Chapters 1-10)</strong></p>
<pre><code>Time: 8-12 weeks
Approach: Deep study + coding exercises

Week 1-2: Fundamentals (Chapters 1-3)
  Understand multimodality
  Learn feature representations
  Build intuition with code

Week 3-4: Techniques (Chapters 4-6)
  Alignment and fusion
  Attention mechanisms
  Implement from scratch

Week 5-6: Modern methods (Chapters 7-8)
  Contrastive learning
  Transformers
  Understand research papers

Week 7-8: Applications (Chapters 9-10)
  Generative models
  Seminal architectures
  Reproduce results

Outcome: Solid foundation in multimodal learning</code></pre>
<p><strong>Phase 2: Specialization (Choose 1-2 areas)</strong></p>
<pre><code>Option A: Efficient Models
  Study: MobileViT, DistilBERT, model compression
  Project: Build efficient image-text model
  Timeline: 4-6 weeks

Option B: Vision-Language Models
  Study: CLIP, BLIP-2, ALIGN, LiT
  Project: Fine-tune for specific domain
  Timeline: 4-6 weeks

Option C: Generative Models
  Study: Diffusion models, GANs, VAEs
  Project: Text-to-image system
  Timeline: 6-8 weeks

Option D: Video Understanding
  Study: Temporal modeling, 3D CNNs, video transformers
  Project: Video classification or captioning
  Timeline: 6-8 weeks

Option E: Reasoning and Compositionality
  Study: Scene graphs, neuro-symbolic AI, modular networks
  Project: VQA system with reasoning
  Timeline: 6-8 weeks</code></pre>
<p><strong>Phase 3: Research/Industry Application</strong></p>
<pre><code>Research Track:
  Identify open problem from Chapter 12
  Literature review
  Propose novel solution
  Implement and evaluate
  Write paper
  Submit to conference
  Timeline: 3-6 months

Industry Track:
  Choose real-world problem
  Collect domain-specific data
  Build production system
  Evaluate and iterate
  Deploy with monitoring
  Timeline: 2-4 months</code></pre>
</section>
<section id="resources-for-continuous-learning" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="resources-for-continuous-learning"><span class="header-section-number">1.5.2</span> Resources for Continuous Learning</h3>
<p><strong>Research papers:</strong></p>
<pre><code>Top venues for multimodal learning:
  ① CVPR (Computer Vision and Pattern Recognition)
  ② ICCV (International Conference on Computer Vision)
  ③ ECCV (European Conference on Computer Vision)
  ④ NeurIPS (Neural Information Processing Systems)
  ⑤ ICML (International Conference on Machine Learning)
  ⑥ ICLR (International Conference on Learning Representations)
  ⑦ ACL (Association for Computational Linguistics)
  ⑧ EMNLP (Empirical Methods in NLP)
  ⑨ NAACL (North American Chapter of ACL)

How to follow:
  Subscribe to arXiv newsletters
  Follow #MachineLearning on Twitter
  Join Discord/Reddit communities
  Attend conferences/seminars

Must-read papers (by year):
  2021: CLIP (Radford et al.)
  2022: Flamingo (Alayrac et al.)
  2023: LLaVA (Liu et al.), GPT-4V
  2024: Latest in multimodal agents</code></pre>
<p><strong>Online resources:</strong></p>
<pre><code>Free courses:
  - Andrew Ng's ML specialization (Coursera)
  - Stanford CS231N (Computer Vision)
  - Stanford CS224N (NLP)
  - DeepLearning.AI short courses

Books:
  - "Deep Learning" by Goodfellow, Bengio, Courville
  - "Attention is All You Need" (paper, but well-written)
  - "Neural Networks from Scratch" by Trask

Code repositories:
  - Hugging Face (pre-trained models)
  - PyTorch examples
  - GitHub research implementations
  - Papers with Code

Communities:
  - r/MachineLearning (Reddit)
  - ML Discord servers
  - Local AI meetups
  - Conference workshops</code></pre>
</section>
</section>
<section id="contributing-to-the-field" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="contributing-to-the-field"><span class="header-section-number">1.6</span> 12.5 Contributing to the Field</h2>
<section id="how-to-contribute" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="how-to-contribute"><span class="header-section-number">1.6.1</span> How to Contribute</h3>
<p><strong>Option 1: Open-Source Contributions</strong></p>
<pre><code>Getting started:
  1. Find project on GitHub
  2. Check issues/feature requests
  3. Fork repository
  4. Create branch
  5. Make improvements
  6. Write tests
  7. Submit pull request
  8. Iterate on feedback

Good first contributions:
  - Bug fixes
  - Documentation
  - Performance improvements
  - New features

Projects needing help:
  - Hugging Face Transformers
  - PyTorch
  - Stable Diffusion
  - LLaVA
  - Many others!

Benefits:
  - Build reputation
  - Learn from experts
  - Help community
  - Get experience</code></pre>
<p><strong>Option 2: Research and Publishing</strong></p>
<pre><code>Steps to publish:

1. Identify problem
   Survey existing work
   Find gap or improvement

2. Propose solution
   Design approach
   Theoretical justification

3. Implement
   Write code
   Ensure reproducibility

4. Evaluate
   Benchmarks
   Comparisons
   Ablations

5. Write paper
   Clear writing
   Good figures/tables
   Reproducible details

6. Submit
   Choose conference/journal
   Follow submission guidelines

7. Iterate
   Respond to reviewers
   Refine paper

Timeline: 6-12 months per paper</code></pre>
<p><strong>Option 3: Dataset Creation</strong></p>
<pre><code>Create multimodal datasets:

Important datasets lacking:
  - Domain-specific (medical, legal, scientific)
  - Low-resource languages
  - Underrepresented groups
  - New modalities

Steps:
  1. Define task/domain
  2. Data collection strategy
  3. Annotation guidelines
  4. Quality control
  5. Release methodology

Considerations:
  - Privacy and consent
  - Licensing
  - Documentation
  - Accessibility

Venues for dataset papers:
  - Dataset track at major conferences
  - Journals specializing in datasets
  - Hugging Face datasets hub</code></pre>
<p><strong>Option 4: Building Applications</strong></p>
<pre><code>Create practical systems:

Ideas:
  - Medical imaging analysis
  - Educational tools
  - Accessibility applications
  - Content creation tools
  - Research tools
  - Developer tools

Impact:
  - Solve real problems
  - Help people
  - Get user feedback
  - Test techniques in practice

Path:
  1. Prototype
  2. Beta testing
  3. Gather feedback
  4. Iterate
  5. Release
  6. Support users</code></pre>
</section>
</section>
<section id="career-opportunities" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="career-opportunities"><span class="header-section-number">1.7</span> 12.6 Career Opportunities</h2>
<section id="academic-path" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1" class="anchored" data-anchor-id="academic-path"><span class="header-section-number">1.7.1</span> Academic Path</h3>
<pre><code>PhD research:
  ① Apply to graduate programs
  ② Find advisor in multimodal learning
  ③ Propose research project
  ④ 4-6 years of research
  ⑤ Publish papers
  ⑥ Defend dissertation

Postdoc:
  Continue research
  Build reputation
  Collaborate widely

Faculty:
  Run research group
  Teach courses
  Mentor students
  Long-term career

Skills needed:
  - Research design
  - Writing
  - Communication
  - Mentoring
  - Persistence</code></pre>
</section>
<section id="industry-path" class="level3" data-number="1.7.2">
<h3 data-number="1.7.2" class="anchored" data-anchor-id="industry-path"><span class="header-section-number">1.7.2</span> Industry Path</h3>
<pre><code>ML Engineer roles:
  - Build systems using multimodal models
  - Optimize for production
  - Maintain and improve systems
  - 3-5 years experience typical

Research Scientist:
  - Conduct research while employed
  - Publish papers
  - Balance research and product
  - Typically PhD required

ML Product Manager:
  - Define product requirements
  - Prioritize features
  - Work with engineers and researchers
  - Less technical but strategic

Entrepreneur:
  - Start company based on technology
  - Commercialize models/tools
  - Build business
  - High risk/reward

Typical progression:
  Junior ML Engineer → Senior ML Engineer → Manager/Lead
  Research Scientist → Principal Scientist
  Both paths lead to Director/VP roles</code></pre>
</section>
<section id="current-job-market" class="level3" data-number="1.7.3">
<h3 data-number="1.7.3" class="anchored" data-anchor-id="current-job-market"><span class="header-section-number">1.7.3</span> Current Job Market</h3>
<p><strong>Demand:</strong></p>
<pre><code>High demand for:
  ① Multimodal ML engineers
  ② Vision-language model experts
  ③ LLM fine-tuning specialists
  ④ Efficient model developers
  ⑤ GenAI product managers

Growing rapidly:
  Generative AI jobs grew 74% in 2023
  Competition increasing

Salaries (US, 2024):
  Junior ML Engineer: $120-180K
  Senior ML Engineer: $200-300K
  Research Scientist: $180-250K
  Manager/Lead: $250-350K+

Location: SF, NYC, Seattle, Boston pay highest</code></pre>
<p><strong>Future outlook:</strong></p>
<pre><code>Next 5 years:
  ① More specialized models (domain-specific)
  ② Smaller, more efficient models
  ③ Multimodal agents increasingly common
  ④ Video understanding breakthroughs
  ⑤ Reasoning capabilities improve

Implications:
  More jobs in AI/ML
  Higher specialization needed
  Continuous learning required
  Ethical AI becoming critical skill

Recommendation:
  Develop T-shaped skills
  Deep expertise in 1-2 areas
  Broad knowledge of field
  Stay current with research</code></pre>
</section>
</section>
<section id="final-reflections" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="final-reflections"><span class="header-section-number">1.8</span> Final Reflections</h2>
<section id="why-multimodal-learning-matters" class="level3" data-number="1.8.1">
<h3 data-number="1.8.1" class="anchored" data-anchor-id="why-multimodal-learning-matters"><span class="header-section-number">1.8.1</span> Why Multimodal Learning Matters</h3>
<pre><code>Multimodal learning is how humans learn:
  We see images
  We hear sounds
  We read text
  We feel textures
  We taste foods

All integrated into understanding

Current AI:
  Processing single modalities
  Missing the integration

Future AI:
  Multimodal understanding
  Integration across senses
  More human-like reasoning

Impact:
  Better AI systems
  More accessible technology
  Understanding between humans and machines
  Potential for AGI</code></pre>
</section>
<section id="challenges-ahead" class="level3" data-number="1.8.2">
<h3 data-number="1.8.2" class="anchored" data-anchor-id="challenges-ahead"><span class="header-section-number">1.8.2</span> Challenges Ahead</h3>
<pre><code>Technical:
  ① Efficiency
  ② Reasoning
  ③ Long-context
  ④ Robustness
  ⑤ Interpretability

Ethical:
  ① Bias and fairness
  ② Privacy protection
  ③ Environmental impact
  ④ Misinformation prevention
  ⑤ Responsible deployment

Social:
  ① Education and literacy
  ② Regulatory frameworks
  ③ Equitable access
  ④ Job displacement concerns

Solving these requires:
  Technical expertise
  Ethical reasoning
  Cross-disciplinary collaboration
  Long-term vision</code></pre>
</section>
<section id="call-to-action" class="level3" data-number="1.8.3">
<h3 data-number="1.8.3" class="anchored" data-anchor-id="call-to-action"><span class="header-section-number">1.8.3</span> Call to Action</h3>
<pre><code>You now have foundation to:
  ① Understand multimodal learning deeply
  ② Build practical systems
  ③ Contribute to research
  ④ Help address challenges
  ⑤ Advance the field

What to do next:
  1. Choose specialization
  2. Work on concrete project
  3. Build portfolio
  4. Connect with community
  5. Keep learning

The field needs:
  Researchers pushing boundaries
  Engineers building systems
  Ethicists ensuring responsibility
  Educators sharing knowledge
  Practitioners solving problems

Your contribution matters!</code></pre>
</section>
</section>
<section id="key-takeaways" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">1.9</span> Key Takeaways</h2>
<ul>
<li><strong>Research frontiers</strong> offer exciting opportunities (efficiency, reasoning, etc.)</li>
<li><strong>Emerging trends</strong> show direction (foundation models, RAG, agents)</li>
<li><strong>Ethical considerations</strong> are as important as technical performance</li>
<li><strong>Continuous learning</strong> essential in rapidly evolving field</li>
<li><strong>Multiple paths</strong> available (research, industry, entrepreneurship)</li>
<li><strong>Community engagement</strong> accelerates growth</li>
</ul>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/guokai8\.github\.io\/mml_learning\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Kai Guo - Multimodal Learning Guide</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>