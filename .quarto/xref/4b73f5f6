{"entries":[],"headings":["chapter-8-transformer-architecture","learning-objectives","the-problem-transformers-solve","limitations-of-sequential-models-rnns","cnn-limitations-for-sequences","transformer-solution","self-attention-mechanism","intuition","mathematical-definition","numerical-example","multi-head-attention","scaled-dot-product-attention-revisited","transformer-encoder","architecture-overview","detailed-layer-breakdown","full-transformer-encoder","transformer-decoder","causal-masking","autoregressive-generation","cross-attention-in-decoder","putting-it-together-vision-transformer-vit","architecture-for-images","why-vit-works","key-takeaways","exercises"]}