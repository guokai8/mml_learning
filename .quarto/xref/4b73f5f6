{"entries":[],"headings":["chapter-8-transformer-architecture","learning-objectives","motivation-why-transformers","sequential-processing-limitations","gradient-flow-analysis-chain-rule","cnn-limitations-for-sequences","the-transformer-solution","attention-mechanism-deep-dive","scaled-dot-product-attention","concrete-example","why-scale-by-d_k","implementation","gradient-flow-in-attention","multi-head-attention","why-multiple-heads","mathematical-formulation","implementation-1","positional-encoding","the-position-problem","sinusoidal-positional-encoding","implementation-2","complete-transformer-architecture","transformer-encoder","implementation-3","transformer-for-multimodal-applications","cross-modal-attention","multimodal-transformer-architecture","key-advantages-and-limitations","advantages","limitations","when-to-use-transformers","key-takeaways","further-reading"]}