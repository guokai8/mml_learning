{"title":"Chapter 7: Contrastive Learning","markdown":{"headingText":"Chapter 7: Contrastive Learning","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 6: Attention Mechanisms in Multimodal Systems](chapter-06.md) | **Next**: [Chapter 8: Transformer Architecture](chapter-08.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand contrastive learning principles and motivation\n- Implement InfoNCE loss\n- Understand CLIP's revolutionary approach\n- Compare different contrastive methods\n- Apply contrastive learning to your own problems\n\n## 7.1 The Problem Contrastive Learning Solves\n\n### Traditional Supervised Learning\n\n**Standard approach:**\n\n```\nTraining data: (input, label) pairs\n\nTask: Image classification\n  Input: Image\n  Label: \"cat\" or \"dog\"\n\n  Process:\n  ① Pass image through network\n  ② Output logits for each class\n  ③ Cross-entropy loss compares to label\n  ④ Backprop updates weights\n\nRequirements:\n  ✗ Requires labels for everything\n  ✗ Labels are expensive (human annotation)\n  ✗ Limited to labeled dataset size\n  ✗ New task = new labeled data needed\n```\n\n**Bottleneck in practice:**\n\n```\nProblem: Most data is unlabeled\n\nExample:\n  ImageNet: 1.4M labeled images\n  Internet: Billions of images daily\n\n  Ratio: ~1 labeled per 1 million unlabeled!\n\nQuestion: How to leverage the vast unlabeled data?\n\nTraditional supervised learning: Can't use it!\nSolution: Contrastive learning\n```\n\n### Self-Supervised Learning Intuition\n\n**Key insight:**\n```\nDon't need explicit labels!\nCreate labels from data itself using natural structure\n```\n\n**Example - Image rotation prediction:**\n\n```\nUnlabeled image:\n  [Photo of cat]\n\nCreate self-supervised task:\n  Rotate image 90°\n\n  Rotated image → Network → Predict rotation\n\nLabel is free! (We created it by rotation)\n\nTraining:\n  ① Rotate image by random angle (0°, 90°, 180°, 270°)\n  ② Network predicts angle\n  ③ Loss: Cross-entropy between predicted and actual angle\n\nResult:\n  Network learns visual representations\n  Without any human labels!\n\nBenefit:\n  Can train on billions of unlabeled images\n  Representations useful for downstream tasks\n  Transfer to real tasks with small labeled dataset\n```\n\n**Why this works:**\n\n```\nTo predict rotation, network must understand:\n  - What's the \"up\" direction? (spatial orientation)\n  - What are objects and their structure? (semantics)\n  - What's foreground vs background? (attention)\n\nThese are useful representations for other tasks!\n```\n\n### Contrastive Learning Idea\n\n**Core concept:**\n\n```\nSupervised learning: \"Is this input A or B or C?\"\nContrastive learning: \"Which B is similar to A?\"\n\nExample:\n  Supervised:      \"Is this a dog?\" (Yes/No)\n  Contrastive:     \"Given this dog photo, which text matches best?\n                    A) 'A dog running'\n                    B) 'A cat sleeping'\n                    C) 'A car parked'\"\n\nContrastive doesn't need explicit labels\nJust needs relative similarities!\n```\n\n**Why it's powerful:**\n\n```\nAdvantage 1: No labels needed\n  ✓ Use unlabeled data directly\n  ✓ Billions of image-text pairs from web\n  ✓ Much cheaper than labeling\n\nAdvantage 2: Richer signal\n  Binary classification: Yes/No (1 bit)\n  Contrastive: Ranking among many (log₂(N) bits)\n\n  With N=1000 options:\n  Ranking gives ~10 bits of information\n  vs 1 bit for binary\n\nAdvantage 3: Metric learning\n  Directly optimize for similarity\n  Better representations for retrieval\n  Natural distance metrics\n```\n\n## 7.2 InfoNCE Loss - The Foundation\n\n### Understanding the Loss\n\n**Name breakdown:**\n- **Info** = Information theory\n- **NCE** = Noise Contrastive Estimation\n\n**Goal:**\n```\nMake positive pairs similar\nMake negative pairs dissimilar\n\nPositive pair: (cat image, \"cat\" text)\nNegative pair: (cat image, \"dog\" text)\n```\n\n### Mathematical Formulation\n\n**Formula:**\n\n```\nL = -log [ exp(sim(q,k+)/τ) / (exp(sim(q,k+)/τ) + Σⱼ exp(sim(q,k⁻ⱼ)/τ)) ]\n\nBreakdown:\n\nq = query (e.g., image)\nk+ = positive key (e.g., matching text)\nk⁻ⱼ = negative keys (non-matching texts)\nτ = temperature (controls sharpness)\nsim = similarity function (cosine, dot product)\n```\n\n**Step-by-step explanation:**\n\n```\nStep 1: Compute similarities\n  sim(query, positive) = dot product\n  sim(query, negative₁) = dot product\n  sim(query, negative₂) = dot product\n  ...\n\n  Result: Scores (could be any value)\n\nStep 2: Scale by temperature\n  Score / τ\n\n  Temperature effect:\n    τ small (0.01): Scores become extreme\n    τ normal (0.1): Moderate scaling\n    τ large (1.0): Minimal scaling\n\n  Why temperature?\n    Prevents softmax from being too sharp\n    Allows gradient flow during training\n\nStep 3: Exponential\n  exp(score / τ)\n\n  Result: All positive (e^x > 0 for all x)\n\n  Effect:\n    Larger scores → larger exponents\n    Softmax then emphasizes them\n\nStep 4: Softmax (normalize)\n  exp(positive) / (exp(positive) + Σ exp(negatives))\n\n  Result: Probability in [0, 1]\n\n  Interpretation:\n    Probability that positive is highest ranked\n    Perfect: Probability = 1.0\n    Random: Probability = 1/(1+num_negatives)\n\nStep 5: Negative log\n  -log(probability)\n\n  If probability = 1.0: loss = 0 (perfect!)\n  If probability = 0.1: loss = -log(0.1) = 2.3 (bad)\n  If probability = 0.5: loss = -log(0.5) = 0.69 (medium)\n```\n\n### Numerical Example\n\n**Setup:**\n\n```\nQuery: Image of red cat\nPositive: Text \"a red cat\"\nNegatives:\n  - \"a blue dog\"\n  - \"a green parrot\"\n  - \"a car\"\n\nSimilarities (before temperature):\n  sim(query, positive) = 0.8    (high, should be!)\n  sim(query, neg1) = 0.2        (low, good)\n  sim(query, neg2) = 0.15       (low, good)\n  sim(query, neg3) = 0.1        (low, good)\n\nTemperature τ = 0.1\n```\n\n**Computing loss:**\n\n```\nStep 1: Scale by temperature\n  0.8 / 0.1 = 8.0\n  0.2 / 0.1 = 2.0\n  0.15 / 0.1 = 1.5\n  0.1 / 0.1 = 1.0\n\nStep 2: Exponentials\n  e^8.0 ≈ 2981\n  e^2.0 ≈ 7.4\n  e^1.5 ≈ 4.5\n  e^1.0 ≈ 2.7\n\nStep 3: Softmax (probability)\n  2981 / (2981 + 7.4 + 4.5 + 2.7)\n  = 2981 / 2995.6\n  ≈ 0.995   (99.5% probability positive is best!)\n\nStep 4: Loss\n  loss = -log(0.995) ≈ 0.005   (very small! Model doing great)\n```\n\n**What if model was bad:**\n\n```\nSimilarities:\n  sim(query, positive) = 0.1    (low! bad!)\n  sim(query, neg1) = 0.5        (high! worse)\n  sim(query, neg2) = 0.4\n  sim(query, neg3) = 0.3\n\nAfter temperature scaling (τ = 0.1):\n  0.1 / 0.1 = 1.0     → e^1.0 ≈ 2.7\n  0.5 / 0.1 = 5.0     → e^5.0 ≈ 148\n  0.4 / 0.1 = 4.0     → e^4.0 ≈ 55\n  0.3 / 0.1 = 3.0     → e^3.0 ≈ 20\n\nSoftmax:\n  2.7 / (2.7 + 148 + 55 + 20)\n  = 2.7 / 225.7\n  ≈ 0.012   (1.2% probability - terrible!)\n\nLoss:\n  -log(0.012) ≈ 4.4   (very large! Forces update)\n```\n\n### Why This Works\n\n**Mathematical properties:**\n\n```\n1. Bounded between 0 and log(1+N)\n   where N = number of negatives\n\n   N=10: Loss ∈ [0, log(11) ≈ 2.4]\n   N=100: Loss ∈ [0, log(101) ≈ 4.6]\n\n   Interpretable scale\n\n2. Gradient is informative\n\n   Perfect case (prob ≈ 1): gradient ≈ 0\n   Good case (prob ≈ 0.9): gradient ≈ small\n   Bad case (prob ≈ 0.1): gradient ≈ large\n\n   Automatically focuses on hard cases\n\n3. Invariant to scale\n\n   If all similarities multiplied by constant K:\n   exp(K*sim) has same relative ordering\n   Softmax still works correctly\n\n   Enables using unnormalized similarities\n```\n\n### Temperature Parameter\n\n**Role of τ:**\n\n```\nTemperature controls softmax sharpness\n\nτ = 0.01 (very cold):\n  Softmax becomes nearly one-hot\n  exp(5) = 148\n  exp(4) = 55\n  exp(3) = 20\n  Ratio: 148/55 = 2.7x difference\n\n  Large differences between outputs\n  Large gradients\n  Potential instability\n\nτ = 0.1 (standard):\n  Moderate softmax\n  exp(0.5) = 1.65\n  exp(0.4) = 1.49\n  exp(0.3) = 1.35\n  Ratio: 1.65/1.49 = 1.1x difference\n\n  Balanced gradients\n  Stable training\n  Common choice\n\nτ = 1.0 (very hot):\n  Softmax becomes smooth\n  exp(0.05) = 1.05\n  exp(0.04) = 1.04\n  exp(0.03) = 1.03\n  Ratio: 1.05/1.04 ≈ 1.01x difference\n\n  Small differences between outputs\n  Small gradients\n  Slow learning\n\nτ = 10.0 (extremely hot):\n  Softmax nearly uniform\n  All classes almost equally likely\n  Nearly no signal\n  Training doesn't work\n```\n\n**Effect on learning:**\n\n```\nOptimal temperature depends on:\n  - Number of negatives\n  - Difficulty of task\n  - Data quality\n\nTypical range: τ ∈ [0.05, 0.2]\n\nCLIP uses: τ ≈ 0.07 (learned during training)\n```\n\n## 7.3 CLIP - Contrastive Learning Success Story\n\n### Context and Impact\n\n**Problem statement (2020):**\n\n```\nExisting vision models:\n  - Trained on ImageNet (1.4M images)\n  - Limited to 1000 classes\n  - Can't generalize to new concepts\n  - Require supervised fine-tuning\n\nQuestion:\n  Can we use web data (unsupervised) for vision?\n  Can we match NLP's success with massive unlabeled data?\n```\n\n**CLIP solution:**\n\n```\nData: 400M image-caption pairs from web\nTask: Learn from natural language supervision\nMethod: Contrastive learning on image-text pairs\n\nResult: Revolutionary zero-shot transfer\n```\n\n### CLIP Architecture\n\n**Components:**\n\n```\nImage encoder:           Text encoder:\n  Vision Transformer      Transformer (BERT-like)\n  Input: 224×224 image    Input: Text tokens\n  Output: 512D vector     Output: 512D vector\n\n            ↓                     ↓\n\n    [Normalize to unit length]\n\n            ↓                     ↓\n\n    Similarity computation (dot product of normalized)\n\n            ↓\n\n    Contrastive loss\n```\n\n**Data collection:**\n\n```\n400 million image-caption pairs from internet\n\nSources:\n  - Web pages with images and captions\n  - Publicly available image databases\n  - Social media posts with text\n  - Stock photo sites with descriptions\n\nQuality:\n  - Uncurated and diverse\n  - Contains noise and biases\n  - Reflects web distribution\n  - Natural language (not formal labels)\n```\n\n### Training Process\n\n**Batch construction:**\n\n```\nBatch size: 32,768 (massive!)\n\nImages: [img_1, img_2, ..., img_32k]\nCaptions: [caption_1, caption_2, ..., caption_32k]\n\nEncode all:\n  Image embeddings: 32k × 512\n  Caption embeddings: 32k × 512\n\nCompute similarity matrix (32k × 32k):\n  sim[i,j] = image_i · caption_j\n\nGoal:\n  Diagonal elements high (matched pairs)\n  Off-diagonal elements low (mismatched pairs)\n```\n\n**Loss computation:**\n\n```\nFor each image:\n  Compute InfoNCE loss\n  Positive: matching caption\n  Negatives: all other 32k-1 captions\n\nFor each caption:\n  Compute InfoNCE loss\n  Positive: matching image\n  Negatives: all other 32k-1 images\n\nTotal loss = average of all losses\n\nOptimization:\n  Adam optimizer\n  Learning rate: 5×10⁻⁴\n  Training: ~2 weeks on large clusters\n```\n\n### Zero-Shot Transfer - Revolutionary Capability\n\n**Traditional approach:**\n\n```\nNew task: Classify images of birds (not in ImageNet)\n\nSteps:\n  1. Get labeled training data for birds\n  2. Fine-tune ImageNet model\n  3. Get predictions\n\nProblem: Need labeled bird data!\nCost: Expensive annotation\n```\n\n**CLIP zero-shot approach:**\n\n```\nNew task: Classify images of birds\n\nNo training needed!\n\nSteps:\n  1. Text prompts: \"a photo of a bird\"\n                   \"a photo of a person\"\n                   \"a photo of a car\"\n\n  2. Encode each prompt with CLIP text encoder\n     → 512D vectors\n\n  3. For test image:\n     - Encode with CLIP image encoder\n     - Compute similarity to each prompt\n     - Select highest similarity\n\n  4. Done! Zero-shot classification\n\nExample:\n  Image similarity scores:\n    \"a photo of a bird\": 0.92    ← Highest\n    \"a photo of a person\": 0.15\n    \"a photo of a car\": 0.08\n\n  Prediction: Bird\n```\n\n**Why it works:**\n\n```\nCLIP trained on 400M diverse image-caption pairs\nLearned that:\n  - Images with birds cluster with \"bird\" text\n  - Images with people cluster with \"person\" text\n  - Images with cars cluster with \"car\" text\n\nThese mappings generalize to new images!\n\nTransfer learning without fine-tuning:\n  - No labeled data needed\n  - No training required\n  - Immediate deployment\n```\n\n### Benchmark Results\n\n**Zero-shot transfer (ImageNet classification):**\n\n```\nTraditional supervised:\n  ResNet-50: 76.1% accuracy\n\nCLIP zero-shot:\n  CLIP-ViT-L/14: 62.8% accuracy\n\nSeems lower, BUT:\n  - CLIP trained on NO labeled images\n  - Just 400M raw internet data\n  - Immediately applicable to any category\n  - ResNet trained with 1.4M labeled ImageNet\n\nAdjusted for training data:\n  ResNet: 76.1% on specific dataset\n  CLIP: 62.8% on ANY dataset (zero-shot)\n\n  CLIP more generalizable!\n```\n\n**After fine-tuning on small labeled sets:**\n\n```\nImageNet (1% labeled):\n  CLIP: 76.3% accuracy\n\nComparison:\n  - CLIP fine-tuned with 1% labels ≈ ResNet with 100% labels\n  - 100× more data-efficient!\n  - Shows power of pre-training\n```\n\n**Other domains:**\n\n```\nTransfer to new datasets:\n\nSTL10 (airplane, bird, car, etc.):\n  CLIP: 92.9% zero-shot\n\nFood101 (food classification):\n  CLIP: 88.3% zero-shot\n\nEuroSAT (satellite imagery):\n  CLIP: 58.4% zero-shot\n\nWorks across diverse domains!\n```\n\n### Why CLIP is Revolutionary\n\n**1. Scale:**\n```\n400M image-text pairs >> 1.4M ImageNet\nShows power of scale in representation learning\nUnlabeled data is abundant!\n```\n\n**2. Natural supervision:**\n```\nLanguage is natural way to describe images\nNot forced to 1000 classes like ImageNet\nFlexible descriptors\nCan specify any attribute\n```\n\n**3. Zero-shot transfer:**\n```\nNo fine-tuning needed\nImmediate deployment\nNo labeled data required\nGeneralizes across domains\n```\n\n**4. Open-ended prediction:**\n```\nNot limited to predefined classes\nCan describe images with any text\n\"A cat wearing a hat\"\n\"A red car on a mountain\"\nAny description works!\n```\n\n### Impact on Field\n\n```\nCLIP (April 2021) was watershed moment\n\nBefore CLIP:\n  - Supervised learning paradigm dominant\n  - Limited to ImageNet 1000 classes\n  - Required labeled data for new tasks\n  - Struggled on out-of-distribution data\n\nAfter CLIP:\n  - Contrastive learning became mainstream\n  - Foundation model era began\n  - Zero-shot transfer became practical\n  - Industry adopted language-grounded vision\n\nInspired:\n  - ALIGN (Google)\n  - LiT (Google)\n  - COCA (Meta)\n  - Flamingo (DeepMind)\n  - BLIP (Salesforce)\n  - Many others...\n```\n\n## 7.4 Variants and Extensions of Contrastive Learning\n\n### Method 1: SimCLR - Self-Supervised Vision\n\n**Motivation:**\n```\nCLIP uses text for supervision\nWhat if we only have unlabeled images?\n\nAnswer: Use image augmentations as \"supervision\"\n```\n\n**Core idea:**\n\n```\nSingle image:\n  [Original cat photo]\n\nCreate two augmented versions:\n  [Rotated, cropped, color-adjusted]\n  [Different rotation, crop, colors]\n\nTreat as positive pair:\n  Both should have similar representations\n  (Same cat, different augmentations)\n\nNegatives:\n  Other images in batch\n\nLoss: Make augmentations similar,\n      other images dissimilar\n```\n\n**Process:**\n\n```\n1. Sample image x from dataset\n\n2. Create two augmented versions:\n   x_i = Aug(x)  (augmentation 1)\n   x_j = Aug(x)  (augmentation 2)\n\n   Different random augmentations!\n\n3. Encode both through network f:\n   h_i = f(x_i)\n   h_j = f(x_j)\n\n4. Project to embedding space:\n   z_i = g(h_i)\n   z_j = g(h_j)\n\n5. Contrastive loss:\n   sim(z_i, z_j) should be high\n   sim(z_i, z_k) should be low (for k ≠ i,j)\n\n6. Backprop updates f and g\n```\n\n**Key insights:**\n\n```\nWhy this works:\n\nAssumptions:\n  1. Augmentations preserve content\n  2. Different images are different\n\nImplications:\n  Model learns representations that:\n  - Survive augmentations (robust features)\n  - Differ between images (discriminative features)\n  - Capture semantic content (not style)\n\nResult:\n  Representations useful for downstream tasks\n  Without any labels!\n```\n\n**Augmentations used:**\n\n```\nStrong augmentations needed for self-supervised learning:\n\nRandom crop:\n  (up to 85% crop)\n  ↑ Forces learning of part representations\n\nColor jittering:\n  Brightness, contrast, saturation, hue\n  ↑ Prevents learning from color only\n\nGaussian blur:\n  Blurs fine details\n  ↑ Forces learning of structure, not pixels\n\nRandom grayscale:\n  Removes color information\n  ↑ Forces learning of shape and texture\n\nGaussian noise:\n  Adds random noise\n  ↑ Makes features robust\n\nNote: Extreme augmentations avoid (would destroy content)\n  - Extreme rotation: Flips meaning\n  - Extreme scaling: Makes object invisible\n  - Extreme distortion: No longer recognizable\n```\n\n**Differences from CLIP:**\n\n```\n                SimCLR          CLIP\n────────────────────────────────────\nSupervision     Image augment   Text\nData            Unlabeled       Image-caption pairs\nRequires        Images only     Images + text\nGeneralization  Moderate        Excellent\nTask alignment  Generic vision  Language grounding\nTransfer        Good            Excellent\nInterpretable   No              Yes (language)\n\nWhen to use:\n  SimCLR: When you only have unlabeled images\n  CLIP: When you have image-caption pairs\n```\n\n### Method 2: MoCo - Momentum Contrast\n\n**Problem with SimCLR:**\n\n```\nSimCLR requires large batch size:\n  - Small batch: Few negatives → weak learning signal\n  - Large batch: Better negatives → better learning\n\n  Batch size 4096 requires massive GPU memory\n  And distributed training complexity\n```\n\n**MoCo solution:**\n\n```\nUse memory bank instead of current batch\n\nBenefits:\n  ✓ Can use smaller batch size\n  ✓ Negatives more diverse (from different times)\n  ✓ More efficient\n```\n\n**Architecture:**\n\n```\nOnline encoder: f_q\n  Learns from current batch\n  Updated every step\n\nMemory bank: Queue\n  Stores recent representations\n  Old representations pushed out as new added\n\nMomentum encoder: f_k\n  Slowly following online encoder\n  f_k = α × f_k + (1-α) × f_q\n\n  Typically α = 0.999\n  Moves slowly (momentum!)\n\nProcess:\n\n1. Current batch through online encoder\n   → query embeddings q\n\n2. Pop old representations from queue\n   → memory negatives\n\n3. Compute loss using:\n   - query from online encoder (positive)\n   - memory from momentum encoder (negatives)\n\n4. Push new representations to queue\n\n5. Update momentum encoder (slowly follows online)\n```\n\n**Why momentum encoder:**\n\n```\nWithout it:\n  Queue contains representations from old network\n  Network keeps changing → representations inconsistent\n  Training unstable\n\nWith momentum encoder:\n  Queue contains representations from slow network\n  Representations are consistent\n  Training stable\n\nEffect:\n  Momentum = inertia\n  Small updates accumulate\n  Smooth trajectory\n```\n\n**Performance:**\n\n```\nImageNet pre-training → transfer to other tasks\n\n                Top-1 Accuracy\n────────────────────────────────\nSupervised      76.5% (ResNet-50)\nSimCLR          69.3% (requires large batch)\nMoCo v1         60.6% (with 65K negatives)\nMoCo v2         71.3% (improved version)\nMoCo v3         76.7% (vision transformer)\n\nNote: Self-supervised eventually matched supervised!\n      Shows power of approach\n```\n\n### Method 3: BYOL - Contrastive Without Negatives\n\n**Surprising finding (Grill et al., 2020):**\n\n```\nDo we even need negative examples?\n\nTraditional contrastive:\n  Make positives similar\n  Make negatives dissimilar\n\nBYOL:\n  Only make positives similar\n  No explicit negatives!\n\nQuestion: How does this work?\n\nAnswer: Still has implicit negatives\n        (Through model architecture and learning dynamics)\n```\n\n**Architecture:**\n\n```\nOnline network:\n  Encoder f + Projector g\n  Input: image → output: representation\n  Updated every step\n\nTarget network:\n  Copy of online network\n  Parameter updates: EMA (exponential moving average)\n  target_param = α × target_param + (1-α) × online_param\n\nPredictor h:\n  Additional MLP on top of online network\n  NOT on target network (asymmetry!)\n\nLoss:\n  For two augmentations of same image:\n  loss = ||h(online(aug1)) - target(aug2)||²\n\n  Make online and target predictions close\n  Using MSE loss (not contrastive!)\n\n  Also symmetrically:\n  loss += ||h(online(aug2)) - target(aug1)||²\n```\n\n**Why this works (still debated!):**\n\n```\nPossible explanations:\n\n1. Implicit negatives through optimization\n   - Mini-batch gradient descent creates diversity\n   - Network can't collapse to constant\n   - Similar to negative mining\n\n2. Momentum encoder provides stability\n   - Target network changes slowly\n   - Creates effective \"negatives\" through difference\n\n3. Predictor prevents mode collapse\n   - Without predictor: Would learn trivial solution\n   - With predictor: Breaks symmetry\n   - Forces meaningful learning\n\nEmpirical results:\n  BYOL works surprisingly well!\n  Without explicit negatives!\n  Counterintuitive but effective\n```\n\n**Advantages:**\n\n```\n✓ Doesn't need negative pairs\n✓ Don't need image-text pairs (image-only sufficient)\n✓ Works with small batches\n✓ Stable training\n✓ Strong performance (competitive with SimCLR)\n```\n\n**Disadvantages:**\n\n```\n✗ Why it works still not fully understood\n✗ Less interpretable\n✗ More complex architecture\n✗ Harder to debug when it fails\n```\n\n## 7.5 Practical Guide to Contrastive Learning\n\n### Implementing Contrastive Learning\n\n**Basic template:**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContrastiveLearningModel(nn.Module):\n    def __init__(self, encoder, projection_dim=256):\n        super().__init__()\n        self.encoder = encoder\n        self.projector = nn.Linear(encoder.output_dim, projection_dim)\n\n    def forward(self, x):\n        # Encode\n        h = self.encoder(x)\n\n        # Project\n        z = self.projector(h)\n\n        # Normalize\n        z = F.normalize(z, p=2, dim=1)\n\n        return z\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j):\n        \"\"\"\n        Compute NT-Xent loss\n        z_i, z_j: (batch_size, embedding_dim) tensors\n        \"\"\"\n        batch_size = z_i.shape[0]\n\n        # Concatenate: positive pairs are diagonal\n        z = torch.cat([z_i, z_j], dim=0)  # (2*batch, dim)\n\n        # Similarity matrix\n        similarity = torch.mm(z, z.t()) / self.temperature\n\n        # Create labels: diagonal elements are positives\n        labels = torch.arange(batch_size, device=z.device)\n        labels = torch.cat([labels, labels])\n\n        # Positive pairs at positions (i, batch+i) and (batch+i, i)\n        # Compute loss: each sample should match its pair\n\n        # Loss for all positions\n        loss = F.cross_entropy(similarity, labels)\n\n        return loss\n\n# Training loop\ndef train_contrastive(model, data_loader, optimizer, device, epochs=100):\n    criterion = ContrastiveLoss(temperature=0.07)\n\n    for epoch in range(epochs):\n        total_loss = 0\n\n        for images in data_loader:\n            # Get two augmented versions\n            x_i = augment(images)\n            x_j = augment(images)\n\n            x_i = x_i.to(device)\n            x_j = x_j.to(device)\n\n            # Forward pass\n            z_i = model(x_i)\n            z_j = model(x_j)\n\n            # Compute loss\n            loss = criterion(z_i, z_j)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(data_loader)\n        print(f\"Epoch {epoch}: Loss = {avg_loss:.3f}\")\n```\n\n### Choosing Hyperparameters\n\n**Temperature:**\n\n```\nRange: [0.05, 0.2]\n\nDiagnostic:\n  Training loss plateaus at high value?\n    → Temperature too low (sharp, unstable)\n    → Increase τ\n\n  Training loss decreases but very slowly?\n    → Temperature too high (smooth, weak signal)\n    → Decrease τ\n\nRule of thumb:\n  Start with τ = 0.1\n  Adjust based on loss curve\n```\n\n**Batch size:**\n\n```\nLarger batch = more negatives = better signal\n\nTypical choices:\n  Small GPU: 256-512\n  Medium GPU: 1024-2048\n  Large GPU: 4096+\n  Multi-GPU: 32K+ (like CLIP)\n\nTrade-off:\n  Larger batch: Better learning, slower per epoch\n  Smaller batch: Worse learning, faster per epoch\n```\n\n**Projection dimension:**\n\n```\nEmbedding dimension (before projection): 1024-2048 (from encoder)\nProjection dimension: 128-512\n\nCommon choices:\n  256D (standard)\n  128D (more compression)\n  512D (less compression)\n\nEffect:\n  Smaller: Faster computation, less memory\n  Larger: More expressive, risk of overfitting\n```\n\n**Number of negatives:**\n\n```\nWithin batch:\n  Batch size 256 → 255 negatives per sample\n\nMemory bank (MoCo):\n  Queue size 65536 → 65535 negatives\n\nMore negatives → better learning signal\nBut more computation\nTypical: 255-65K negatives\n```\n\n### Evaluating Contrastive Models\n\n**Method 1: Linear evaluation protocol**\n\n```\n1. Train contrastive model on unlabeled data\n   → Get representations\n\n2. Freeze encoder\n   → Don't update weights\n\n3. Train linear classifier on representations\n   → Small labeled dataset\n\n4. Evaluate on test set\n\nMetric: Accuracy of linear classifier\nInsight: If representations good → linear classifier accurate\n\nExample:\n  CIFAR-10 (50K training images)\n  Contrastive pre-training: All 50K unlabeled\n  Linear eval: 5K labeled for training, 10K for testing\n\n  Result: 96% accuracy\n  Interpretation: Representations capture meaningful patterns\n```\n\n**Method 2: Transfer learning evaluation**\n\n```\n1. Train contrastive model on source dataset\n2. Fine-tune on target task\n3. Compare to:\n   - Supervised baseline\n   - Random initialization\n   - Other pre-training methods\n\nMetric: Downstream task accuracy\nInsight: Better representations → better transfer\n```\n\n**Method 3: Downstream task performance**\n\n```\nPre-training dataset: ImageNet (unlabeled contrastive)\nDownstream tasks:\n  1. ImageNet-100 classification (supervised fine-tune)\n  2. CIFAR-10 classification\n  3. STL10 classification\n  4. Transfer to object detection\n  5. Transfer to segmentation\n\nResults show generalization across tasks\n```\n\n## 7.6 Troubleshooting Contrastive Learning\n\n### Problem 1: Loss not decreasing\n\n**Potential causes:**\n\n```\n① Temperature too low\n   Effect: Softmax too sharp\n   Solution: Increase τ (e.g., 0.1 → 0.2)\n\n② Learning rate too small\n   Effect: Updates too tiny\n   Solution: Increase learning rate\n\n③ Batch size too small\n   Effect:\n\n-----\n\n\n```\n   Effect: Weak learning signal\n   Solution: Increase batch size if possible\n\n④ Bad initialization\n   Effect: Starting in bad local minimum\n   Solution: Use proper weight initialization\n\n⑤ Augmentations too weak\n   Effect: Positive pairs too similar anyway\n   Solution: Increase augmentation strength\n\n⑥ Augmentations too strong\n   Effect: Positive pairs become different objects\n   Solution: Decrease augmentation strength\n```\n\n**Debugging steps:**\n\n```python\n# 1. Check loss values\nprint(f\"Initial loss: {loss.item()}\")\n# Should decrease over time\n# If increasing or constant: something wrong\n\n# 2. Check similarity matrix\nsimilarity = torch.mm(z, z.t())\nprint(f\"Max similarity: {similarity.max():.3f}\")\nprint(f\"Min similarity: {similarity.min():.3f}\")\n# Should: Max ≈ 1, Min ≈ -1 for normalized vectors\n\n# 3. Check gradients\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name}: grad_norm={param.grad.norm():.3f}\")\n# Should be reasonable values (not 0, not inf)\n\n# 4. Check temperature effect\ntemperatures = [0.01, 0.05, 0.1, 0.2, 0.5]\nfor tau in temperatures:\n    loss = compute_loss(embeddings, tau)\n    print(f\"τ={tau}: loss={loss:.3f}\")\n# Should have sweet spot, not too high/low everywhere\n```\n\n### Problem 2: Representation collapse\n\n**What is it:**\n\n```\nModel learns to make all representations nearly identical\n\nExample:\n  All images → representation [0.5, 0.5, 0.5, ...]\n  All images → representation [0.51, 0.49, 0.50, ...]\n\n  Trivial solution: \"All same = all similar\"\n  Loss can be artificially low!\n  But representations useless for downstream tasks\n```\n\n**Symptoms:**\n\n```\n✓ Loss decreasing nicely\n✗ Linear evaluation performance poor\n✗ Representations clustered at single point\n✗ Variance of representations near zero\n```\n\n**Causes and solutions:**\n\n```\nCause 1: No negatives (only positives)\n  Solution: Ensure you have negatives in batch\n\nCause 2: Batch too small\n  Solution: Increase batch size\n\nCause 3: No regularization\n  Solution: Add normalization (L2 normalization helps)\n\nCause 4: Poor augmentations\n  Solution: Ensure augmentations are meaningful\n  (Reproduce the issue with weak augmentations)\n```\n\n**Prevention:**\n\n```python\n# Monitor variance\ndef monitor_collapse(z):\n    \"\"\"Check if representations are collapsing\"\"\"\n    # Variance across batch\n    variance = torch.var(z, dim=0).mean()\n\n    # Std across batch\n    std = torch.std(z, dim=0).mean()\n\n    print(f\"Variance: {variance:.4f}\")\n    print(f\"Std: {std:.4f}\")\n\n    if variance < 0.001:\n        print(\"WARNING: Representations collapsing!\")\n        return False\n    return True\n\n# During training\nfor z_i, z_j in batches:\n    if not monitor_collapse(z_i):\n        # Take corrective action\n        # Adjust learning rate, batch size, etc.\n        pass\n```\n\n### Problem 3: Slow convergence\n\n**Causes:**\n\n```\n① Learning rate too small\n   → Gradients don't produce meaningful updates\n   → Training takes forever\n\n② Too few negatives\n   → Weak learning signal\n   → Takes many steps to learn\n\n③ Bad data augmentation\n   → Positive pairs too similar/different\n   → Model confused about what to learn\n\n④ Model too complex\n   → Slow to train\n   → Consider simpler architecture\n```\n\n**Solutions:**\n\n```\n1. Learning rate warmup\n   Gradually increase LR from 0 to target\n   Helps with stability\n\n   Schedule:\n   LR(t) = target_lr * min(1, t / warmup_steps)\n\n2. Learning rate scheduling\n   Reduce LR as training progresses\n   Helps fine-tuning\n\n   CosineAnnealingLR: Common choice\n\n3. Increase batch size\n   If hardware permits\n   Each sample gets more negatives\n   Stronger learning signal\n\n4. Use momentum\n   Keep moving average of gradients\n   Smooths noisy gradient signal\n```\n\n## Key Takeaways\n\n- **Contrastive learning** learns from similarity/dissimilarity without labels\n- **InfoNCE loss** is the foundation: maximize positive similarity relative to negatives\n- **CLIP** revolutionized the field with language-grounded vision at scale\n- **Temperature** controls softmax sharpness and learning signal\n- **Self-supervised variants** (SimCLR, MoCo, BYOL) enable learning from unlabeled data\n- **Large batch size** provides more negatives and stronger signal\n- **Hyperparameter tuning** (temperature, batch size, augmentation) is crucial\n- **Representation collapse** is a real risk to monitor\n\n## Exercises\n\n**⭐ Beginner:**\n1. Implement InfoNCE loss from scratch\n2. Compute temperature effects on loss\n3. Understand positive/negative pairs in a batch\n\n**⭐⭐ Intermediate:**\n4. Build image-text contrastive model on small dataset\n5. Implement temperature scheduling\n6. Compare different similarity metrics\n\n**⭐⭐⭐ Advanced:**\n7. Implement SimCLR with proper augmentations\n8. Build MoCo with momentum encoder\n9. Debug and fix representation collapse\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-07.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}