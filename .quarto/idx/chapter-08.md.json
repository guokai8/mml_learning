{"title":"Chapter 8: Transformer Architecture","markdown":{"headingText":"Chapter 8: Transformer Architecture","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 7: Contrastive Learning](chapter-07.md) | **Next**: [Chapter 9: Generative Models for Multimodal Data](chapter-09.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand transformer fundamentals\n- Explain self-attention mechanism\n- Implement multi-head attention\n- Understand encoder-decoder architecture\n- Apply transformers to multimodal tasks\n\n## 8.1 The Problem Transformers Solve\n\n### Limitations of Sequential Models (RNNs)\n\n**RNN limitations:**\n\n```\nProcessing sequence: w1, w2, w3, w4, w5\n\nRNN forward pass (sequential):\n  h0 = initialization\n  h1 = RNN(w1, h0)  ← Must wait for h0\n  h2 = RNN(w2, h1)  ← Must wait for h1\n  h3 = RNN(w3, h2)  ← Must wait for h2\n  h4 = RNN(w4, h3)  ← Must wait for h3\n  h5 = RNN(w5, h4)  ← Must wait for h4\n\nProblems:\n① Cannot parallelize\n   Each step depends on previous\n   Sequential bottleneck\n\n② Gradient flow issues\n   Backprop through 5 steps:\n   gradient = ∂h5/∂h4 × ∂h4/∂h3 × ∂h3/∂h2 × ∂h2/∂h1 × ∂h1/∂h0\n\n   Each factor typically < 1:\n   0.9^5 = 0.59  (50% loss)\n   0.9^100 ≈ 0   (vanishing gradient)\n\n③ Limited context window\n   Position t can see positions [0, t-1]\n   Cannot look ahead (in some RNNs)\n   Information degrades over long sequences\n```\n\n### CNN Limitations for Sequences\n\n**CNN characteristics:**\n\n```\nLocal receptive field:\n  3×3 kernel sees 9 neighbors\n  To see position distance 10:\n  Need log(10) ≈ 4 layers\n\n  For long sequences:\n  Need many layers\n  Deep networks = hard to train\n```\n\n### Transformer Solution\n\n**Key insight:**\n\n```\nWhy wait for sequential dependencies?\n\nWhat if every position could see every other position simultaneously?\n\nQuery: Position i\nKey/Value: All positions (including i)\n\nAttention: Position i attends to all positions\nResult: Global context immediately available!\n\nBenefit:\n① Fully parallelizable\n   All positions process simultaneously\n   Each GPU core handles one position\n\n② No sequential bottleneck\n\n③ Long-range dependencies captured immediately\n   Position 0 can \"see\" position 100 in layer 1\n   No need for deep networks\n```\n\n## 8.2 Self-Attention Mechanism\n\n### Intuition\n\n**Example - Machine translation:**\n\n```\nEnglish: \"The animal didn't cross the street because it was too tired\"\n\nAmbiguity: What does \"it\" refer to?\n  Option A: \"animal\" (correct)\n  Option B: \"street\" (incorrect)\n\nHow humans understand:\n  Focus on \"it\" (pronoun)\n  Look back at possible referents: \"animal\", \"street\"\n  \"Animal\" makes more sense in context\n  → \"it\" = \"animal\"\n\nSelf-attention for \"it\":\n  Query: \"it\"\n  Key/Value options: [\"The\", \"animal\", \"didn't\", ..., \"tired\"]\n  Attention: Which words help interpret \"it\"?\n    \"animal\": High attention (antecedent)\n    \"cross\": Medium attention (related event)\n    \"The\": Low attention (not informative)\n  Result: \"it\" representation influenced mainly by \"animal\"\n```\n\n### Mathematical Definition\n\n**Components:**\n\n```\nQuery (Q): What am I asking about?\nKey (K): What information is available?\nValue (V): What to retrieve?\n\nAnalogy - Database:\n  Query: Search terms (\"animal\")\n  Keys: Database field names and values\n  Values: Data to retrieve\n\nExample:\n  Query: \"hungry\"\n  Key matches: \"starving\" (high similarity), \"tired\" (medium)\n  Values: Corresponding word embeddings\n  Result: Weighted sum of values based on key similarity to query\n```\n\n**Formula:**\n\n```\nAttention(Q, K, V) = softmax(Q @ K^T / √d_k) @ V\n\nBreakdown:\n\nQ @ K^T:\n  Query dot Key\n  Shape: (seq_len, seq_len)\n  Result: similarity matrix\n  Element [i,j] = how much query_i matches key_j\n\n  / √d_k:\n  Normalization by embedding dimension\n  Prevents gradient explosion\n\nsoftmax(...):\n  Convert similarities to probabilities [0,1]\n  Sum to 1 per row\n  Interpretation: How much to \"pay attention\" to each position\n\n@ V:\n  Weight values by attention weights\n  Result: Weighted combination of value vectors\n  Each query gets context-specific value\n```\n\n### Numerical Example\n\n**Setup:**\n\n```\nSequence: [\"The\", \"cat\", \"sat\"]\nEmbedding dimension: d_k = 4\n\nQuery vectors:\n  Q1 = [0.1, 0.2, 0.3, 0.1]  for \"The\"\n  Q2 = [0.4, 0.1, 0.2, 0.3]  for \"cat\"\n  Q3 = [0.2, 0.3, 0.1, 0.4]  for \"sat\"\n\nKey vectors (same as query in self-attention):\n  K1 = [0.1, 0.2, 0.3, 0.1]  for \"The\"\n  K2 = [0.4, 0.1, 0.2, 0.3]  for \"cat\"\n  K3 = [0.2, 0.3, 0.1, 0.4]  for \"sat\"\n\nValue vectors:\n  V1 = [1, 0, 0, 0]  for \"The\"\n  V2 = [0, 1, 0, 0]  for \"cat\"\n  V3 = [0, 0, 1, 0]  for \"sat\"\n```\n\n**Computation for first query (position 0: \"The\"):**\n\n```\nStep 1: Q1 @ K^T (similarity scores)\n  Q1·K1 = 0.1*0.1 + 0.2*0.2 + 0.3*0.3 + 0.1*0.1 = 0.15\n  Q1·K2 = 0.1*0.4 + 0.2*0.1 + 0.3*0.2 + 0.1*0.3 = 0.15\n  Q1·K3 = 0.1*0.2 + 0.2*0.3 + 0.3*0.1 + 0.1*0.4 = 0.15\n\n  Scores: [0.15, 0.15, 0.15]  (all equal - new in training)\n\nStep 2: Divide by √d_k = √4 = 2\n  [0.075, 0.075, 0.075]\n\nStep 3: Softmax\n  exp(0.075) ≈ 1.078\n  exp(0.075) ≈ 1.078\n  exp(0.075) ≈ 1.078\n\n  Sum: 3.234\n\n  Softmax: [1.078/3.234, 1.078/3.234, 1.078/3.234]\n         = [0.333, 0.333, 0.333]\n         (uniform distribution)\n\nStep 4: Weight values\n  0.333 * V1 + 0.333 * V2 + 0.333 * V3\n  = 0.333 * [1,0,0,0] + 0.333 * [0,1,0,0] + 0.333 * [0,0,1,0]\n  = [0.333, 0.333, 0.333, 0]\n```\n\n**After training:**\n\n```\nWith learned embeddings, differences emerge:\n\nStep 1: Q1 @ K^T\n  Q1·K1 = 0.8   (high - \"The\" attends to itself)\n  Q1·K2 = 0.2   (low - \"The\" doesn't attend to \"cat\")\n  Q1·K3 = 0.3   (low - \"The\" doesn't attend to \"sat\")\n\nStep 2: After scaling and softmax\n  [0.7, 0.15, 0.15]\n\nStep 3: Weighted values\n  0.7 * V1 + 0.15 * V2 + 0.15 * V3\n  = [0.7, 0.15, 0.15, 0]\n\n  Interpretation:\n  \"The\" mostly looks at itself\n  Some information from neighboring words\n  Reasonable: \"The\" is article, not much context needed\n```\n\n### Multi-Head Attention\n\n**Why multiple heads?**\n\n```\nSingle attention head learns one type of relationship\nDifferent heads can learn different patterns\n\nHead 1: Syntactic (grammar)\n  \"verb\" attends to \"object\"\n  \"noun\" attends to \"adjective\"\n\nHead 2: Semantic (meaning)\n  \"pronoun\" attends to \"antecedent\"\n  \"reference\" attends to \"entity\"\n\nHead 3: Long-range\n  \"end of sentence\" attends to \"beginning\"\n  Captures discourse structure\n\nHead 4: Word type\n  Different parts of speech have different patterns\n\nMultiple heads = multiple representation subspaces\nMore expressive than single head\n```\n\n**Architecture:**\n\n```\nInput: x (seq_len, d_model)\n\nFor each head h = 1 to num_heads:\n  ① Project to query space\n     Q_h = x @ W_q^(h)    (seq_len, d_k)\n\n  ② Project to key space\n     K_h = x @ W_k^(h)    (seq_len, d_k)\n\n  ③ Project to value space\n     V_h = x @ W_v^(h)    (seq_len, d_v)\n\n  ④ Compute attention\n     head_h = Attention(Q_h, K_h, V_h)  (seq_len, d_v)\n\nConcatenate all heads:\n  MultiHead = [head_1 || head_2 || ... || head_h]\n              (seq_len, h*d_v)\n\nLinear projection:\n  output = MultiHead @ W_o\n           (seq_len, d_model)\n```\n\n**Example - 8 heads with d_model=512:**\n\n```\nEach head operates in d_k = 512/8 = 64 dimensional space\n8 different projection matrices per Q, K, V\n\nResult:\n  8 independent attention mechanisms\n  Each learns different patterns\n  Combined through concatenation and final projection\n\nTotal parameters for multi-head attention:\n  Q projections: 8 × 512 × 64 = 262K\n  K projections: 8 × 512 × 64 = 262K\n  V projections: 8 × 512 × 64 = 262K\n  Output projection: 512 × 512 = 262K\n  Total: ~1M parameters per multi-head attention layer\n```\n\n### Scaled Dot-Product Attention Revisited\n\n**Why scale by 1/√d_k?**\n\n```\nReason: Prevents gradient vanishing\n\nWithout scaling:\n  For large d_k:\n  Q @ K^T values become very large\n\n  Example: Q and K each 64D\n  Dot product: 64 independent terms\n  Average value: 64 * (avg term)\n\n  Large values → softmax saturates → gradients → 0\n\nScaling by 1/√d_k:\n  Normalizes dot product variance\n  Keep values in reasonable range [-1, 1] roughly\n  Softmax doesn't saturate\n  Gradients flow properly\n\nMathematical justification:\n  Var(Q @ K^T) = Var(Σ q_i * k_i)\n                = Σ Var(q_i * k_i)\n                = d_k  (if independent)\n\n  Std dev = √d_k\n\n  Scaling by 1/√d_k makes std dev = 1\n  Keeps gradients stable\n```\n\n## 8.3 Transformer Encoder\n\n### Architecture Overview\n\n```\nInput sequence\n    ↓\nEmbedding + Positional Encoding\n    ↓\n┌─────────────────────────────┐\n│  Transformer Encoder Layer  │ ×N (typically 12)\n│  ┌────────────────────────┐ │\n│  │ Multi-Head Attention   │ │\n│  └────────┬───────────────┘ │\n│           ↓                  │\n│  ┌─────────────────────────┐ │\n│  │ Add & Normalize         │ │\n│  └────────┬────────────────┘ │\n│           ↓                  │\n│  ┌─────────────────────────┐ │\n│  │ Feed-Forward Network    │ │\n│  │ (2 linear layers, ReLU) │ │\n│  └────────┬────────────────┘ │\n│           ↓                  │\n│  ┌─────────────────────────┐ │\n│  │ Add & Normalize         │ │\n│  └────────┬────────────────┘ │\n└─────────────────────────────┘\n    ↓\nOutput (same shape as input)\n```\n\n### Detailed Layer Breakdown\n\n**1. Positional Encoding**\n\n```\nProblem: Self-attention is permutation invariant\n  Meaning: Word order doesn't matter!\n\n  Attention doesn't care about position\n  Just about content similarity\n\n  Example:\n    \"dog bites man\" vs \"man bites dog\"\n    Same words, different meaning\n    But attention treats them the same!\n\nSolution: Add position information\n\nSinusoidal encoding:\n  PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n  PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\n  Where:\n    pos = position in sequence (0, 1, 2, ...)\n    i = dimension index (0, 1, 2, ..., d_model/2)\n\nExample: Position 0, dimension 0\n  PE(0, 0) = sin(0) = 0\n\n  Position 1, dimension 0:\n  PE(1, 0) = sin(1 / 10000^0) = sin(1) ≈ 0.84\n\nProperties:\n  ① Different positions have different encodings\n  ② Patterns repeat at different frequencies\n  ③ Model can learn relative positions\n  ④ Can extrapolate to longer sequences than training\n```\n\n**2. Multi-Head Self-Attention**\n\n```\nAll positions attend to all positions\n8-12 heads typically\nEach head learns different patterns\n\nOutput same shape as input\n```\n\n**3. Add & Normalize (Residual Connection + Layer Normalization)**\n\n```\nResidual connection:\n  output = attention_output + input\n\n  Why?\n  ① Preserves original information\n  ② Enables deep networks (gradient flows directly)\n  ③ Output can learn \"residual\" (difference)\n\nLayer Normalization:\n  Normalize across feature dimension\n\n  mean = mean(x along d_model dimension)\n  variance = var(x along d_model dimension)\n  normalized = (x - mean) / sqrt(variance + epsilon)\n  output = γ * normalized + β\n\n  γ, β are learnable parameters\n\n  Why LN instead of Batch Norm?\n  ① Batch norm depends on batch statistics\n     Different at train/test time\n\n  ② Layer norm is deterministic\n     Doesn't depend on batch\n     Same at train/test\n\n  ③ Works better for sequences\n```\n\n**4. Feed-Forward Network**\n\n```\nMLP with 2 layers and ReLU:\n\nFFN(x) = Linear2(ReLU(Linear1(x)))\n\nDimensions:\n  Linear1: d_model → d_ff (usually 4*d_model)\n  ReLU: d_ff → d_ff\n  Linear2: d_ff → d_model\n\nExample with d_model = 512:\n  Linear1: 512 → 2048\n  ReLU: 2048 → 2048\n  Linear2: 2048 → 512\n\nWhy expand then contract?\n  ① Increases non-linearity\n  ② More expressive intermediate representation\n  ③ Standard in deep learning\n```\n\n### Full Transformer Encoder\n\n**BERT (Bidirectional Encoder Representations from Transformers):**\n\n```\nArchitecture:\n  ① Input: Tokens or subword units\n\n  ② Embeddings:\n     - Token embedding (word id → vector)\n     - Positional embedding (position → vector)\n     - Segment embedding (which sentence)\n     - Sum all three\n\n  ③ 12 layers (BERT-base) or 24 (BERT-large) of:\n     - Multi-head attention (8-12 heads)\n     - Feed-forward network\n     - Residual connections + Layer norm\n\n  ④ Output: Contextual embedding for each token\n\nExample input: \"The cat sat on the mat\"\n\nProcessing:\n  [CLS] The cat sat on the mat [SEP]\n    ↓\n  Embed each token\n    ↓\n  Add positional info\n    ↓\n  Layer 1:\n    All tokens attend to all tokens\n    Self-attention with 12 different heads\n\n    \"The\" attends to: \"The\" (self), \"cat\", \"sat\", \"on\", \"the\", \"mat\"\n    Different heads pay attention to different words\n    Results concatenated\n\n    Feed-forward applied to each position\n    Residual + Layer norm\n\n  Layer 2-12:\n    Same process, but input is output from previous layer\n    Further refinement\n\nOutput:\n  12 vectors for each token\n  Plus one special [CLS] token representing full sequence\n```\n\n## 8.4 Transformer Decoder\n\n### Causal Masking\n\n**Problem:**\n\n```\nDuring inference, generate one token at a time:\n  Step 1: Predict token 1 (no prior tokens)\n  Step 2: Predict token 2 (given token 1)\n  Step 3: Predict token 3 (given tokens 1, 2)\n\nDuring training (teacher forcing):\n  Complete sequence available: token 1, 2, 3, 4, 5\n\nTo prepare model for inference:\n  Hide future tokens during training\n\nMechanism: Causal mask\n```\n\n**Causal mask visualization:**\n\n```\nAttention positions (what can attend to what):\n\nSequence: [token_1, token_2, token_3, token_4, token_5]\n\nPosition 1 (token_1):\n  Can attend to: position 1\n  Cannot attend to: positions 2, 3, 4, 5\n\nPosition 2 (token_2):\n  Can attend to: positions 1, 2\n  Cannot attend to: positions 3, 4, 5\n\nPosition 3 (token_3):\n  Can attend to: positions 1, 2, 3\n  Cannot attend to: positions 4, 5\n\nAttention matrix (✓ = can attend, ✗ = masked):\n\n       pos1  pos2  pos3  pos4  pos5\npos1    ✓     ✗     ✗     ✗     ✗\npos2    ✓     ✓     ✗     ✗     ✗\npos3    ✓     ✓     ✓     ✗     ✗\npos4    ✓     ✓     ✓     ✓     ✗\npos5    ✓     ✓     ✓     ✓     ✓\n\nMask implementation:\n  Before softmax, set masked positions to -∞\n  softmax(-∞) = 0\n  Effect: Attention weight = 0 for masked positions\n```\n\n### Autoregressive Generation\n\n**Process:**\n\n```\n① Start: Input special token [START]\n         Decoder produces distribution over vocabulary\n\n② Step 1: Sample/select token with highest probability\n          Let's say we get \"A\"\n\n③ Step 2: Input \"[START] A\"\n          Decoder predicts next token\n          Get \"red\"\n\n④ Step 3: Input \"[START] A red\"\n          Decoder predicts next token\n          Get \"cat\"\n\n⑤ Continue until [END] token or max length\n\nGenerated: \"A red cat\"\n```\n\n**Key points:**\n\n```\n① Causal masking ensures only past tokens visible\n② Gradual refinement of representation\n③ Can use greedy (highest probability) or sampling\n④ Sampling: More diverse but less controlled\n   Greedy: More consistent but can repeat\n```\n\n### Cross-Attention in Decoder\n\n**Integration with encoder:**\n\n```\nEncoder processes source:\n  e.g., Image encoded to 196 patch embeddings\n\nDecoder:\n  ① Self-attention: Decoder attends to previously generated tokens\n  ② Cross-attention: Decoder attends to encoder output\n  ③ Feed-forward\n\nCross-attention details:\n  Query: Current decoder hidden state\n  Key/Value: Encoder output\n\n  Result: Decoder can look at source modality\n          Ground generation in input\n```\n\n**Example - Image captioning:**\n\n```\nImage: [Cat photo]\n       ↓\nImage encoder: 196 patch embeddings\n\nDecoder generating caption:\n\nStep 1:\n  Decoder input: [START]\n  Self-attention: Only [START], attends to itself\n  Cross-attention: [START] attends to all 196 patches\n                   \"What's in image?\"\n  Output: Probability distribution for first word\n\nStep 2:\n  Decoder input: [START] A\n  Self-attention: \"A\" attends to [START] and itself\n                  \"What context?\"\n  Cross-attention: \"A\" attends to patches\n                   \"What modifies 'A'?\"\n  Output: \"cat\"\n\nStep 3:\n  Decoder input: [START] A cat\n  Self-attention: \"cat\" attends to [START], \"A\", \"cat\"\n  Cross-attention: \"cat\" attends to patches\n                   \"What object is this?\"\n  Output: \"sitting\"\n\n...\n\nCaption: \"A cat sitting on a couch\"\n```\n\n## 8.5 Putting it Together: Vision Transformer (ViT)\n\n### Architecture for Images\n\n```\nImage (224×224×3)\n    ↓\nDivide into patches (16×16)\n    ↓\n14 × 14 = 196 patches\n    ↓\nEach patch: 16×16×3 = 768D\n    ↓\nLinear projection: 768D → 768D embedding\n    ↓\nAdd [CLS] special token\n    ↓\nPositional encoding (196 + 1 positions)\n    ↓\nConcatenate: [[CLS]; patch_1; patch_2; ...; patch_196]\n             (197 tokens of 768D each)\n    ↓\nTransformer encoder (12 layers):\n  Multi-head attention (12 heads)\n  Feed-forward (3072D intermediate)\n  Residual connections + Layer norm\n    ↓\nExtract [CLS] token representation\n    ↓\nLinear classifier: 768D → num_classes\n    ↓\nOutput: Class probabilities\n```\n\n### Why ViT Works\n\n```\nKey insight 1: Patches as tokens\n  Images have spatial structure\n  Patches preserve local information\n  Transformer learns global relationships\n\nKey insight 2: Transformer is universal\n  Can process any sequence of tokens\n  Doesn't care if tokens are image or text\n  Same architecture works for both!\n\nKey insight 3: Attention gives global context\n  CNNs need many layers for global receptive field\n  ViT has global attention from layer 1\n  Enables fast learning\n\nEmpirical finding:\n  With small data: CNN >> ViT\n  With large data: ViT >> CNN\n\n  Trade-off: Inductive bias vs expressive power\n  CNN: Strong inductive bias (local structure)\n       Works with limited data\n  ViT: Weak inductive bias\n       Needs large data to learn structure\n```\n\n## Key Takeaways\n\n- **Transformers** solve sequential bottleneck through attention\n- **Self-attention** computes context through similarity\n- **Multi-head attention** learns diverse patterns\n- **Positional encoding** preserves sequence order\n- **Residual connections** enable deep networks\n- **Feed-forward networks** add non-linearity\n- **Causal masking** enables autoregressive generation\n- **Cross-attention** connects source and target\n- **Vision Transformer** shows transformers work for images too\n\n## Exercises\n\n**⭐ Beginner:**\n1. Compute self-attention by hand\n2. Understand causal masking\n3. Visualize positional encoding patterns\n\n**⭐⭐ Intermediate:**\n4. Implement multi-head attention from scratch\n5. Build simple transformer encoder\n6. Visualize attention patterns\n\n**⭐⭐⭐ Advanced:**\n7. Implement full transformer encoder-decoder\n8. Build Vision Transformer\n9. Implement efficient attention variants\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-08.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}