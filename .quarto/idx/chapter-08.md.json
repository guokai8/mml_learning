{"title":"Chapter 8: Transformer Architecture","markdown":{"headingText":"Chapter 8: Transformer Architecture","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 7: Contrastive Learning](chapter-07.md) | **Next**: [Chapter 9: Generative Models for Multimodal Data](chapter-09.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand why transformers replaced RNNs for sequence modeling\n- Implement scaled dot-product attention from scratch\n- Explain the role of positional encoding\n- Understand the complete transformer architecture\n- Apply transformers to multimodal problems\n\n## 8.1 Motivation: Why Transformers?\n\n### Sequential Processing Limitations\n\n**RNN Problem Example:**\n\n```\nProcessing sequence: w₁, w₂, w₃, w₄, w₅\n\nRNN forward pass (sequential):\n  h₀ = initialization\n  h₁ = RNN(w₁, h₀)  ← Must wait for h₀\n  h₂ = RNN(w₂, h₁)  ← Must wait for h₁\n  h₃ = RNN(w₃, h₂)  ← Must wait for h₂\n  h₄ = RNN(w₄, h₃)  ← Must wait for h₃\n  h₅ = RNN(w₅, h₄)  ← Must wait for h₄\n\nProblems:\n① Cannot parallelize\n   Each step depends on previous\n   Sequential bottleneck\n\n② Gradient flow issues during backpropagation\n   Let's analyze this carefully using the chain rule...\n```\n\n### Gradient Flow Analysis (Chain Rule)\n\n**Mathematical derivation of the vanishing gradient problem:**\n\nFor an RNN with hidden states h₀, h₁, h₂, ..., h_T, let's compute the gradient of the loss L with respect to the initial hidden state h₀.\n\nUsing the chain rule:\n```\n∂L/∂h₀ = ∂L/∂h_T × ∂h_T/∂h₀\n\nTo compute ∂h_T/∂h₀, we apply the chain rule through all intermediate states:\n\n∂h_T/∂h₀ = (∂h_T/∂h_{T-1}) × (∂h_{T-1}/∂h_{T-2}) × ... × (∂h₂/∂h₁) × (∂h₁/∂h₀)\n\nThis is a product of T partial derivatives!\n```\n\n**Why this causes problems:**\n\n```\nFor a typical RNN transition function:\nh_t = tanh(W_h h_{t-1} + W_x x_t + b)\n\nThe partial derivative is:\n∂h_t/∂h_{t-1} = diag(tanh'(W_h h_{t-1} + W_x x_t + b)) × W_h\n\nWhere tanh'(z) = 1 - tanh²(z) ∈ (0, 1]\n\nKey insights:\n1. tanh'(z) ≤ 1 always\n2. ||W_h|| (matrix norm) is typically ≤ 1 to prevent exploding gradients\n3. Therefore ||∂h_t/∂h_{t-1}|| ≤ ||W_h|| ≤ 1\n\nFinal gradient magnitude:\n||∂h_T/∂h₀|| ≤ ∏(t=1 to T) ||∂h_t/∂h_{t-1}|| ≤ (max_norm)^T\n\nIf max_norm = 0.9 and T = 100:\nGradient magnitude ≤ 0.9^100 ≈ 2.7 × 10^-5 (vanishing!)\n\nIf max_norm = 1.1 and T = 100:\nGradient magnitude ≥ 1.1^100 ≈ 1.4 × 10^4 (exploding!)\n```\n\n**The fundamental issue:**\n```\nLong sequences require gradients to flow through many multiplicative steps.\nEach step either shrinks (vanishing) or grows (exploding) the gradient.\nStable training requires delicate balance that's hard to achieve.\n\nResult: RNNs struggle with long-term dependencies\n```\n\n### CNN Limitations for Sequences\n\n**CNN characteristics:**\n\n```\nLocal receptive field:\n  3×3 kernel sees 3 neighbors in 1D sequence\n  To connect positions distance d apart:\n  Need O(log d) layers with exponential growth\n  Or O(d) layers with linear growth\n\nFor long sequences (length 1000):\n  Need many layers to see full context\n  Computational depth becomes prohibitive\n  \nPath length between distant positions:\n  RNN: O(n) sequential steps\n  CNN: O(log n) with careful design, O(n) worst case\n```\n\n### The Transformer Solution\n\n**Key insight:** Use attention to connect all positions directly\n\n```\nAttention mechanism:\n  Path length between ANY two positions: O(1)\n  All connections computed in parallel\n  No sequential dependencies\n  \nComputational benefits:\n  ✓ Parallelizable across sequence length\n  ✓ Constant path length for long-range dependencies\n  ✓ No gradient vanishing through temporal steps\n  ✓ More interpretable attention patterns\n```\n\n## 8.2 Attention Mechanism Deep Dive\n\n### Scaled Dot-Product Attention\n\n**Mathematical formulation:**\n```\nAttention(Q, K, V) = softmax(QK^T / √d_k)V\n\nWhere:\nQ ∈ ℝ^(n×d_k) = Query matrix (what we're looking for)\nK ∈ ℝ^(m×d_k) = Key matrix (what we're looking at)  \nV ∈ ℝ^(m×d_v) = Value matrix (what we retrieve)\nn = sequence length of queries\nm = sequence length of keys/values\nd_k = dimension of queries and keys\nd_v = dimension of values\n```\n\n**Step-by-step computation:**\n\n```\nStep 1: Compute similarity scores\n  Scores = Q @ K^T                    # Shape: (n, m)\n  \n  Each element Scores[i,j] = Q[i,:] · K[j,:]\n  Interpretation: How much query i \"matches\" key j\n\nStep 2: Scale by √d_k\n  Scaled_scores = Scores / √d_k\n  \n  Purpose: Prevent dot products from becoming too large\n  (Details explained below)\n\nStep 3: Apply softmax\n  Attention_weights = softmax(Scaled_scores)  # Shape: (n, m)\n  \n  Each row sums to 1: Σⱼ Attention_weights[i,j] = 1\n  Interpretation: Probability distribution over keys for each query\n\nStep 4: Compute weighted sum of values\n  Output = Attention_weights @ V      # Shape: (n, d_v)\n  \n  Each output[i,:] = Σⱼ Attention_weights[i,j] × V[j,:]\n  Interpretation: Weighted combination of values based on attention\n```\n\n### Concrete Example\n\n**Setup:**\n```\nSequence: [\"The\", \"cat\", \"sat\"]\nEmbedding dimension: d_k = 4\n\nQuery vectors (what each word is \"asking about\"):\n  Q = [\n    [0.1, 0.2, 0.3, 0.1],    # \"The\" \n    [0.4, 0.1, 0.2, 0.3],    # \"cat\"\n    [0.2, 0.3, 0.1, 0.4]     # \"sat\"\n  ]\n\nKey vectors (what each word \"offers\"):\n  K = [\n    [0.2, 0.1, 0.4, 0.2],    # \"The\"\n    [0.3, 0.4, 0.1, 0.3],    # \"cat\"  \n    [0.1, 0.2, 0.3, 0.4]     # \"sat\"\n  ]\n\nValue vectors (what each word \"contains\"):\n  V = [\n    [1.0, 0.0],              # \"The\" content\n    [0.0, 1.0],              # \"cat\" content\n    [0.5, 0.5]               # \"sat\" content\n  ]\n```\n\n**Step 1: Compute scores**\n```\nScores = Q @ K^T = [\n  [0.1×0.2 + 0.2×0.1 + 0.3×0.4 + 0.1×0.2,  # \"The\" looks at \"The\"\n   0.1×0.3 + 0.2×0.4 + 0.3×0.1 + 0.1×0.3,  # \"The\" looks at \"cat\"\n   0.1×0.1 + 0.2×0.2 + 0.3×0.3 + 0.1×0.4], # \"The\" looks at \"sat\"\n  [...],  # \"cat\" row\n  [...]   # \"sat\" row\n] = [\n  [0.16, 0.14, 0.18],\n  [0.25, 0.30, 0.32],\n  [0.22, 0.28, 0.26]\n]\n```\n\n**Step 2: Scale**\n```\nScaled = Scores / √4 = Scores / 2 = [\n  [0.08, 0.07, 0.09],\n  [0.125, 0.15, 0.16],\n  [0.11, 0.14, 0.13]\n]\n```\n\n**Step 3: Softmax**\n```\nFor row 1 (word \"The\"):\nexp_values = [exp(0.08), exp(0.07), exp(0.09)] = [1.083, 1.073, 1.094]\nsum_exp = 1.083 + 1.073 + 1.094 = 3.25\nattention_weights[0,:] = [1.083/3.25, 1.073/3.25, 1.094/3.25] \n                       = [0.333, 0.330, 0.337]\n\nSimilarly for other rows:\nAttention_weights = [\n  [0.333, 0.330, 0.337],   # \"The\" attention distribution\n  [0.312, 0.342, 0.346],   # \"cat\" attention distribution  \n  [0.325, 0.349, 0.326]    # \"sat\" attention distribution\n]\n```\n\n**Step 4: Output**\n```\nOutput = Attention_weights @ V = [\n  [0.333×1.0 + 0.330×0.0 + 0.337×0.5,    # \"The\" output dim 1\n   0.333×0.0 + 0.330×1.0 + 0.337×0.5],   # \"The\" output dim 2\n  [...],  # \"cat\" output\n  [...]   # \"sat\" output\n] = [\n  [0.502, 0.498],\n  [0.481, 0.519],\n  [0.488, 0.512]\n]\n\nInterpretation:\n- \"The\" gets roughly equal mix of all word contents\n- \"cat\" pays slightly more attention to later words\n- \"sat\" pays balanced attention across the sequence\n```\n\n### Why Scale by √d_k?\n\n**Mathematical reasoning:**\n\n```\nProblem: Large dot products → extreme softmax values\n\nAssume Q[i,:] and K[j,:] have independent components ~ N(0,1)\nThen Q[i,:] · K[j,:] = Σₖ Q[i,k] × K[j,k]\n\nExpected value: E[Q[i,:] · K[j,:]] = 0\nVariance: Var[Q[i,:] · K[j,:]] = d_k\n\nStandard deviation: √d_k\n\nExample with d_k = 64:\nDot products have std = 8\nValues often in range [-24, 24]\n\nSoftmax of large values:\nexp(24) / (exp(24) + exp(20) + exp(16)) ≈ 1.0\nexp(20) / (exp(24) + exp(20) + exp(16)) ≈ 0.0  \nexp(16) / (exp(24) + exp(20) + exp(16)) ≈ 0.0\n\nResult: Attention becomes nearly one-hot (peaks at single position)\nGradients become very small (softmax saturation)\n```\n\n**Solution: Scale by √d_k**\n```\nScaled dot products: (Q[i,:] · K[j,:]) / √d_k\nNew standard deviation: √d_k / √d_k = 1\n\nKeeps dot products in reasonable range [-3, 3] typically\nSoftmax remains smooth and differentiable\nGradients flow properly during training\n```\n\n### Implementation\n\n```python\nimport torch\nimport torch.nn.functional as F\nimport math\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Compute scaled dot-product attention.\n    \n    Args:\n        Q: Query tensor (batch_size, seq_len_q, d_k)\n        K: Key tensor (batch_size, seq_len_k, d_k)  \n        V: Value tensor (batch_size, seq_len_v, d_v)\n        mask: Optional mask tensor (batch_size, seq_len_q, seq_len_k)\n    \n    Returns:\n        output: Attention output (batch_size, seq_len_q, d_v)\n        attention_weights: Attention scores (batch_size, seq_len_q, seq_len_k)\n    \"\"\"\n    \n    # Get dimension\n    d_k = Q.shape[-1]\n    \n    # Step 1: Compute similarity scores\n    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len_q, seq_len_k)\n    \n    # Step 2: Scale\n    scores = scores / math.sqrt(d_k)\n    \n    # Step 3: Apply mask (if provided)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    \n    # Step 4: Softmax\n    attention_weights = F.softmax(scores, dim=-1)  # (batch, seq_len_q, seq_len_k)\n    \n    # Step 5: Apply attention to values\n    output = torch.matmul(attention_weights, V)  # (batch, seq_len_q, d_v)\n    \n    return output, attention_weights\n\n# Example usage\nbatch_size, seq_len, d_model = 2, 5, 64\nQ = torch.randn(batch_size, seq_len, d_model)\nK = torch.randn(batch_size, seq_len, d_model)  \nV = torch.randn(batch_size, seq_len, d_model)\n\noutput, attention = scaled_dot_product_attention(Q, K, V)\nprint(f\"Output shape: {output.shape}\")      # (2, 5, 64)\nprint(f\"Attention shape: {attention.shape}\") # (2, 5, 5)\n```\n\n### Gradient Flow in Attention\n\n**Why attention helps with gradient flow:**\n\n```\nForward pass:\n  Q @ K^T → Scale → Softmax → @ V\n\nBackward pass (chain rule):\n  ∂L/∂V: Direct gradient from output\n  ∂L/∂(attention_weights): Computed from V gradient  \n  ∂L/∂(scores): Computed from softmax gradient\n  ∂L/∂K, ∂L/∂Q: Computed from scores gradient\n\nKey insight: Gradients flow directly through attention weights\n\nIf attention_weights[i,j] is high:\n  Position i receives strong gradient signal from position j\n  Direct connection enables strong learning\n\nIf attention_weights[i,j] is low:\n  Position i receives weak gradient signal from position j\n  But connection still exists (not zero)\n\nResult: Every position can attend to every other position\n        Gradient paths length = 1 (constant!)\n        No vanishing through sequential multiplication\n```\n\n## 8.3 Multi-Head Attention\n\n### Why Multiple Heads?\n\n**Single attention limitation:**\n```\nOne attention mechanism learns ONE type of relationship:\n- Might focus on syntactic dependencies (\"cat\" → \"sat\")\n- Or semantic similarities (\"cat\" → \"animal\")  \n- Or positional patterns (\"first\" → \"word\")\n\nBut we need MULTIPLE types of relationships simultaneously!\n```\n\n**Multi-head solution:**\n```\nLearn H different attention functions in parallel:\n- Head 1: Syntactic relationships\n- Head 2: Semantic relationships  \n- Head 3: Positional patterns\n- Head 4: Long-range dependencies\n- ...\n\nEach head can specialize in different patterns\n```\n\n### Mathematical Formulation\n\n```\nMultiHead(Q, K, V) = Concat(head₁, head₂, ..., head_h)W^O\n\nwhere head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n\nParameters:\n  W_i^Q ∈ ℝ^(d_model × d_k)  # Query projection for head i\n  W_i^K ∈ ℝ^(d_model × d_k)  # Key projection for head i  \n  W_i^V ∈ ℝ^(d_model × d_v)  # Value projection for head i\n  W^O ∈ ℝ^(h×d_v × d_model)  # Output projection\n\nTypical choices:\n  h = 8 heads\n  d_k = d_v = d_model / h = 64 (for d_model = 512)\n```\n\n**Process:**\n```\nInput: (batch, seq_len, d_model)\n\nFor each head i = 1 to h:\n\n  1. Project to smaller dimension\n     Q_i = input @ W_i^Q     (batch, seq_len, d_k)\n     K_i = input @ W_i^K     (batch, seq_len, d_k)\n     V_i = input @ W_i^V     (batch, seq_len, d_v)\n\n     Typical: d_model = 512, h = 8\n              d_k = d_v = 512/8 = 64\n\n  2. Compute attention\n     head_i = Attention(Q_i, K_i, V_i)  (batch, seq_len, d_v)\n\n3. Concatenate heads\n   concat = Concat(head₁, head₂, ..., head_h)  (batch, seq_len, h×d_v)\n\n4. Final projection  \n   output = concat @ W^O                        (batch, seq_len, d_model)\n```\n\n**Example - 8 heads with d_model=512:**\n\n```\nEach head operates in d_k = 512/8 = 64 dimensional space\n8 different projection matrices per Q, K, V\n\nResult:\n  8 independent attention mechanisms\n  Each learns different patterns\n  Combined through learned output projection\n\nComputational cost:\n  Same as single large head (512×512)\n  But more expressive due to multiple subspaces\n```\n\n### Implementation\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        assert d_model % n_heads == 0\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        # Linear projections for all heads (computed in parallel)\n        self.W_q = nn.Linear(d_model, d_model)  # All Q projections\n        self.W_k = nn.Linear(d_model, d_model)  # All K projections  \n        self.W_v = nn.Linear(d_model, d_model)  # All V projections\n        self.W_o = nn.Linear(d_model, d_model)  # Output projection\n        \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, d_model = query.shape\n        \n        # 1. Linear projections\n        Q = self.W_q(query)  # (batch, seq_len, d_model)\n        K = self.W_k(key)    # (batch, seq_len, d_model)\n        V = self.W_v(value)  # (batch, seq_len, d_model)\n        \n        # 2. Reshape for multi-head: (batch, seq_len, n_heads, d_k)\n        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k)\n        K = K.view(batch_size, seq_len, self.n_heads, self.d_k)\n        V = V.view(batch_size, seq_len, self.n_heads, self.d_k)\n        \n        # 3. Transpose: (batch, n_heads, seq_len, d_k)\n        Q = Q.transpose(1, 2)\n        K = K.transpose(1, 2)  \n        V = V.transpose(1, 2)\n        \n        # 4. Apply attention to each head\n        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n        \n        # 5. Concatenate heads: (batch, seq_len, d_model)\n        attention_output = attention_output.transpose(1, 2).contiguous()\n        attention_output = attention_output.view(batch_size, seq_len, d_model)\n        \n        # 6. Final linear projection\n        output = self.W_o(attention_output)\n        \n        return output, attention_weights\n\n# Usage\nmodel = MultiHeadAttention(d_model=512, n_heads=8)\nx = torch.randn(2, 10, 512)  # (batch, seq_len, d_model)\noutput, attention = model(x, x, x)\nprint(f\"Output shape: {output.shape}\")      # (2, 10, 512)\nprint(f\"Attention shape: {attention.shape}\") # (2, 8, 10, 10)\n```\n\n## 8.4 Positional Encoding\n\n### The Position Problem\n\n**Issue:** Attention is permutation-invariant\n```\nSequences: \"cat sat mat\" and \"mat cat sat\"\n→ Same attention outputs (ignoring word embeddings)\n→ No notion of word order!\n\nBut word order matters:\n- \"dog bit man\" ≠ \"man bit dog\" \n- \"not happy\" ≠ \"happy not\"\n```\n\n**Solution:** Add positional information to embeddings\n\n### Sinusoidal Positional Encoding\n\n**Formula:**\n```\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\nWhere:\n  pos = position in sequence (0, 1, 2, ...)\n  i = dimension index (0, 1, 2, ..., d_model/2)\n\nExample: Position 0, dimension 0\n  PE(0, 0) = sin(0) = 0\n\n  Position 1, dimension 0:\n  PE(1, 0) = sin(1 / 10000^0) = sin(1) ≈ 0.84\n\nProperties:\n  ① Different positions have different encodings\n  ② Patterns repeat at different frequencies\n  ③ Model can learn relative positions\n  ④ Extrapolates to longer sequences than seen in training\n```\n\n**Intuition:**\n```\nEach dimension oscillates at different frequency:\n- Dimension 0: Changes every position (high frequency)\n- Dimension 1: Changes every 2 positions  \n- Dimension 2: Changes every 4 positions\n- ...\n- Dimension d-1: Changes very slowly (low frequency)\n\nLike binary counting:\nPosition 0: [0, 0, 0, 0]\nPosition 1: [1, 0, 0, 0]  \nPosition 2: [0, 1, 0, 0]\nPosition 3: [1, 1, 0, 0]\nPosition 4: [0, 0, 1, 0]\n\nBut with smooth sinusoidal patterns instead of discrete bits\n```\n\n### Implementation\n\n```python\nimport torch\nimport math\n\ndef get_positional_encoding(seq_len, d_model):\n    \"\"\"Generate sinusoidal positional encodings\"\"\"\n    \n    # Create position indices: [0, 1, 2, ..., seq_len-1]\n    position = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)\n    \n    # Create dimension indices: [0, 2, 4, ..., d_model-2]\n    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * \n                        -(math.log(10000.0) / d_model))\n    \n    # Initialize positional encoding matrix\n    pe = torch.zeros(seq_len, d_model)\n    \n    # Apply sine to even dimensions\n    pe[:, 0::2] = torch.sin(position * div_term)\n    \n    # Apply cosine to odd dimensions  \n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    return pe\n\n# Example\nseq_len, d_model = 10, 512\npe = get_positional_encoding(seq_len, d_model)\nprint(f\"Positional encoding shape: {pe.shape}\")  # (10, 512)\n\n# Visualize first few dimensions\nprint(\"Position encoding for first 3 positions, first 8 dimensions:\")\nprint(pe[:3, :8])\n```\n\n**Adding to embeddings:**\n```python\nclass TransformerEmbedding(nn.Module):\n    def __init__(self, vocab_size, d_model, max_seq_len=1000):\n        super().__init__()\n        self.d_model = d_model\n        \n        # Word embeddings\n        self.word_embedding = nn.Embedding(vocab_size, d_model)\n        \n        # Positional encodings (fixed, not learned)\n        self.register_buffer('pos_encoding', \n                           get_positional_encoding(max_seq_len, d_model))\n        \n    def forward(self, x):\n        seq_len = x.size(1)\n        \n        # Word embeddings\n        word_emb = self.word_embedding(x) * math.sqrt(self.d_model)\n        \n        # Add positional encoding\n        pos_emb = self.pos_encoding[:seq_len, :]\n        \n        return word_emb + pos_emb\n```\n\n## 8.5 Complete Transformer Architecture\n\n### Transformer Encoder\n\n**Single encoder layer:**\n```\nInput: (batch, seq_len, d_model)\n    ↓\nMulti-Head Self-Attention\n    ↓  \nAdd & Norm (Residual connection + Layer normalization)\n    ↓\nPosition-wise Feed-Forward Network\n    ↓\nAdd & Norm (Residual connection + Layer normalization)  \n    ↓\nOutput: (batch, seq_len, d_model)\n```\n\n**Detailed components:**\n\n```\nMulti-Head Self-Attention:\n  input → Q, K, V (all same input)\n  Allows each position to attend to all positions\n  \nResidual connection:\n  output = attention_output + input\n  \n  Why?\n  ① Preserves original information\n  ② Enables deep networks (gradient flows directly)\n  ③ Output can learn \"residual\" (difference)\n\nLayer Normalization:\n  Normalize across feature dimension\n  \n  mean = mean(x along d_model dimension)\n  variance = var(x along d_model dimension)\n  normalized = (x - mean) / sqrt(variance + epsilon)\n  output = γ * normalized + β\n  \n  γ, β are learnable parameters\n\nPosition-wise FFN:\n  FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂\n  \n  Typical: d_model=512, d_ff=2048\n  Applied to each position independently\n  Same network for all positions\n```\n\n### Implementation\n\n```python\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        # Multi-head attention\n        self.self_attn = MultiHeadAttention(d_model, n_heads)\n        \n        # Position-wise feed-forward\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        )\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Multi-head attention with residual connection\n        attn_output, _ = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed-forward with residual connection\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        \n        return x\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, \n                 max_seq_len=1000, dropout=0.1):\n        super().__init__()\n        \n        # Embedding layer\n        self.embedding = TransformerEmbedding(vocab_size, d_model, max_seq_len)\n        \n        # Stack of encoder layers\n        self.layers = nn.ModuleList([\n            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Embedding + positional encoding\n        x = self.embedding(x)\n        x = self.dropout(x)\n        \n        # Pass through encoder layers\n        for layer in self.layers:\n            x = layer(x, mask)\n            \n        return x\n\n# Usage\nmodel = TransformerEncoder(\n    vocab_size=10000,\n    d_model=512,\n    n_heads=8,\n    d_ff=2048,\n    n_layers=6\n)\n\n# Example input\nbatch_size, seq_len = 2, 20\ninput_ids = torch.randint(0, 10000, (batch_size, seq_len))\n\noutput = model(input_ids)\nprint(f\"Output shape: {output.shape}\")  # (2, 20, 512)\n```\n\n## 8.6 Transformer for Multimodal Applications\n\n### Cross-Modal Attention\n\n**Key idea:** Attention between different modalities\n\n```\nStandard self-attention:\n  Q, K, V all from same modality\n  \nCross-modal attention:\n  Q from modality A (e.g., text)\n  K, V from modality B (e.g., image)\n  \nResult: Text tokens attend to image regions\n```\n\n**Example - Image Captioning:**\n```\nImage: Processed by CNN → image features (49 regions × 2048D)\nText: Previous words → text features (seq_len × 512D)\n\nCross attention:\n  Q = text features (what text is asking about)\n  K = image features (what image regions are available)  \n  V = image features (what image regions contain)\n  \nOutput: Text representation informed by relevant image regions\n```\n\n### Multimodal Transformer Architecture\n\n```python\nclass MultimodalTransformer(nn.Module):\n    def __init__(self, text_vocab_size, d_model, n_heads, n_layers):\n        super().__init__()\n        \n        # Text encoding\n        self.text_embedding = nn.Embedding(text_vocab_size, d_model)\n        \n        # Image encoding  \n        self.image_projection = nn.Linear(2048, d_model)  # ResNet → d_model\n        \n        # Positional encoding\n        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))\n        \n        # Cross-modal attention layers\n        self.cross_attention_layers = nn.ModuleList([\n            CrossModalAttentionLayer(d_model, n_heads)\n            for _ in range(n_layers)\n        ])\n        \n    def forward(self, text_tokens, image_features):\n        # Encode text\n        text_emb = self.text_embedding(text_tokens)  # (batch, text_len, d_model)\n        text_emb += self.pos_encoding[:text_tokens.size(1)]\n        \n        # Encode image  \n        img_emb = self.image_projection(image_features)  # (batch, img_regions, d_model)\n        \n        # Cross-modal processing\n        for layer in self.cross_attention_layers:\n            text_emb, img_emb = layer(text_emb, img_emb)\n            \n        return text_emb, img_emb\n\nclass CrossModalAttentionLayer(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        \n        # Text self-attention\n        self.text_self_attn = MultiHeadAttention(d_model, n_heads)\n        \n        # Image self-attention  \n        self.img_self_attn = MultiHeadAttention(d_model, n_heads)\n        \n        # Cross-attention: text attends to image\n        self.text_to_img_attn = MultiHeadAttention(d_model, n_heads)\n        \n        # Cross-attention: image attends to text\n        self.img_to_text_attn = MultiHeadAttention(d_model, n_heads)\n        \n        # Layer norms and FFNs\n        self.text_norm1 = nn.LayerNorm(d_model)\n        self.text_norm2 = nn.LayerNorm(d_model)\n        self.img_norm1 = nn.LayerNorm(d_model)\n        self.img_norm2 = nn.LayerNorm(d_model)\n        \n    def forward(self, text_emb, img_emb):\n        # Text self-attention\n        text_self, _ = self.text_self_attn(text_emb, text_emb, text_emb)\n        text_emb = self.text_norm1(text_emb + text_self)\n        \n        # Text-to-image cross-attention\n        text_cross, _ = self.text_to_img_attn(text_emb, img_emb, img_emb)\n        text_emb = self.text_norm2(text_emb + text_cross)\n        \n        # Similar for image...\n        \n        return text_emb, img_emb\n```\n\n## 8.7 Key Advantages and Limitations\n\n### Advantages\n\n```\n✓ Parallelizable: All positions processed simultaneously\n✓ Long-range dependencies: O(1) path length between any positions  \n✓ Interpretable: Attention weights show what model focuses on\n✓ Flexible: Easy to adapt for different modalities\n✓ Transfer learning: Pre-trained models work across tasks\n✓ Scalable: Performance improves with model size and data\n```\n\n### Limitations\n\n```\n✗ Quadratic complexity: O(n²) memory and computation for sequence length n\n✗ No inductive bias: Needs more data than CNNs/RNNs\n✗ Positional encoding: Limited extrapolation to longer sequences\n✗ Attention collapse: All tokens might attend to same positions\n✗ Computational cost: Large models require significant resources\n```\n\n### When to Use Transformers\n\n**Good for:**\n- Long sequences where parallelization matters\n- Tasks requiring long-range dependencies  \n- Multimodal problems\n- Transfer learning scenarios\n- Large-scale pre-training\n\n**Consider alternatives for:**\n- Very long sequences (>10k tokens) due to quadratic cost\n- Small datasets without pre-training\n- Real-time applications with strict latency requirements\n- Tasks where inductive bias helps (CNNs for images, RNNs for certain sequential patterns)\n\n## Key Takeaways\n\n- **Transformers solve fundamental RNN limitations** through parallelization and constant path length\n- **Self-attention enables direct connections** between all sequence positions  \n- **Multi-head attention captures multiple relationship types** simultaneously\n- **Positional encoding provides order information** to permutation-invariant attention\n- **Cross-modal attention enables multimodal understanding** by connecting different modalities\n- **Gradient flow is improved** compared to sequential architectures\n- **Computational cost scales quadratically** with sequence length\n\n## Further Reading\n\n**Original Papers:**\n- Vaswani, A., et al. (2017). Attention Is All You Need. *NIPS 2017*.\n- Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers. *arXiv:1810.04805*.\n\n**Multimodal Applications:**\n- Lu, J., et al. (2019). ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations. *NeurIPS 2019*.\n- Li, J., et al. (2022). BLIP: Bootstrapping Language-Image Pre-training. *ICML 2022*.\n\n**Mathematical Analysis:**\n- Rogers, A., et al. (2020). A Primer on Neural Network Models for Natural Language Processing. *Journal of AI Research*.\n\n---\n\n**Previous**: [Chapter 7: Contrastive Learning](chapter-07.md) | **Next**: [Chapter 9: Generative Models for Multimodal Data](chapter-09.md) | **Home**: [Table of Contents](index.md)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-08.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}