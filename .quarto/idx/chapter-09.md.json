{"title":"Chapter 9: Generative Models for Multimodal Data","markdown":{"headingText":"Chapter 9: Generative Models for Multimodal Data","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 8: Transformer Architecture](chapter-08.md) | **Next**: [Chapter 10: Seminal Models and Architectures](chapter-10.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand autoregressive generation fundamentals\n- Understand diffusion models and their mechanics\n- Implement text-conditional image generation\n- Compare different generative approaches\n- Apply generative models to multimodal tasks\n- Handle training challenges in generative models\n\n## 9.1 Autoregressive Generation\n\n### Core Concept\n\n**Definition:**\n```\nGenerate sequences one token at a time\nEach token probability conditioned on previous tokens\n\nP(x₁, x₂, ..., xₙ) = P(x₁) × P(x₂|x₁) × P(x₃|x₁,x₂) × ... × P(xₙ|x₁,...,xₙ₋₁)\n\nEach factor: one conditional probability to learn\nMultiply together: joint probability of sequence\n```\n\n**Why \"autoregressive\"?**\n```\nAuto = self\nRegressive = using past values to predict future\n\nLike autoregression in statistics:\n  y_t = α + β*y_{t-1} + error\n\nHere:\n  x_t ~ Distribution(previous tokens)\n  Each token generated using previous tokens\n```\n\n**Example - Text generation:**\n\n```\nTask: Generate sentence about cats\n\nStep 0: Start with [START] token\n\nStep 1: Predict first word\n  Input: [START]\n  Model outputs: P(word | [START])\n  Distribution: {the: 0.3, a: 0.2, ..., cat: 0.05}\n  Sample: \"The\" (or use greedy: highest probability)\n\nStep 2: Predict second word\n  Input: [START] The\n  Model outputs: P(word | [START], The)\n  Distribution: {cat: 0.4, dog: 0.1, ...}\n  Sample: \"cat\"\n\nStep 3: Predict third word\n  Input: [START] The cat\n  Model outputs: P(word | [START], The, cat)\n  Distribution: {is: 0.5, sat: 0.2, ...}\n  Sample: \"is\"\n\nContinue until [END] token or maximum length\n\nResult: \"The cat is sleeping peacefully on the couch\"\n```\n\n### Decoding Strategies\n\n**Strategy 1: Greedy Decoding**\n\n```\nAt each step, choose highest probability token\n\nAlgorithm:\n  for t in 1 to max_length:\n    logits = model(previous_tokens)\n    next_token = argmax(logits)\n    previous_tokens.append(next_token)\n\nAdvantages:\n  ✓ Fast (single forward pass per step)\n  ✓ Deterministic (same output every time)\n  ✓ Simple to implement\n\nDisadvantages:\n  ✗ Can get stuck in local optima\n  ✗ May produce suboptimal sequences\n  ✗ \"Does not\" → \"Does\" (highest prob) → \"not\" never chosen\n  ✗ No diversity (always same output)\n\nWhen to use:\n  - When consistency matters more than quality\n  - Real-time applications where speed critical\n  - Baseline comparisons\n```\n\n**Strategy 2: Beam Search**\n\n```\nKeep track of K best hypotheses\nExpand each by one token\nPrune to K best\n\nExample with K=3:\n\nStep 1:\n  Hypotheses: [\"The\", \"A\", \"One\"]\n  Scores: [0.3, 0.2, 0.15]\n\nStep 2 (expand each by one token):\n  From \"The\":\n    \"The cat\" (0.3 × 0.4 = 0.12)\n    \"The dog\" (0.3 × 0.1 = 0.03)\n    \"The bird\" (0.3 × 0.08 = 0.024)\n\n  From \"A\":\n    \"A cat\" (0.2 × 0.35 = 0.07)\n    \"A dog\" (0.2 × 0.15 = 0.03)\n    \"A bird\" (0.2 × 0.10 = 0.02)\n\n  From \"One\":\n    \"One cat\" (0.15 × 0.3 = 0.045)\n    ...\n\nStep 3 (keep top 3):\n  Best: \"The cat\" (0.12)\n  Second: \"The dog\" (0.03)\n  Third: \"A cat\" (0.07) or \"One cat\" (0.045)\n\nContinue...\n\nAlgorithm:\n  hypotheses = [[start_token]]\n  scores = [0]\n\n  for t in 1 to max_length:\n    candidates = []\n\n    for each hypothesis h in hypotheses:\n      logits = model(h)\n      for next_token in vocab:\n        score = scores[h] + log(logits[next_token])\n        candidates.append((h + [next_token], score))\n\n    # Keep best K\n    hypotheses, scores = topK(candidates, K)\n\n    # Stop if all ended\n    if all ended: break\n\n  return hypotheses[0]  # Best hypothesis\n\nAdvantages:\n  ✓ Better quality than greedy\n  ✓ Still relatively fast\n  ✓ Finds better global optimum\n\nDisadvantages:\n  ✗ Slower than greedy (K hypotheses tracked)\n  ✗ Still deterministic\n  ✗ No diversity\n\nWhen to use:\n  - Standard for machine translation\n  - When quality important but speed constrained\n  - Most common in practice\n```\n\n**Strategy 3: Sampling (Temperature-Based)**\n\n```\nInstead of greedy, sample from distribution\n\nAlgorithm:\n  for t in 1 to max_length:\n    logits = model(previous_tokens)\n    logits = logits / temperature\n    probabilities = softmax(logits)\n    next_token = sample(probabilities)\n    previous_tokens.append(next_token)\n\nTemperature effect:\n\ntemperature = 0.1 (cold - sharp):\n  Softmax becomes one-hot-like\n  Mostly sample highest probability\n  Like greedy but with small randomness\n  Output: Deterministic\n\ntemperature = 1.0 (normal):\n  Standard softmax\n  Sample according to distribution\n  Balanced randomness\n  Output: Somewhat random\n\ntemperature = 2.0 (hot - smooth):\n  Softmax becomes nearly uniform\n  All tokens equally likely\n  Very random generation\n  Output: Very random, often nonsensical\n\nExample:\n  Logits: [2.0, 1.0, 0.5]\n\n  Temperature 0.1:\n    After scaling: [20, 10, 5]\n    After softmax: [0.99, 0.01, 0.0]\n    Sample distribution: Mostly first token\n\n  Temperature 1.0:\n    After scaling: [2.0, 1.0, 0.5]\n    After softmax: [0.66, 0.24, 0.09]\n    Sample distribution: Balanced\n\n  Temperature 2.0:\n    After scaling: [1.0, 0.5, 0.25]\n    After softmax: [0.54, 0.30, 0.15]\n    Sample distribution: More uniform\n\nAdvantages:\n  ✓ Diverse outputs\n  ✓ Can be creative\n  ✓ Different each time\n\nDisadvantages:\n  ✗ Can produce nonsense\n  ✗ Quality depends on temperature tuning\n  ✗ Slower (need many samples to evaluate)\n\nWhen to use:\n  - Creative tasks (poetry, stories)\n  - When diversity valued\n  - User-facing applications (less repetitive)\n```\n\n**Strategy 4: Top-K Sampling**\n\n```\nOnly sample from K most probable tokens\n\nAlgorithm:\n  for t in 1 to max_length:\n    logits = model(previous_tokens)\n\n    # Get top K logits\n    topk_logits, topk_indices = topk(logits, K)\n\n    # Compute probabilities from only these K\n    probabilities = softmax(topk_logits)\n\n    # Sample from this restricted distribution\n    next_token_idx = sample(probabilities)\n    next_token = topk_indices[next_token_idx]\n\n    previous_tokens.append(next_token)\n\nExample with K=5:\n\nLogits: [5, 4, 3, 1, 0.5, 0.2, 0.1, ...]\nTop 5: [5, 4, 3, 1, 0.5]\nSoftmax of top 5: [0.4, 0.3, 0.2, 0.08, 0.02]\n\nSample from these 5 tokens only\nNever sample from tail tokens\n```\n\n**Strategy 5: Top-P (Nucleus) Sampling**\n\n```\nSample from smallest set of tokens with cumulative probability > p\n\nAlgorithm:\n  for t in 1 to max_length:\n    logits = model(previous_tokens)\n    probabilities = softmax(logits)\n\n    # Sort by probability descending\n    sorted_probs = sort(probabilities, descending=True)\n\n    # Find cutoff\n    cumsum = cumsum(sorted_probs)\n    cutoff_idx = first index where cumsum > p\n\n    # Keep tokens up to cutoff\n    mask = cumsum <= p\n\n    # Renormalize and sample\n    filtered_probs = probabilities * mask\n    filtered_probs = filtered_probs / sum(filtered_probs)\n\n    next_token = sample(filtered_probs)\n    previous_tokens.append(next_token)\n\nExample with p=0.9:\n\nProbabilities: [0.5, 0.3, 0.1, 0.05, 0.03, 0.02]\nCumsum: [0.5, 0.8, 0.9, 0.95, 0.98, 1.0]\n\nKeep tokens where cumsum <= 0.9:\n  [0.5, 0.3, 0.1] with cumsum [0.5, 0.8, 0.9]\n\nSample from these three tokens\nNever sample from last three (low probability)\n```\n\n### Training Autoregressive Models\n\n**Training objective:**\n\n```\nGoal: Maximize probability of correct sequence\n\nFor sequence [w₁, w₂, w₃, w₄]:\n\nLoss = -log P(w₁, w₂, w₃, w₄)\n     = -log [P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × P(w₄|w₁,w₂,w₃)]\n     = -[log P(w₁) + log P(w₂|w₁) + log P(w₃|w₁,w₂) + log P(w₄|w₁,w₂,w₃)]\n\nEach term: Cross-entropy loss for predicting next token\n\nTotal loss = Sum of cross-entropy losses for each position\n\nGradient flows to each position\nAll trained simultaneously (efficient!)\n```\n\n**Teacher forcing:**\n\n```\nDuring training:\n  Use true tokens for context (not predicted tokens)\n\nWithout teacher forcing:\n  Step 1: Predict w₂ from w₁ (could be wrong)\n  Step 2: Predict w₃ from (w₁, [predicted w₂]) (error accumulates)\n  Step 3: Predict w₄ from (w₁, [predicted w₂], [predicted w₃]) (more errors)\n\nResult: Model learns on error distribution\n        Model overfits to teacher forcing\n        At test time, predicted tokens are different!\n\nWith teacher forcing:\n  Step 1: Predict w₂ from w₁ (true)\n  Step 2: Predict w₃ from (w₁, w₂) (true)\n  Step 3: Predict w₄ from (w₁, w₂, w₃) (true)\n\nResult: Clean training signal\n        But distribution mismatch at test time!\n\nSolution: Scheduled sampling\n  Start with teacher forcing\n  Gradually use predicted tokens during training\n  Mix of training and test distribution\n```\n\n**Implementation:**\n\n```python\ndef train_autoregressive(model, sequences, optimizer, device):\n    \"\"\"Train autoregressive model with teacher forcing\"\"\"\n    model.train()\n    total_loss = 0\n\n    for sequence in sequences:\n        sequence = sequence.to(device)  # (seq_len,)\n\n        # Input: all but last token\n        input_ids = sequence[:-1]  # (seq_len-1,)\n\n        # Target: all but first token\n        target_ids = sequence[1:]  # (seq_len-1,)\n\n        # Forward pass\n        logits = model(input_ids)  # (seq_len-1, vocab_size)\n\n        # Compute loss\n        loss = F.cross_entropy(\n            logits.view(-1, vocab_size),\n            target_ids.view(-1)\n        )\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(sequences)\n```\n\n## 9.2 Diffusion Models\n\n### Core Idea\n\n**The diffusion process (forward):**\n\n```\nStart with clean image\nAdd noise gradually\nAfter many steps: Pure noise\n\nImage → slightly noisy → more noisy → ... → pure noise\n\nReverse process (learning):\nPure noise → slightly less noisy → ... → clean image\n\nIf we learn reverse process:\n  Can generate images from noise!\n  noise → network → slightly clean → network → ... → image\n```\n\n**Why this works:**\n\n```\nTraditional approach:\n  Learn complex distribution directly\n  High-dimensional, multi-modal distribution\n  Hard!\n\nDiffusion approach:\n  Learn simple steps: noise → slightly cleaner\n  Each step: Small denoising\n  Accumulate small steps: noise → image\n  Each step easier to learn!\n\nAnalogy:\n  Hard: Draw perfect portrait in one step\n  Easy: Start with sketch, refine step-by-step\n       Each refinement small improvement\n       Final result: Beautiful portrait\n```\n\n### Forward Process (Diffusion)\n\n**Markov chain:**\n\n```\nq(x_t | x_{t-1}) = N(x_t; √(1-β_t) x_{t-1}, β_t I)\n\nInterpretation:\n  Take previous x_{t-1}\n  Scale by √(1-β_t) (slightly shrink)\n  Add Gaussian noise with variance β_t\n  Result: x_t\n\nβ_t is variance schedule\n  Usually small: 0.0001 to 0.02\n  Controls how much noise added\n\n  Small β_t: Small change (smooth)\n  Large β_t: Big change (abrupt)\n```\n\n**Closed form solution:**\n\n```\nInstead of T sequential steps, compute directly:\n\nq(x_t | x_0) = N(x_t; √(ᾱ_t) x_0, (1-ᾱ_t) I)\n\nwhere ᾱ_t = ∏_{s=1}^t (1-β_s)\n\nBenefit:\n  Sample x_t directly from x_0 and noise\n  Don't need to compute all intermediate steps\n  Fast training!\n\nFormula:\n  x_t = √(ᾱ_t) x_0 + √(1-ᾱ_t) ε\n\n  where ε ~ N(0, I) is Gaussian noise\n\nProperties:\n  At t=0: ᾱ_0 = 1\n    x_0 = 1 * x_0 + 0 * ε = x_0 (clean image)\n\n  At t=T: ᾱ_T ≈ 0\n    x_T ≈ 0 * x_0 + 1 * ε = ε (pure noise)\n\n  Intermediate: ᾱ_t ∈ (0, 1)\n    Mix of original and noise\n```\n\n**Visualization:**\n\n```\nClean image ────→ Slight noise ────→ More noise ────→ Pure noise\n  x_0                x_100              x_500            x_1000\n\nᾱ_t = 1.0          ᾱ_t ≈ 0.9          ᾱ_t ≈ 0.3         ᾱ_t ≈ 0.001\n\n[Clear cat]  →  [Slightly fuzzy]  →  [Grainy]  →  [Random pixels]\n```\n\n### Reverse Process (Denoising)\n\n**Learning the reverse:**\n\n```\nForward: q(x_t | x_{t-1})  [given by math]\nReverse: p_θ(x_{t-1} | x_t)  [learn with network!]\n\nNetwork predicts:\n  Given noisy image x_t\n  Predict slightly less noisy image x_{t-1}\n\nTraining:\n  Use forward process to create noisy versions\n  Train network to denoise\n  Loss: How close is predicted to true x_{t-1}\n```\n\n**Equivalent formulation - Noise prediction:**\n\n```\nInstead of predicting x_{t-1}, predict noise:\n\nx_t = √(ᾱ_t) x_0 + √(1-ᾱ_t) ε\n\nRearrange:\n  ε = (x_t - √(ᾱ_t) x_0) / √(1-ᾱ_t)\n\nNetwork learns: ε_θ(x_t, t)\n  Given: x_t (noisy image) and t (timestep)\n  Predict: ε (noise that was added)\n\nThen:\n  x_{t-1} = (x_t - √(1-ᾱ_t) ε_θ(x_t, t)) / √(1-β_t)\n\nBenefit:\n  Network predicts smaller values (noise)\n  Easier to learn than predicting full image\n  More stable training\n```\n\n**Training loss:**\n\n```\nFor each training image x_0:\n  1. Sample random timestep t\n  2. Sample random noise ε ~ N(0, I)\n  3. Create noisy version: x_t = √(ᾱ_t) x_0 + √(1-ᾱ_t) ε\n  4. Predict noise: ε_pred = ε_θ(x_t, t)\n  5. Loss: ||ε_pred - ε||²\n\nIntuition:\n  Network learns to predict noise\n  For any timestep\n  For any noise level\n  From corresponding noisy image\n```\n\n### Sampling (Generation)\n\n**Iterative denoising:**\n\n```\nStart: x_T ~ N(0, I)  (pure random noise)\n\nFor t from T down to 1:\n  ε_pred = ε_θ(x_t, t)  (network predicts noise)\n\n  x_{t-1} = (x_t - √(1-ᾱ_t) ε_pred) / √(1-β_t)\n\n  Add small noise (for stochasticity):\n  x_{t-1} = x_{t-1} + √(β_t) z\n  where z ~ N(0, I)\n\nResult: x_0 is generated image\n```\n\n**Why this works:**\n\n```\nStep 1: x_1000 = pure noise\nStep 2: Apply denoising step → x_999 (slightly cleaner)\nStep 3: Apply denoising step → x_998 (more refined)\n...\nStep 1000: Apply denoising step → x_0 (clean image!)\n\nEach step removes some noise\n1000 small improvements → coherent image\n```\n\n**Scaling - How many steps?**\n\n```\nMore steps = better quality but slower\n\nT = 50:   Fast, okay quality\nT = 100:  Standard, good quality\nT = 1000: Very good quality, slow\n\nIn practice:\n  Train with T = 1000 (for learning)\n  Can sample with smaller T (faster, slightly worse)\n  DDIM: Sample in 50 steps instead of 1000\n```\n\n### Conditional Diffusion\n\n**Adding text conditioning:**\n\n```\nStandard diffusion:\n  ε_θ(x_t, t) predicts noise\n  Only input: noisy image, timestep\n  Output: unconditioned noise prediction\n\nText-conditioned:\n  ε_θ(x_t, t, c) predicts noise\n  Inputs: noisy image, timestep, text embedding c\n  Output: text-aware noise prediction\n\nTraining:\n  1. Sample image x_0 and text description c\n  2. Encode text: c = text_encoder(c)  (768D)\n  3. Sample timestep t and noise ε\n  4. Noisy image: x_t = √(ᾱ_t) x_0 + √(1-ᾱ_t) ε\n  5. Network prediction: ε_pred = ε_θ(x_t, t, c)\n  6. Loss: ||ε_pred - ε||²\n\nEffect:\n  Network learns text-image alignment\n  During denoising, follows text guidance\n  Generated image matches description\n```\n\n**Cross-attention for conditioning:**\n\n```\nNetwork architecture:\n\nInput x_t:\n  ├─ CNN layers (process noisy image)\n  │  └─ Feature maps\n  │       ├─ Self-attention (refine image understanding)\n  │       │\n  │       └─ Cross-attention to text\n  │           Query: image features\n  │           Key/Value: text embeddings\n  │           ↓ Result: Image attends to relevant text\n\nText embedding c:\n  └─ Project to key/value space\n```\n\n**Classifier-free guidance:**\n\n```\nProblem: Guidance strength vs diversity trade-off\n\nSolution: Predict both conditioned and unconditioned\n\nDuring training:\n  Some batches: Predict with text (conditioned)\n  Some batches: Predict without text (unconditioned)\n\n  Network learns both paths\n\nDuring sampling:\n  Compute both predictions:\n    ε_cond = ε_θ(x_t, t, c)      (with text)\n    ε_uncond = ε_θ(x_t, t, None) (without text)\n\n  Interpolate with guidance scale w:\n    ε_final = ε_uncond + w * (ε_cond - ε_uncond)\n\nInterpretation:\n  w=0: Ignore text, purely random\n  w=1: Normal, follow text\n  w=7: Strong guidance, adhere closely to text\n  w=15: Extreme guidance, saturated colors, distorted\n\nTrade-off:\n  w=1:  High diversity, moderate text adherence\n  w=7:  Good balance\n  w=15: Low diversity, extreme text adherence\n\nSweet spot: Usually w ∈ [7, 15]\n```\n\n### Stable Diffusion Architecture\n\n**Full pipeline:**\n\n```\nText prompt: \"A red cat on a chair\"\n    ↓\nText encoder (CLIP):\n  \"A red cat on a chair\" → 77×768 embeddings\n    ↓\nDiffusion model:\n  Input: noise (H×W×4) from VAE latent space\n         timestep t\n         text embeddings (77×768)\n\n  Processing:\n    ① ResNet blocks (noise processing)\n    ② Self-attention (within image)\n    ③ Cross-attention to text\n    ④ Repeat 12 times\n\n  Output: Predicted noise\n    ↓\nDenoising loop (1000 steps):\n  For each step:\n    ① Input current noisy latent\n    ② Network predicts noise\n    ③ Denoise: x_{t-1} = denoise(x_t, prediction)\n    ④ Next step\n    ↓\nLatent space representation of clean image\n    ↓\nVAE decoder:\n  4D latent → 512×512×3 RGB image\n    ↓\nImage: Red cat on chair!\n```\n\n**Why VAE compression?**\n\n```\nDiffusion on high-res images:\n  512×512×3 = 786,432 dimensions\n  Computationally infeasible!\n\nSolution: VAE compression\n  512×512×3 image → 64×64×4 latent\n  ~100× compression!\n\n  Latent captures semantic information\n  Pixels details discarded\n\nBenefit:\n  ① Faster computation\n  ② Diffusion on semantics, not pixels\n  ③ Better scaling\n```\n\n## 9.3 Text-Conditional Image Generation\n\n### Dataset Requirements\n\n**For training text-to-image models:**\n\n```\nBillions of image-caption pairs needed:\n\nLAION dataset: 5.8 billion pairs\n  Collected from web\n  Uncurated, noisy\n  Large diversity\n  ↓ Used for Stable Diffusion\n\nConceptual Captions: 3.3M pairs\n  More curated than LAION\n  Better quality\n  Smaller\n\nFor fine-tuning: 10K-100K pairs often sufficient\nFor training from scratch: Billions needed\n```\n\n**Data quality considerations:**\n\n```\nGood pairs:\n  Image of red car\n  Caption: \"A shiny red sports car\"\n\nBad pairs (but exist in web data):\n  Image of red car\n  Caption: \"Why cars are important\"\n  (Not descriptive of image)\n\nImpact:\n  Model learns incorrect alignments\n  Generates wrong things from descriptions\n\nSolution:\n  Filter low-quality pairs\n  Use robust training (contrastive pre-training helps)\n  Ensure at least 80% correct pairs\n```\n\n### Training Process\n\n**Step 1: Pre-training (Image-Text Alignment)**\n\n```\nBefore training diffusion, learn text-image alignment\n\nMethod: CLIP-style contrastive learning\n\nDataset: 400M+ image-caption pairs\nLoss: Make matched pairs similar in embedding space\n\nResult:\n  Text encoder learns to encode descriptions meaningfully\n  Image features align with text\n  Diffusion can then learn from well-aligned signal\n```\n\n**Step 2: Diffusion Model Training**\n\n```\nStart: Noisy latent z_t\nTimestep: t (1 to 1000)\nCondition: Text embedding c\n\nNetwork learns:\n  Given z_t and c, predict noise\n\nLoss function:\n  L = ||ε - ε_θ(z_t, t, c)||²\n\nTraining:\n  Batch size: 256-4096 (huge!)\n  Learning rate: 1e-4\n  Optimizer: Adam or AdamW\n  Duration: Days to weeks on large GPU clusters\n\n  Example:\n    4 clusters, 8 GPUs each\n    32 V100 GPUs total\n    Training for 2 weeks\n    Cost: ~$100K in compute\n```\n\n**Step 3: Fine-tuning (Optional)**\n\n```\nPre-trained model trained on billions of pairs\nGeneral knowledge of image generation\n\nFine-tune on specific domain:\n\nDomain: Medical imaging\n  1. Take pre-trained Stable Diffusion\n  2. Add new layers for medical images\n  3. Train on 10K medical image-description pairs\n  4. 1-2 days training on single GPU\n  5. Result: Medical image generation model\n\nOther domains:\n  - Anime art\n  - Product design\n  - Fashion\n  - Architecture\n```\n\n### Inference Tricks\n\n**Latent space optimization:**\n\n```\nInstead of denoising from random noise,\noptimize noise latent directly\n\nProcess:\n  1. Encode target image to latent z\n  2. Add timestep t noise: z_t = noise_t(z)\n  3. Denoise from z_t\n  4. Result: Image similar to target but modified per text\n\nUse case: Inpainting (fill in regions)\n```\n\n**Negative prompts:**\n\n```\nText prompt: \"A beautiful cat\"\nNegative prompt: \"ugly, blurry, deformed\"\n\nEffect:\n  Network learns what NOT to generate\n  Classifier-free guidance applied to both\n\n  ε_final = ε_uncond + w * (ε_cond - ε_uncond)\n            - w_neg * (ε_neg - ε_uncond)\n\nBenefit:\n  More control over generation\n  Avoid common artifacts\n```\n\n**Multi-step refinement:**\n\n```\nStep 1: Generate image with text\n  Prompt: \"A cat\"\n  Result: Generic cat\n\nStep 2: Inpaint to add details\n  Prompt: \"A red cat\"\n  Mask: Cat region\n  Result: Red cat\n\nStep 3: Upscale\n  Use super-resolution model\n  Result: High-res red cat\n\nBenefits:\n  ① Progressive refinement\n  ② More control\n  ③ Better results than single step\n```\n\n## 9.4 Practical Generative Systems\n\n### Building Text-to-Image System\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nclass TextToImageGenerator:\n    def __init__(self, model_name=\"stabilityai/stable-diffusion-2\"):\n        # Load pre-trained model\n        self.pipe = StableDiffusionPipeline.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16\n        )\n        self.pipe = self.pipe.to(\"cuda\")\n\n    def generate(self, prompt, num_images=1, guidance_scale=7.5,\n                 steps=50, seed=None):\n        \"\"\"\n        Generate images from text prompt\n\n        Args:\n            prompt: Text description\n            num_images: Number of images to generate\n            guidance_scale: How much to follow prompt (7.5 is default)\n            steps: Number of denoising steps (more = better quality but slower)\n            seed: Random seed for reproducibility\n\n        Returns:\n            images: List of PIL Images\n        \"\"\"\n        if seed is not None:\n            torch.manual_seed(seed)\n\n        # Generate\n        output = self.pipe(\n            prompt=prompt,\n            num_images_per_prompt=num_images,\n            guidance_scale=guidance_scale,\n            num_inference_steps=steps\n        )\n\n        return output.images\n\n    def generate_with_negative(self, prompt, negative_prompt=\"\",\n                               guidance_scale=7.5, steps=50):\n        \"\"\"Generate with negative prompt to avoid artifacts\"\"\"\n        output = self.pipe(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            guidance_scale=guidance_scale,\n            num_inference_steps=steps\n        )\n        return output.images\n\n    def inpaint(self, image, mask, prompt, guidance_scale=7.5, steps=50):\n        \"\"\"\n        Inpaint: modify specific regions of image\n\n        Args:\n            image: PIL Image to modify\n            mask: Binary mask (white = inpaint region)\n            prompt: Text description of what to generate\n\n        Returns:\n            Modified image\n        \"\"\"\n        from diffusers import StableDiffusionInpaintPipeline\n\n        inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-inpaint\",\n            torch_dtype=torch.float16\n        )\n        inpaint_pipe = inpaint_pipe.to(\"cuda\")\n\n        output = inpaint_pipe(\n            prompt=prompt,\n            image=image,\n            mask_image=mask,\n            guidance_scale=guidance_scale,\n            num_inference_steps=steps\n        )\n\n        return output.images[0]\n\n# Usage\ngenerator = TextToImageGenerator()\n\n# Simple generation\nimages = generator.generate(\"A beautiful sunset over mountains\")\n\n# With negative prompt to improve quality\nimages = generator.generate(\n    prompt=\"A realistic portrait of a woman\",\n    negative_prompt=\"ugly, blurry, deformed\",\n    guidance_scale=10.0,\n    steps=50\n)\n\n# Save\nimages[0].save(\"generated_image.png\")\n```\n\n### Building Image-Text Model for Understanding\n\n```python\nclass ImageCaptioningModel(nn.Module):\n    \"\"\"Generate captions from images\"\"\"\n\n    def __init__(self, image_encoder, text_decoder, embedding_dim=256):\n        super().__init__()\n        self.image_encoder = image_encoder\n        self.text_decoder = text_decoder\n        self.projection = nn.Linear(2048, embedding_dim)\n\n    def forward(self, images, text_ids=None):\n        \"\"\"\n        Args:\n            images: Batch of\n\n-----\n\n> continue\n\nimages\n            text_ids: Optional, for training\n\n        Returns:\n            logits or loss\n        \"\"\"\n        # Encode images\n        image_features = self.image_encoder(images)  # (batch, 2048)\n        image_embeddings = self.projection(image_features)  # (batch, 256)\n\n        if text_ids is None:\n            # Inference mode\n            return image_embeddings\n        else:\n            # Training mode\n            logits = self.text_decoder(\n                image_embeddings=image_embeddings,\n                input_ids=text_ids\n            )\n            return logits\n\n    def generate_caption(self, image, max_length=50, temperature=0.7):\n        \"\"\"Generate caption for image\"\"\"\n        self.eval()\n\n        with torch.no_grad():\n            # Encode image\n            image_features = self.image_encoder(image.unsqueeze(0))\n            image_embeddings = self.projection(image_features)\n\n            # Start with [CLS] token\n            caption_ids = [tokenizer.cls_token_id]\n\n            # Generate tokens\n            for _ in range(max_length):\n                # Predict next token\n                logits = self.text_decoder.predict_next(\n                    image_embeddings,\n                    torch.tensor([caption_ids]).to(device)\n                )\n                logits = logits[:, -1, :] / temperature\n\n                # Sample\n                probs = torch.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, 1)\n                caption_ids.append(next_token.item())\n\n                # Stop on [SEP] token\n                if next_token.item() == tokenizer.sep_token_id:\n                    break\n\n        # Decode to text\n        caption = tokenizer.decode(caption_ids)\n        return caption\n\ndef train_captioning_model(model, train_loader, optimizer, device, epochs=10):\n    \"\"\"Train image captioning model\"\"\"\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in train_loader:\n            images = batch['image'].to(device)\n            caption_ids = batch['caption_ids'].to(device)\n\n            # Forward pass\n            logits = model(images, caption_ids)\n\n            # Reshape for loss\n            logits = logits.view(-1, vocab_size)\n            targets = caption_ids[:, 1:].contiguous().view(-1)\n\n            # Compute loss\n            loss = criterion(logits, targets)\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n```\n\n### Handling Generation Failures\n\n**Problem 1: Mode collapse (generating same thing)**\n\n```\nSymptoms:\n  All outputs identical or very similar\n  Low diversity\n\nCauses:\n  Temperature too low\n  Batch size too small\n  Insufficient diversity in training data\n\nSolutions:\n  Increase temperature (0.7 → 0.9)\n  Use top-p sampling (not greedy)\n  Increase batch size\n  Data augmentation\n\nCode:\n  # Low temperature (bad)\n  next_token = argmax(logits)\n\n  # Better: Use temperature\n  logits = logits / temperature\n  probs = softmax(logits)\n  next_token = sample(probs)\n\n  # Best: Top-p sampling\n  probs = softmax(logits)\n  probs = top_p_filter(probs, p=0.9)\n  next_token = sample(probs)\n```\n\n**Problem 2: Generating nonsense**\n\n```\nSymptoms:\n  Output doesn't match prompt\n  Incoherent sequences\n  Missing objects from description\n\nCauses:\n  Insufficient text conditioning strength\n  Poor text encoder\n  Text alignment not learned well\n\nSolutions:\n  Increase guidance scale (7 → 10 or 15)\n  Pre-train text encoder more (CLIP)\n  Use stronger conditioning\n\nExample - Diffusion models:\n  # Weak guidance\n  guidance_scale = 1.0\n  Result: ~50% follow prompt\n\n  # Standard guidance\n  guidance_scale = 7.5\n  Result: ~80% follow prompt\n\n  # Strong guidance\n  guidance_scale = 15.0\n  Result: ~95% follow prompt, but less diversity\n```\n\n**Problem 3: Slow generation**\n\n```\nSymptoms:\n  Takes minutes per image\n  Not practical for deployment\n\nCauses:\n  Too many denoising steps (1000 default)\n  Inefficient implementation\n  No GPU acceleration\n\nSolutions:\n  Reduce inference steps (1000 → 50)\n  Use distilled model (faster but lower quality)\n  Use DDIM sampler (faster convergence)\n  Batch generation (process multiple at once)\n\nPerformance trade-off:\n\n  Steps    Quality    Time\n  ─────────────────────────\n   10      Poor       10ms\n   20      Okay       50ms\n   50      Good       200ms (Stable Diffusion standard)\n  100      Very good  400ms\n 1000      Best       4000ms (training standard)\n\nFor production: 50 steps usually sufficient\n```\n\n## 9.5 Comparing Generative Approaches\n\n**Autoregressive vs Diffusion:**\n\n```\n                  Autoregressive    Diffusion\n────────────────────────────────────────────────\nOutput quality    Good              Excellent\nTraining time     Moderate          Very long\nInference steps   100-1000          50-1000\nInference speed   Moderate          Slower\nDiversity         High              Moderate\nTraining simplicity Easier          Harder\n                  (language model)  (complex process)\n\nWhen to use:\n  Autoregressive: Text generation, fast inference needed\n  Diffusion: High-quality images, time not critical\n```\n\n**Generating text vs images:**\n\n```\nTEXT GENERATION (Autoregressive):\n  ✓ Fast inference (greedy decoding)\n  ✓ Easy to understand (token by token)\n  ✓ Works well with beam search\n  ✗ Can repeat or get stuck\n\nUse: Chatbots, summarization, translation\n\nIMAGE GENERATION (Diffusion):\n  ✓ High quality with text control\n  ✓ Flexible (can do inpainting, editing)\n  ✗ Slow (many denoising steps)\n\nUse: Art, design, content creation\n```\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-09.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}