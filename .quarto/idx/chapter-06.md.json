{"title":"Chapter 6: Attention Mechanisms in Multimodal Systems","markdown":{"headingText":"Chapter 6: Attention Mechanisms in Multimodal Systems","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 5: Fusion Strategies](chapter-05.md) | **Next**: [Chapter 7: Contrastive Learning](chapter-07.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand attention mechanism fundamentals and intuition\n- Implement scaled dot-product attention from scratch\n- Understand multi-head attention and its role\n- Apply cross-attention for multimodal fusion\n- Visualize and interpret attention patterns\n- Debug attention-based models\n- Optimize attention for efficiency\n\n## 6.1 Foundations of Attention\n\n### The Problem Attention Solves\n\n**Before attention (sequence-to-sequence models):**\n\n```\nTask: Translate English to French\n\nEnglish: \"The quick brown fox jumps\"\nFrench:  \"Le rapide renard brun saute\"\n\nRNN approach (encoder-decoder):\n\nEncoder:\n  Step 1: Process \"The\" → h₁\n  Step 2: Process \"quick\" → h₂\n  Step 3: Process \"brown\" → h₃\n  Step 4: Process \"fox\" → h₄\n  Step 5: Process \"jumps\" → h₅\n\n  Final state: h₅ (tries to contain all information!)\n\nDecoder:\n  Uses only h₅ to generate entire translation\n\n  Step 1: Generate \"Le\" from h₅\n  Step 2: Generate \"rapide\" from h₅\n  Step 3: Generate \"renard\" from h₅\n  Step 4: Generate \"brun\" from h₅\n  Step 5: Generate \"saute\" from h₅\n\nProblem:\n  ✗ All information bottlenecked into single vector h₅\n  ✗ Cannot remember which input word to focus on\n  ✗ Long sentences lose information\n  ✗ No obvious alignment between input and output\n```\n\n**With attention:**\n\n```\nEncoder (same):\n  Produces h₁, h₂, h₃, h₄, h₅\n\nDecoder with attention:\n  Step 1: Generate \"Le\"\n    Where to look? \"The\" → attention to h₁\n    Generate \"Le\" using context from h₁\n\n  Step 2: Generate \"rapide\"\n    Where to look? \"quick\" → attention to h₂\n    Generate \"rapide\" using context from h₂\n\n  Step 3: Generate \"renard\"\n    Where to look? \"brown\" or \"fox\" → attention to h₃ and h₄\n    Generate \"renard\" using blended context\n\n  Step 4: Generate \"brun\"\n    Where to look? \"brown\" → attention to h₃\n    Generate \"brun\" using context from h₃\n\n  Step 5: Generate \"saute\"\n    Where to look? \"jumps\" → attention to h₅\n    Generate \"saute\" using context from h₅\n\nBenefits:\n  ✓ Each output can look at relevant inputs\n  ✓ No information bottleneck\n  ✓ Explicit alignment learned\n  ✓ Works better on long sequences\n```\n\n### Attention Intuition\n\n**Analogy 1: Restaurant waiter**\n\n```\nScene: Busy restaurant with 10 tables\n\nWaiter's task: Serve Table 5\n\nProcess:\n  1. Look around (attention mechanism)\n  2. Pay attention to Table 5 specifically\n  3. Focus 90% on Table 5\n  4. Glance at nearby tables (10% split)\n  5. Retrieve correct order from Table 5\n  6. Serve Table 5\n\nAttention score for each table:\n  Table 1: 0.0  (far away)\n  Table 2: 0.02 (nearby but not relevant)\n  Table 3: 0.03\n  Table 4: 0.05\n  Table 5: 0.85 ← Focus here!\n  Table 6: 0.03\n  Table 7: 0.01\n  Table 8: 0.01\n  Table 9: 0.0\n  Table 10: 0.0\n\nResult: Service based on relevant information\n```\n\n**Analogy 2: Reading comprehension**\n\n```\nQuestion: \"What did the fox do?\"\n\nPassage: \"The quick brown fox jumped over the lazy dog\"\n\nHuman reading process:\n  1. Read question: \"What did the fox do?\"\n  2. Scan passage\n  3. Pay attention to parts mentioning \"fox\"\n    - \"brown fox\" ← relevant\n    - \"jumped over\" ← relevant\n  4. Ignore irrelevant parts\n    - \"quick\" ← less relevant\n    - \"lazy dog\" ← not about fox\n  5. Combine relevant information\n  6. Answer: \"jumped over the lazy dog\"\n\nAttention mechanism:\n  Query: \"fox\" (what are we asking about?)\n  Keys: [the, quick, brown, fox, jumped, over, the, lazy, dog]\n  Attention: Focus on \"fox\", \"jumped\", \"over\"\n  Values: Combine corresponding information\n  Result: Answer the question\n```\n\n### Why Attention is Powerful\n\n```\nKey insight: Solve \"what to look at\" problem\n\nBefore attention:\n  Model processes everything equally\n  Must compress all info into fixed vector\n  Gradient flow: Diluted through all positions\n\nWith attention:\n  Model focuses on relevant information\n  Can dynamically select what matters\n  Gradient flow: Strong to important positions\n  Learning: Faster and better\n```\n\n## 6.2 Scaled Dot-Product Attention (Complete)\n\n### Mathematical Deep Dive\n\n**Core formula:**\n\n```\nAttention(Q, K, V) = softmax(Q @ K^T / √d_k) @ V\n\nComponents:\n  Q (Query): (batch, seq_len, d_k)\n  K (Key):   (batch, seq_len, d_k)\n  V (Value): (batch, seq_len, d_v)\n\n  Output: (batch, seq_len, d_v)\n\nDimensions typically:\n  d_k = 64\n  d_v = 64\n  seq_len = 196 (for image patches) or 77 (for text tokens)\n```\n\n### Step-by-Step Computation\n\n**Complete example with real numbers:**\n\n```\nSetup:\n  Sequence: [\"cat\", \"sat\", \"mat\"]\n  Query dimension: 2 (for simplicity)\n\nQuery vectors:\n  Q = [\n    [1.0, 0.5],      # \"cat\"\n    [0.5, 1.0],      # \"sat\"\n    [0.3, 0.7]       # \"mat\"\n  ]\n\nKey vectors (same as queries in self-attention):\n  K = Q = [\n    [1.0, 0.5],\n    [0.5, 1.0],\n    [0.3, 0.7]\n  ]\n\nValue vectors:\n  V = [\n    [2, 1],          # \"cat\" value\n    [1, 2],          # \"sat\" value\n    [1.5, 1.5]       # \"mat\" value\n  ]\n\n─────────────────────────────────────────\n\nStep 1: Compute Q @ K^T (similarity)\n\nQ @ K^T:\n  Q[0] · K^T = [1.0, 0.5] @ [[1.0, 0.5, 0.3],\n                              [0.5, 1.0, 0.7]]\n             = [1.0*1.0 + 0.5*0.5,    1.0*0.5 + 0.5*1.0,   1.0*0.3 + 0.5*0.7]\n             = [1.0 + 0.25,           0.5 + 0.5,           0.3 + 0.35]\n             = [1.25,                 1.0,                 0.65]\n\n  Q[1] · K^T = [0.5, 1.0] @ ...\n             = [0.5*1.0 + 1.0*0.5,    0.5*0.5 + 1.0*1.0,   0.5*0.3 + 1.0*0.7]\n             = [0.5 + 0.5,            0.25 + 1.0,          0.15 + 0.7]\n             = [1.0,                  1.25,                0.85]\n\n  Q[2] · K^T = [0.3, 0.7] @ ...\n             = [0.3*1.0 + 0.7*0.5,    0.3*0.5 + 0.7*1.0,   0.3*0.3 + 0.7*0.7]\n             = [0.3 + 0.35,           0.15 + 0.7,          0.09 + 0.49]\n             = [0.65,                 0.85,                0.58]\n\nResult: Similarity matrix\n  [\n    [1.25, 1.0,  0.65],\n    [1.0,  1.25, 0.85],\n    [0.65, 0.85, 0.58]\n  ]\n\nInterpretation:\n  Position 0 most similar to: itself (1.25)\n  Position 1 most similar to: itself (1.25)\n  Position 2 most similar to: itself (0.58)\n\n─────────────────────────────────────────\n\nStep 2: Scale by 1/√d_k\n\nd_k = 2, so √d_k = √2 ≈ 1.414\n\nScaled:\n  [\n    [1.25/1.414,  1.0/1.414,  0.65/1.414],\n    [1.0/1.414,   1.25/1.414, 0.85/1.414],\n    [0.65/1.414,  0.85/1.414, 0.58/1.414]\n  ]\n= [\n    [0.884,  0.707, 0.460],\n    [0.707,  0.884, 0.601],\n    [0.460,  0.601, 0.410]\n  ]\n\nWhy scale?\n  Prevents dot product from getting too large\n  Keeps gradients reasonable\n  Stabilizes training\n\n─────────────────────────────────────────\n\nStep 3: Apply softmax\n\nFor position 0: [0.884, 0.707, 0.460]\n\nFirst compute exponentials:\n  e^0.884 ≈ 2.42\n  e^0.707 ≈ 2.03\n  e^0.460 ≈ 1.58\n  Sum = 6.03\n\nSoftmax:\n  [2.42/6.03,  2.03/6.03,  1.58/6.03]\n= [0.401,      0.337,      0.262]\n\nInterpretation:\n  \"cat\" attends 40% to itself\n  \"cat\" attends 34% to \"sat\"\n  \"cat\" attends 26% to \"mat\"\n\nFor position 1: [0.707, 0.884, 0.601]\n  e^0.707 ≈ 2.03\n  e^0.884 ≈ 2.42\n  e^0.601 ≈ 1.82\n  Sum = 6.27\n\n  Softmax: [0.324, 0.386, 0.290]\n\nFor position 2: [0.460, 0.601, 0.410]\n  e^0.460 ≈ 1.58\n  e^0.601 ≈ 1.82\n  e^0.410 ≈ 1.51\n  Sum = 4.91\n\n  Softmax: [0.322, 0.371, 0.307]\n\nAttention matrix (after softmax):\n  [\n    [0.401, 0.337, 0.262],\n    [0.324, 0.386, 0.290],\n    [0.322, 0.371, 0.307]\n  ]\n\nEach row sums to 1 ✓\n\n─────────────────────────────────────────\n\nStep 4: Apply to values\n\nFor position 0:\n  attention_output[0] = 0.401 * V[0] + 0.337 * V[1] + 0.262 * V[2]\n                      = 0.401 * [2, 1] + 0.337 * [1, 2] + 0.262 * [1.5, 1.5]\n                      = [0.802, 0.401] + [0.337, 0.674] + [0.393, 0.393]\n                      = [1.532, 1.468]\n\nFor position 1:\n  attention_output[1] = 0.324 * [2, 1] + 0.386 * [1, 2] + 0.290 * [1.5, 1.5]\n                      = [0.648, 0.324] + [0.386, 0.772] + [0.435, 0.435]\n                      = [1.469, 1.531]\n\nFor position 2:\n  attention_output[2] = 0.322 * [2, 1] + 0.371 * [1, 2] + 0.307 * [1.5, 1.5]\n                      = [0.644, 0.322] + [0.371, 0.742] + [0.461, 0.461]\n                      = [1.476, 1.525]\n\nFinal output:\n  [\n    [1.532, 1.468],\n    [1.469, 1.531],\n    [1.476, 1.525]\n  ]\n\nInterpretation:\n  Each position now contains weighted combination of all values\n  Weights determined by attention scores\n  Result: Context-aware representations\n```\n\n### Implementation from Scratch\n\n```python\nimport torch\nimport torch.nn.functional as F\nimport math\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Compute scaled dot-product attention\n\n    Args:\n        Q: Query tensor (batch, seq_len, d_k)\n        K: Key tensor (batch, seq_len, d_k)\n        V: Value tensor (batch, seq_len, d_v)\n        mask: Optional mask for positions to ignore\n\n    Returns:\n        output: Attention output (batch, seq_len, d_v)\n        attention_weights: Attention scores (batch, seq_len, seq_len)\n    \"\"\"\n\n    # Get dimension\n    d_k = Q.shape[-1]\n\n    # Step 1: Compute similarity scores\n    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)\n\n    # Step 2: Scale by √d_k\n    scores = scores / math.sqrt(d_k)\n\n    # Step 3: Apply mask (optional)\n    if mask is not None:\n        # Set masked positions to very negative number\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n\n    # Step 4: Apply softmax\n    attention_weights = torch.softmax(scores, dim=-1)\n\n    # Handle NaN from softmax(-inf)\n    attention_weights = torch.nan_to_num(attention_weights, 0.0)\n\n    # Step 5: Apply to values\n    output = torch.matmul(attention_weights, V)  # (batch, seq_len, d_v)\n\n    return output, attention_weights\n\n# Example usage\nbatch_size = 2\nseq_len = 3\nd_k = 2\nd_v = 2\n\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_v)\n\noutput, attention_weights = scaled_dot_product_attention(Q, K, V)\n\nprint(f\"Output shape: {output.shape}\")  # (2, 3, 2)\nprint(f\"Attention weights shape: {attention_weights.shape}\")  # (2, 3, 3)\nprint(f\"Attention weights row sum: {attention_weights.sum(dim=-1)}\")  # Should be all 1s\n```\n\n### Understanding Gradients\n\n**Backpropagation through attention:**\n\n```\nForward pass:\n  Q @ K^T → Scale → Softmax → @ V\n\nBackward pass:\n  dL/dV: Direct gradient from output\n  dL/dSoftmax: Chain from V gradient\n  dL/dScale: Chain from softmax gradient\n  dL/dScores: Chain from scale\n  dL/dK, dL/dQ: Chain from scores\n\nKey insight: Gradients flow through attention weights\n\nIf attention_weights[i,j] is high:\n  Position i receives strong gradient from j\n  Strong learning signal\n\nIf attention_weights[i,j] is low:\n  Position i receives weak gradient from j\n  Weak learning signal\n\nResult: Model learns to attend to relevant positions\n        through gradient flow\n```\n\n## 6.3 Multi-Head Attention\n\n### Why Multiple Heads?\n\n**Problem with single head:**\n\n```\nSingle attention head learns one type of relationship\n\nFor text \"The cat sat on the mat\":\n\nWhat if different relationships matter?\n  Syntactic: Articles attend to nouns\n  Semantic: Pronouns attend to antecedents\n  Discourse: Later sentences attend to earlier context\n\nSingle head must learn all simultaneously\nDifficult optimization problem\nLimited capacity\n\nSolution: Multiple heads\nEach head learns different relationships\nParallel processing\nCombine results\n```\n\n### Architecture\n\n**Multi-head formula:**\n\n```\nMultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\n\nwhere head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)\n\nh = number of heads (typically 8-16)\nW_i^Q, W_i^K, W_i^V = Projection matrices for head i\nW^O = Output projection\n```\n\n**Detailed breakdown:**\n\n```\nInput: (batch, seq_len, d_model)\n\nFor each head i = 1 to h:\n\n  1. Project to smaller dimension\n     Q_i = input @ W_i^Q     (batch, seq_len, d_k)\n     K_i = input @ W_i^K     (batch, seq_len, d_k)\n     V_i = input @ W_i^V     (batch, seq_len, d_v)\n\n     Typical: d_model = 512, h = 8\n              d_k = d_v = 512/8 = 64\n\n  2. Compute attention\n     head_i = Attention(Q_i, K_i, V_i)  (batch, seq_len, 64)\n\n  3. Repeat for all 8 heads\n     Result: 8 attention outputs\n             Each (batch, seq_len, 64)\n\nConcatenate all heads:\n  Combined = [head_1 || head_2 || ... || head_8]\n           (batch, seq_len, 512)\n\nOutput projection:\n  output = Combined @ W^O\n         (batch, seq_len, d_model)\n```\n\n### Implementation\n\n```python\nclass MultiHeadAttention(torch.nn.Module):\n    \"\"\"Multi-head attention layer\"\"\"\n\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Linear projections\n        self.W_q = torch.nn.Linear(d_model, d_model)\n        self.W_k = torch.nn.Linear(d_model, d_model)\n        self.W_v = torch.nn.Linear(d_model, d_model)\n        self.W_o = torch.nn.Linear(d_model, d_model)\n\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, Q, K, V, mask=None):\n        \"\"\"\n        Args:\n            Q: Query (batch, seq_len_q, d_model)\n            K: Key (batch, seq_len_k, d_model)\n            V: Value (batch, seq_len_v, d_model)\n            mask: Optional attention mask\n\n        Returns:\n            output: (batch, seq_len_q, d_model)\n        \"\"\"\n        batch_size = Q.shape[0]\n\n        # Step 1: Linear projections\n        Q = self.W_q(Q)  # (batch, seq_len_q, d_model)\n        K = self.W_k(K)  # (batch, seq_len_k, d_model)\n        V = self.W_v(V)  # (batch, seq_len_v, d_model)\n\n        # Step 2: Reshape for multi-head attention\n        # Split into h heads\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        # (batch, num_heads, seq_len_q, d_k)\n\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        # (batch, num_heads, seq_len_k, d_k)\n\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        # (batch, num_heads, seq_len_v, d_k)\n\n        # Step 3: Attention for each head\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        # (batch, num_heads, seq_len_q, seq_len_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        attention_weights = torch.softmax(scores, dim=-1)\n        attention_weights = torch.nan_to_num(attention_weights, 0.0)\n        attention_weights = self.dropout(attention_weights)\n\n        # Apply to values\n        output = torch.matmul(attention_weights, V)\n        # (batch, num_heads, seq_len_q, d_k)\n\n        # Step 4: Concatenate heads\n        output = output.transpose(1, 2).contiguous()\n        # (batch, seq_len_q, num_heads, d_k)\n\n        output = output.view(batch_size, -1, self.d_model)\n        # (batch, seq_len_q, d_model)\n\n        # Step 5: Output projection\n        output = self.W_o(output)\n\n        return output\n\n# Example\nmha = MultiHeadAttention(d_model=512, num_heads=8)\nQ = torch.randn(2, 10, 512)  # batch_size=2, seq_len=10, d_model=512\nK = torch.randn(2, 10, 512)\nV = torch.randn(2, 10, 512)\n\noutput = mha(Q, K, V)\nprint(f\"Output shape: {output.shape}\")  # (2, 10, 512)\n```\n\n### Head Specialization\n\n**What different heads learn:**\n\n```\nExample: Sentence \"The cat sat on the mat\"\n\nHead 1 (Syntactic):\n  Attention pattern:\n    \"The\" → \"cat\" (article to noun)\n    \"sat\" → \"cat\", \"on\", \"mat\" (verb to objects)\n  Learns: Grammatical relationships\n\nHead 2 (Semantic):\n  Attention pattern:\n    \"cat\" → \"mat\" (related nouns)\n    \"on\" → \"cat\", \"mat\" (location relation)\n  Learns: Semantic relationships\n\nHead 3 (Long-range):\n  Attention pattern:\n    \"mat\" → \"The\" (distant words)\n    \"sat\" → \"cat\" (key pairs)\n  Learns: Global context\n\nHead 4 (Rare/Noise):\n  Attention pattern:\n    \"on\" → \"on\", \"the\" (less obvious)\n    \"sat\" → \"sat\" (self-attention)\n  Learns: Residual patterns\n\nResult: Complementary representations\n        Ensemble of different perspectives\n```\n\n## 6.4 Cross-Attention for Multimodal Fusion\n\n### Concept and Setup\n\n**What is cross-attention?**\n\n```\nSelf-attention:\n  Q, K, V all from same source\n  Example: Text attends to text\n  \"Which words are relevant to which other words?\"\n\nCross-attention:\n  Q from one modality, K/V from another\n  Example: Text queries image features\n  \"Which image regions are relevant to this word?\"\n\nBenefits for multimodal:\n  ① Explicit alignment between modalities\n  ② Each modality can query the other\n  ③ Information flow controlled by queries\n```\n\n### Example: Image-to-Text Cross-Attention\n\n**Setup:**\n\n```\nImage: Visual features from CNN/ViT\n  Shape: (batch, num_patches, d_image)\n  Example: (2, 196, 2048) from ResNet50\n\nText: Token embeddings from BERT\n  Shape: (batch, seq_len, d_text)\n  Example: (2, 77, 768)\n\nGoal: Text should understand image context\n      Image should influence text processing\n```\n\n**Cross-attention computation:**\n\n```\nQuery: Text embeddings\n  Q = text_embeddings @ W_q\n  Shape: (batch, seq_len_text, d_k)\n\nKey/Value: Image features\n  K = image_features @ W_k\n  Shape: (batch, num_patches, d_k)\n\n  V = image_features @ W_v\n  Shape: (batch, num_patches, d_v)\n\nAttention:\n  scores = Q @ K^T / √d_k\n  Shape: (batch, seq_len_text, num_patches)\n\n  Interpretation:\n    For each word (seq_len_text)\n    How relevant is each image patch (num_patches)?\n\n    Word \"red\" attends to:\n      Red patches in image (high score)\n      Other patches (low score)\n\nWeighted sum:\n  output = softmax(scores) @ V\n  Shape: (batch, seq_len_text, d_v)\n\n  Each word now contains information about\n  relevant image regions\n```\n\n### Implementation\n\n```python\nclass CrossAttention(torch.nn.Module):\n    \"\"\"Cross-attention between two modalities\"\"\"\n\n    def __init__(self, d_q, d_k, d_v, num_heads=8):\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.d_k = d_k // num_heads\n        self.d_v = d_v // num_heads\n\n        # Query projection (from modality 1)\n        self.W_q = torch.nn.Linear(d_q, d_k)\n\n        # Key/Value projection (from modality 2)\n        self.W_k = torch.nn.Linear(d_k, d_k)\n        self.W_v = torch.nn.Linear(d_k, d_v)\n\n        # Output projection\n        self.W_o = torch.nn.Linear(d_v, d_v)\n\n    def forward(self, query_feats, key_value_feats, mask=None):\n        \"\"\"\n        Args:\n            query_feats: Queries from modality 1\n                        (batch, len_q, d_q)\n            key_value_feats: Keys/values from modality 2\n                            (batch, len_k, d_k)\n            mask: Optional mask\n\n        Returns:\n            output: (batch, len_q, d_v)\n        \"\"\"\n        batch_size = query_feats.shape[0]\n\n        # Project\n        Q = self.W_q(query_feats)  # (batch, len_q, d_k)\n        K = self.W_k(key_value_feats)  # (batch, len_k, d_k)\n        V = self.W_v(key_value_feats)  # (batch, len_k, d_v)\n\n        # Reshape for multi-head\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n\n        # Attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        weights = torch.softmax(scores, dim=-1)\n        output = torch.matmul(weights, V)\n\n        # Concatenate heads\n        output = output.transpose(1, 2).contiguous()\n        output = output.view(batch_size, -1, self.num_heads * self.d_v)\n        output = self.W_o(output)\n\n        return output\n\n# Example: Text attending to image\nclass ImageTextFusionLayer(torch.nn.Module):\n    def __init__(self, d_text=768, d_image=2048):\n        super().__init__()\n\n        self.text_to_image = CrossAttention(\n            d_q=d_text,\n            d_k=d_image,\n            d_v=d_image,\n            num_heads=8\n        )\n\n        self.image_to_text = CrossAttention(\n            d_q=d_image,\n            d_k=d_text,\n            d_v=d_text,\n            num_heads=8\n        )\n\n    def forward(self, text_feats, image_feats):\n        \"\"\"\n        Args:\n            text_feats: (batch, len_text, d_text)\n            image_feats: (batch, num_patches, d_image)\n\n        Returns:\n            text_out: Text enriched with image context\n            image_out: Image enriched with text context\n        \"\"\"\n        # Text queries image\n        text_out = self.text_to_image(text_feats, image_feats)\n\n        # Image queries text\n        image_out = self.image_to_text(image_feats, text_feats)\n\n        return text_out, image_out\n\n# Usage\nfusion_layer = ImageTextFusionLayer()\n\ntext_feats = torch.randn(2, 77, 768)  # Text features\nimage_feats = torch.randn(2, 196, 2048)  # Image patches\n\ntext_enhanced, image_enhanced = fusion_layer(text_feats, image_feats)\n\nprint(f\"Text enhanced shape: {text_enhanced.shape}\")  # (2, 77, 2048)\nprint(f\"Image enhanced shape: {image_enhanced.shape}\")  # (2, 196, 768)\n```\n\n### Bidirectional Fusion\n\n**Why both directions matter:**\n\n```\nText → Image only:\n  Text understands image\n  But image doesn't know what text is asking\n  One-way flow\n\nImage → Text only:\n  Image influences text\n  But text doesn't guide image processing\n  Unbalanced\n\nBoth directions (bidirectional):\n  Text and image mutually influence each other\n  Balanced information flow\n  Better alignment\n```\n\n**Architecture with bidirectional fusion:**\n\n```python\nclass BidirectionalFusion(torch.nn.Module):\n    \"\"\"Bidirectional attention between text and image\"\"\"\n\n    def __init__(self, d_text=768, d_image=2048, num_layers=6):\n        super().__init__()\n\n        self.num_layers = num_layers\n\n        # Projections to common space\n        self.text_project = torch.nn.Linear(d_text, 512)\n        self.image_\n\n-----\n\n> continue\n\n```python\n        self.image_project = torch.nn.Linear(d_image, 512)\n\n        # Layers of bidirectional attention\n        self.layers = torch.nn.ModuleList([\n            BidirectionalAttentionLayer(512, 512)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, text_feats, image_feats):\n        \"\"\"\n        Args:\n            text_feats: (batch, len_text, d_text)\n            image_feats: (batch, num_patches, d_image)\n\n        Returns:\n            text_out: (batch, len_text, 512)\n            image_out: (batch, num_patches, 512)\n        \"\"\"\n        # Project to common space\n        text = self.text_project(text_feats)  # (batch, len_text, 512)\n        image = self.image_project(image_feats)  # (batch, num_patches, 512)\n\n        # Apply bidirectional fusion layers\n        for layer in self.layers:\n            text_new, image_new = layer(text, image)\n\n            # Residual connections\n            text = text + text_new\n            image = image + image_new\n\n        return text, image\n\nclass BidirectionalAttentionLayer(torch.nn.Module):\n    \"\"\"Single layer of bidirectional attention\"\"\"\n\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n\n        # Cross-attention: text queries image\n        self.text_attn = torch.nn.MultiheadAttention(\n            d_model, num_heads=8, batch_first=True\n        )\n\n        # Cross-attention: image queries text\n        self.image_attn = torch.nn.MultiheadAttention(\n            d_model, num_heads=8, batch_first=True\n        )\n\n        # Feed-forward networks\n        self.text_ff = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n        self.image_ff = torch.nn.Sequential(\n            torch.nn.Linear(d_model, d_ff),\n            torch.nn.ReLU(),\n            torch.nn.Linear(d_ff, d_model)\n        )\n\n        # Layer normalization\n        self.text_norm1 = torch.nn.LayerNorm(d_model)\n        self.text_norm2 = torch.nn.LayerNorm(d_model)\n        self.image_norm1 = torch.nn.LayerNorm(d_model)\n        self.image_norm2 = torch.nn.LayerNorm(d_model)\n\n    def forward(self, text, image):\n        \"\"\"\n        Args:\n            text: (batch, len_text, d_model)\n            image: (batch, num_patches, d_model)\n\n        Returns:\n            text_out: (batch, len_text, d_model)\n            image_out: (batch, num_patches, d_model)\n        \"\"\"\n        # Text attends to image\n        text_norm = self.text_norm1(text)\n        text_attn_out, _ = self.text_attn(\n            text_norm,  # Query\n            image, image,  # Key, Value\n            need_weights=False\n        )\n        text = text + text_attn_out\n\n        # Text feed-forward\n        text_norm = self.text_norm2(text)\n        text = text + self.text_ff(text_norm)\n\n        # Image attends to text\n        image_norm = self.image_norm1(image)\n        image_attn_out, _ = self.image_attn(\n            image_norm,  # Query\n            text, text,  # Key, Value\n            need_weights=False\n        )\n        image = image + image_attn_out\n\n        # Image feed-forward\n        image_norm = self.image_norm2(image)\n        image = image + self.image_ff(image_norm)\n\n        return text, image\n\n# Usage\nfusion = BidirectionalFusion(d_text=768, d_image=2048, num_layers=6)\n\ntext_feats = torch.randn(2, 77, 768)\nimage_feats = torch.randn(2, 196, 2048)\n\ntext_out, image_out = fusion(text_feats, image_feats)\n\nprint(f\"Text output shape: {text_out.shape}\")  # (2, 77, 512)\nprint(f\"Image output shape: {image_out.shape}\")  # (2, 196, 512)\n```\n\n## 6.5 Attention Visualization and Interpretation\n\n### Visualizing Attention Weights\n\n**Text-to-text attention visualization:**\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_attention(attention_weights, tokens, layer_idx=0, head_idx=0):\n    \"\"\"\n    Visualize attention weights for a single layer and head\n\n    Args:\n        attention_weights: (num_layers, batch, num_heads, seq_len, seq_len)\n        tokens: List of token strings\n        layer_idx: Which layer to visualize\n        head_idx: Which head to visualize\n    \"\"\"\n    # Extract attention for specific layer and head\n    attn = attention_weights[layer_idx, 0, head_idx]  # (seq_len, seq_len)\n    attn = attn.detach().cpu().numpy()\n\n    # Create heatmap\n    fig, ax = plt.subplots(figsize=(10, 10))\n    im = ax.imshow(attn, cmap='viridis')\n\n    # Set labels\n    ax.set_xticks(range(len(tokens)))\n    ax.set_yticks(range(len(tokens)))\n    ax.set_xticklabels(tokens, rotation=45, ha='right')\n    ax.set_yticklabels(tokens)\n\n    # Add colorbar\n    cbar = plt.colorbar(im, ax=ax)\n    cbar.set_label('Attention weight')\n\n    ax.set_title(f'Attention weights (Layer {layer_idx}, Head {head_idx})')\n    ax.set_xlabel('Key (attended to)')\n    ax.set_ylabel('Query (attending from)')\n\n    plt.tight_layout()\n    return fig\n\n# Example usage\ntokens = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n# attention_weights would come from model\nfig = visualize_attention(attention_weights, tokens, layer_idx=0, head_idx=0)\nplt.show()\n```\n\n**Pattern interpretation:**\n\n```\nDifferent attention patterns reveal model behavior:\n\nPattern 1: Diagonal (self-attention)\n  ╱ (each token attends mostly to itself)\n  Interpretation: Position focuses on its own context\n  Meaning: Refines own representation\n\nPattern 2: Stripes (position-based)\n  ║ ║ ║ (same columns attended)\n  Interpretation: Multiple positions attend to same word\n  Meaning: Word is important reference point\n\nPattern 3: Distributed\n  ░ (uniform attention across sequence)\n  Interpretation: No clear focus\n  Meaning: Context comes from multiple sources\n\nPattern 4: Concentrated\n  ◾ (attention on few positions)\n  Interpretation: Clear focus\n  Meaning: Strong alignment to specific positions\n```\n\n### Cross-Modal Attention Visualization\n\n```python\ndef visualize_cross_attention(text_to_image_attn, text_tokens,\n                              image_patches, head_idx=0):\n    \"\"\"\n    Visualize what image regions text tokens attend to\n\n    Args:\n        text_to_image_attn: (seq_len_text, num_patches)\n        text_tokens: List of text tokens\n        image_patches: Could be image itself or placeholder\n        head_idx: Which head (if multi-head)\n    \"\"\"\n    attn = text_to_image_attn.detach().cpu().numpy()\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n\n    # For each text token, show what it attends to in image\n    for i, token in enumerate(text_tokens[:6]):\n        ax = axes[i]\n\n        # Get attention for this token\n        token_attn = attn[i]  # (num_patches,)\n\n        # Reshape to image grid (assuming 14x14 patches for 196 total)\n        grid_size = int(np.sqrt(len(token_attn)))\n        attn_grid = token_attn.reshape(grid_size, grid_size)\n\n        # Show as heatmap overlaid on image\n        im = ax.imshow(attn_grid, cmap='hot')\n        ax.set_title(f'Attention from \"{token}\"')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.colorbar(im, ax=ax)\n\n    plt.tight_layout()\n    return fig\n\n# Example usage\ntext_to_image = model.get_text_to_image_attention()\nfig = visualize_cross_attention(text_to_image[0, 0],\n                                text_tokens,\n                                image)\nplt.show()\n```\n\n## 6.6 Common Attention Patterns and Their Meanings\n\n### Pattern 1: Positional Attention\n\n**What it looks like:**\n```\nAttention matrix with clear bands:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   ▓▓░░░░░░░░░░░░░\npos1   ░▓▓░░░░░░░░░░░░\npos2   ░░▓▓░░░░░░░░░░░\npos3   ░░░▓▓░░░░░░░░░░\npos4   ░░░░▓▓░░░░░░░░░\n\n(Each position mainly attends to neighbors)\n```\n\n**Interpretation:**\n```\nModel learns local structure\nEffective for sequences with local dependencies\nExamples: Natural language, time series\n```\n\n**When it occurs:**\n```\nEarly layers of language models\nLocal relationships matter (syntax)\nLimited context needed\n```\n\n### Pattern 2: Hub Attention\n\n**What it looks like:**\n```\nOne column has high values:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   ░▓▓▓▓▓▓▓▓▓▓▓▓▓\npos1   ░▓▓▓▓▓▓▓▓▓▓▓▓▓\npos2   ░▓▓▓▓▓▓▓▓▓▓▓▓▓\npos3   ░▓▓▓▓▓▓▓▓▓▓▓▓▓\npos4   ░▓▓▓▓▓▓▓▓▓▓▓▓▓\n\n(All positions attend to pos1)\n```\n\n**Interpretation:**\n```\n\"Hub\" token is very important\nAll other tokens depend on it\nExamples: [CLS] token in BERT, verb in sentence\n```\n\n**When it occurs:**\n```\nLate layers (higher abstraction)\nGlobal information needed\nOne position summarizes all others\n```\n\n### Pattern 3: Diagonal + Off-Diagonal\n\n**What it looks like:**\n```\nSelf-attention plus other patterns:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   ▓▓░░░░░░░░▓░░░\npos1   ░▓▓░░░░░░░░▓░░\npos2   ░░▓▓░░░░░░░░▓░\npos3   ░░░▓▓░░░░░░░░▓\npos4   ░░░░▓▓░░░░░░░░\n\n(Diagonal + secondary pattern)\n```\n\n**Interpretation:**\n```\nSelf-attention + specific relationships\nExample: Each word attends to self + its subject\nComplex linguistic structure\n```\n\n### Pattern 4: Random/Noise\n\n**What it looks like:**\n```\nNo clear pattern:\n\n       pos0  pos1  pos2  pos3  pos4\npos0   ▓░▓░▓░▓░▓░▓░▓░\npos1   ░▓░▓░▓░▓░▓░▓░\npos2   ▓░▓░▓░▓░▓░▓░▓\npos3   ░▓░▓░▓░▓░▓░▓░\npos4   ▓░▓░▓░▓░▓░▓░▓\n\n(Uniform or random)\n```\n\n**Interpretation:**\n```\nHead not learning clear patterns\nCould indicate:\n  - Poor training\n  - Redundant head\n  - Learning different subspace\n```\n\n## 6.7 Debugging Attention Problems\n\n### Problem 1: Attention Collapse\n\n**Symptoms:**\n```\nAttention weights become nearly uniform\nExample: [0.25, 0.25, 0.25, 0.25] instead of [0.8, 0.1, 0.05, 0.05]\n\nEffects:\n  No clear focus\n  All positions equally weighted\n  Information not well integrated\n  Model performance poor\n```\n\n**Causes:**\n```\n① Temperature scaling issue\n   Softmax too smooth\n   All values similar\n\n② Poorly initialized queries/keys\n   Q and K nearly orthogonal\n   All dot products similar\n\n③ Gradients not flowing\n   Attention not updating during training\n```\n\n**Solutions:**\n```python\n# Debug: Check attention entropy\ndef check_attention_collapse(attention_weights):\n    \"\"\"\n    High entropy = collapse (uniform distribution)\n    Low entropy = focused attention\n    \"\"\"\n    # entropy = -sum(p * log(p))\n    entropy = -(attention_weights * torch.log(attention_weights + 1e-10)).sum(dim=-1)\n\n    print(f\"Attention entropy: {entropy.mean().item():.4f}\")\n    print(f\"Max entropy (uniform): {torch.log(torch.tensor(attention_weights.shape[-1])).item():.4f}\")\n\n    if entropy.mean() > 0.8 * max_entropy:\n        print(\"WARNING: Attention may be collapsing!\")\n        return True\n    return False\n\n# Fix: Increase temperature (smooth more)\n# Or fix: Reduce temperature (sharpen more)\n# Or fix: Check initialization\n```\n\n### Problem 2: Attention Not Converging\n\n**Symptoms:**\n```\nAttention weights don't change during training\nAlways [0.333, 0.333, 0.333] for 3 positions\n\nEffects:\n  Model can't learn what to focus on\n  No improvement over training\n```\n\n**Causes:**\n```\n① Learning rate too low\n   Gradients too tiny\n   No meaningful updates\n\n② Attention parameters frozen\n   Not being updated\n\n③ No gradient signal\n   Previous layers not helping\n```\n\n**Debugging code:**\n```python\ndef debug_attention_convergence(model, initial_weights, final_weights):\n    \"\"\"Check if attention changed\"\"\"\n\n    change = (final_weights - initial_weights).abs().mean()\n\n    print(f\"Attention weight change: {change.item():.6f}\")\n\n    if change < 1e-6:\n        print(\"WARNING: Attention not converging!\")\n\n        # Check gradients\n        for name, param in model.named_parameters():\n            if 'attention' in name:\n                if param.grad is not None:\n                    grad_norm = param.grad.norm()\n                    print(f\"  {name}: grad_norm = {grad_norm.item():.6f}\")\n                else:\n                    print(f\"  {name}: NO GRADIENT\")\n\n        return False\n    return True\n```\n\n### Problem 3: Misaligned Cross-Attention\n\n**Symptoms:**\n```\nCross-attention between modalities doesn't make sense\nExample: Word \"red\" attends to random image patches, not red regions\n\nEffects:\n  Poor multimodal alignment\n  Model can't understand relationship between modalities\n```\n\n**Debugging:**\n```python\ndef analyze_cross_attention_alignment(text_tokens, image_labels,\n                                     cross_attn_weights):\n    \"\"\"\n    Check if cross-attention makes semantic sense\n\n    Args:\n        text_tokens: ['red', 'cat', 'on', 'mat']\n        image_labels: ['red_region', 'cat_region', 'ground', 'background']\n        cross_attn_weights: (len_text, num_patches)\n    \"\"\"\n\n    for i, token in enumerate(text_tokens):\n        attn = cross_attn_weights[i]  # Attention for this token\n        top_indices = torch.topk(attn, k=3).indices  # Top 3 attended regions\n\n        attended_regions = [image_labels[idx] for idx in top_indices]\n\n        print(f\"Token '{token}' attends to: {attended_regions}\")\n\n        # Simple heuristic: check if token and attended regions match\n        if token in ' '.join(attended_regions).lower():\n            print(f\"  ✓ Makes sense!\")\n        else:\n            print(f\"  ✗ Misaligned!\")\n```\n\n## 6.8 Attention Efficiency Optimizations\n\n### Challenge: Quadratic Complexity\n\n**Problem:**\n```\nAttention complexity: O(n²) where n = sequence length\n\nExamples:\n  n = 100: 10,000 operations\n  n = 1000: 1,000,000 operations\n  n = 10,000: 100,000,000 operations\n\nFor images with 196 patches: Manageable\nFor long documents with 4096 tokens: Problematic\nFor videos with 1000+ frames: Very difficult\n```\n\n### Solution 1: Sparse Attention\n\n**Idea: Don't attend to all positions**\n\n```python\nclass SparseAttention(torch.nn.Module):\n    \"\"\"Attention with sparse connections\"\"\"\n\n    def __init__(self, window_size=32):\n        super().__init__()\n        self.window_size = window_size\n\n    def forward(self, Q, K, V):\n        \"\"\"\n        Only attend to nearby positions\n\n        Each position attends to:\n          - Itself\n          - window_size//2 positions before\n          - window_size//2 positions after\n        \"\"\"\n        seq_len = Q.shape[1]\n\n        # Create sparse mask\n        mask = torch.ones(seq_len, seq_len, device=Q.device)\n\n        for i in range(seq_len):\n            # Mask everything outside window\n            start = max(0, i - self.window_size // 2)\n            end = min(seq_len, i + self.window_size // 2)\n            mask[i, :start] = 0\n            mask[i, end:] = 0\n\n        # Standard attention with mask\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        attention_weights = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n\n        return output\n\n# Complexity: O(n * window_size) instead of O(n²)\n```\n\n### Solution 2: Linear Attention\n\n**Idea: Approximate softmax with kernel methods**\n\n```python\nclass LinearAttention(torch.nn.Module):\n    \"\"\"Linear complexity attention\"\"\"\n\n    def forward(self, Q, K, V):\n        \"\"\"\n        Standard attention:\n          Attention(Q,K,V) = softmax(QK^T) @ V\n          Complexity: O(n²)\n\n        Linear attention:\n          Approximate softmax with kernel\n          φ(QK^T) can be computed differently\n          Complexity: O(n)\n        \"\"\"\n\n        # Apply kernel function (e.g., elu + 1)\n        Q_proj = torch.nn.functional.elu(Q) + 1  # Ensure positivity\n        K_proj = torch.nn.functional.elu(K) + 1\n\n        # Rewrite attention:\n        # standard: softmax(QK^T) @ V\n        # linear: φ(Q) @ (φ(K)^T @ V) / (φ(Q) @ φ(K)^T @ 1)\n\n        numerator = torch.einsum('bne,bnd->bnd', K_proj, V)  # (batch, seq, d)\n        numerator = torch.einsum('bnd,bne->bnd', Q_proj, numerator)\n\n        denominator = torch.einsum('bne,bn->bne', Q_proj,\n                                   K_proj.sum(dim=1))  # (batch, seq, 1)\n        denominator = denominator + 1e-6  # Avoid division by zero\n\n        output = numerator / denominator\n\n        return output\n\n# Complexity: O(n * d²) where d is embedding dim\n# For n >> d: Linear in n\n```\n\n### Solution 3: Flash Attention\n\n**Idea: GPU-friendly attention computation**\n\n```\nStandard attention:\n  1. Compute QK^T: O(n²) memory\n  2. Apply softmax\n  3. Multiply by V\n\nFlash Attention:\n  1. Compute attention in blocks\n  2. Fuse operations (CUDA)\n  3. Reduce memory and computation\n\nResult:\n  2-4× faster\n  Less memory\n  Same result\n\nImplementation: Use existing libraries\n  torch.nn.functional.scaled_dot_product_attention  (PyTorch 2.0+)\n  flash-attn package\n```\n\n### Practical Optimization Example\n\n```python\n# Before: Standard attention\nattention = torch.nn.MultiheadAttention(d_model=512, num_heads=8)\n\n# Memory: O(batch * seq_len²)\n# Speed: Slower\n\n# After: Optimized attention\nclass OptimizedAttention(torch.nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n\n        # Option 1: Use Flash Attention (PyTorch 2.0+)\n        self.use_flash = True\n\n        # Option 2: Use sparse attention for long sequences\n        if seq_len > 1000:\n            self.attention = SparseAttention(window_size=64)\n        else:\n            self.attention = torch.nn.MultiheadAttention(d_model, num_heads)\n\n    def forward(self, Q, K, V):\n        if self.use_flash:\n            return torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        else:\n            return self.attention(Q, K, V)\n```\n\n## Key Takeaways\n\n- **Attention solves \"what to look at\" problem** efficiently\n- **Scaled dot-product is the foundation** - normalize by √d_k\n- **Multi-head attention learns diverse patterns** in parallel\n- **Cross-attention connects modalities** bidirectionally\n- **Visualization reveals model behavior** - debug with patterns\n- **Efficiency matters** - use sparse, linear, or flash attention for long sequences\n\n## Exercises\n\n**⭐ Beginner:**\n1. Implement scaled dot-product attention by hand\n2. Visualize attention weights from pre-trained model\n3. Understand what each attention head specializes in\n\n**⭐⭐ Intermediate:**\n4. Build cross-attention fusion layer\n5. Implement bidirectional attention\n6. Debug attention collapse in custom model\n\n**⭐⭐⭐ Advanced:**\n7. Implement sparse attention\n8. Optimize attention with flash mechanisms\n9. Analyze cross-modal alignment quality\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-06.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}