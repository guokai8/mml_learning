{"title":"Chapter 2: Foundations and Core Concepts","markdown":{"headingText":"Chapter 2: Foundations and Core Concepts","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 1: Introduction to Multimodal Learning](chapter-01.md) | **Next**: [Chapter 3: Feature Representation for Each Modality](chapter-03.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand mathematical foundations of embeddings\n- Calculate and interpret similarity metrics\n- Recognize good vs. poor embedding properties\n- Handle multimodal feature combinations correctly\n- Avoid common pitfalls in cross-modal comparison\n\n## 2.1 Mathematical Foundations\n\n### Vectors and Embedding Spaces\n\n**Definition:** An embedding is a representation of data as a vector in a high-dimensional space.\n\n**Mathematical notation:**\n```\nFor data point x, its embedding e is:\ne ∈ ℝ^d\n\nwhere:\nd = dimensionality of embedding space\nℝ^d = d-dimensional real number space\n\nExample:\nImage of cat → **e**_image = [0.2, -0.5, 0.8, ..., 0.1] ∈ ℝ^2048\nText \"cat\" → **e**_text = [0.1, 0.3, -0.2, ..., 0.5] ∈ ℝ^768\n```\n\n### Distance and Similarity Metrics\n\n#### Cosine Similarity (Recommended)\n\n**Definition:**\n```\nFor vectors **u** and **v**:\n\ncosine_similarity(**u**, **v**) = (**u** · **v**) / (||**u**|| × ||**v**||)\n\nwhere:\n**u** · **v** = dot product = u₁v₁ + u₂v₂ + ... + uₙvₙ\n||**u**|| = magnitude = √(u₁² + u₂² + ... + uₙ²)\n```\n\n**Range:** [-1, 1]\n- 1 = identical direction (most similar)\n- 0 = perpendicular (unrelated)\n- -1 = opposite direction (most dissimilar)\n\n**Geometric interpretation:**\n```\nCosine similarity measures the angle between vectors:\n- θ = 0° → cos(0°) = 1 (identical)\n- θ = 90° → cos(90°) = 0 (orthogonal)\n- θ = 180° → cos(180°) = -1 (opposite)\n```\n\n**Why preferred for embeddings:**\n- Invariant to magnitude (direction matters, not size)\n- Computationally efficient\n- Interpretable as angle between vectors\n- Works well in high-dimensional spaces\n\n**Example calculation:**\n\n```python\nimport numpy as np\n\n# Example vectors\nu = np.array([1, 2, 3])\nv = np.array([4, 5, 6])\n\n# Manual calculation\ndot_product = np.dot(u, v)  # 1*4 + 2*5 + 3*6 = 32\nnorm_u = np.linalg.norm(u)  # √(1² + 2² + 3²) = √14 ≈ 3.74\nnorm_v = np.linalg.norm(v)  # √(4² + 5² + 6²) = √77 ≈ 8.77\n\ncosine_sim = dot_product / (norm_u * norm_v)  # 32 / (3.74 × 8.77) ≈ 0.975\n\n# Using sklearn\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_sim = cosine_similarity([u], [v])[0][0]  # Same result\n```\n\n#### Euclidean Distance (L2 Distance)\n\n**Definition:**\n```\nFor vectors **u** and **v**:\n\neuclidean_distance(**u**, **v**) = ||**u** - **v**|| = √(Σᵢ(uᵢ - vᵢ)²)\n\nExample:\n**u** = [1, 2]\n**v** = [4, 6]\ndistance = √((1-4)² + (2-6)²) = √(9 + 16) = √25 = 5\n```\n\n**When to use:**\n- When absolute differences matter\n- In clustering algorithms\n- When coordinates have physical meaning\n\n**Drawback for high dimensions:**\n- \"Curse of dimensionality\" - distances become less meaningful\n- Cosine similarity preferred for embeddings\n\n#### Manhattan Distance (L1 Distance)\n\n**Definition:**\n```\nmanhattan_distance(**u**, **v**) = Σᵢ|uᵢ - vᵢ|\n\nExample:\n**u** = [1, 2]\n**v** = [4, 6]\ndistance = |1-4| + |2-6| = 3 + 4 = 7\n```\n\n**When to use:**\n- When features are independent\n- Robust to outliers\n- Some recommendation systems\n\n### What Makes a Good Embedding?\n\nA quality embedding space should satisfy several properties:\n\n1. **Meaningfulness**\n   - Similar inputs → similar vectors\n   - Related concepts → nearby in space\n\n2. **Efficiency**\n   - Reasonable dimensionality (not too large)\n   - Fast to compute\n   - Fast to compare\n\n3. **Stability**\n   - Small input changes → small embedding changes\n   - Noise in input shouldn't drastically change embedding\n\n4. **Interpretability** (optional but helpful)\n   - Can understand what dimensions represent\n   - Some dimensions → face detection, others → color, etc.\n\n5. **Transferability**\n   - Learned embeddings work across tasks\n   - Generalizes to new data\n\n### Dimensionality Considerations\n\n**Common embedding dimensions:**\n\n```\nTask                          Typical Dimension\n────────────────────────────────────────────\nWord embeddings (Word2Vec)    300\nSentence embeddings (BERT)    768\nImage features (ResNet)       2048\nAudio features (MFCC)         39\nHybrid multimodal            256-512\n```\n\n**Trade-offs:**\n\n```\nLow dimensions (64-256):\n  ✓ Fast computation  \n  ✓ Less memory\n  ✓ Less prone to overfitting\n  ✗ May lose important information\n  ✗ Limited expressiveness\n\nHigh dimensions (1024-4096):\n  ✓ Can capture fine details\n  ✓ Better expressiveness\n  ✗ Slow computation\n  ✗ High memory usage\n  ✗ Prone to overfitting\n  ✗ Curse of dimensionality (distances become uninformative)\n\nSweet spot: 256-2048D for most applications\n```\n\n## 2.3 Core Concepts Illustrated\n\n### Embedding Space Visualization\n\nWhile we can't visualize high-dimensional spaces, we can use dimensionality reduction:\n\n**t-SNE (t-Distributed Stochastic Neighbor Embedding):**\n\n```\n2048D embedding space\n        ↓ [t-SNE projection]\n2D visualization space\n\nPurpose: Preserve local neighborhoods\nGood for: Exploring clusters and relationships\nLimitation: Global structure may be distorted\n```\n\n**UMAP (Uniform Manifold Approximation and Projection):**\n\n```\nBenefits over t-SNE:\n- Preserves both local and global structure\n- Faster for large datasets\n- More theoretically grounded\n- Better preservation of distances\n```\n\n**PCA (Principal Component Analysis):**\n\n```\nBenefits:\n- Linear transformation (interpretable)\n- Preserves global structure\n- Fast computation\n\nLimitations:\n- May lose non-linear relationships\n- First components may not be most semantically meaningful\n```\n\n### Semantic Relationships in Vector Space\n\n**Key insight:** Semantic relationships become geometric relationships\n\n**Example - Word Embeddings:**\n\n```\nMathematical relationship:\n**king** - **man** + **woman** ≈ **queen**\n\nGeometric interpretation:\n- Vector from \"man\" to \"king\" captures \"royalty\" direction\n- Applying same direction to \"woman\" yields \"queen\"\n- Gender and status become orthogonal dimensions\n```\n\n**Verification in embedding space:**\n\n```python\n# Hypothetical vectors (simplified to 3D for illustration)\nking = np.array([0.8, 0.1, 0.9])    # High royalty, low gender, high status\nman = np.array([0.1, -0.8, 0.3])    # Low royalty, male, medium status  \nwoman = np.array([0.1, 0.8, 0.3])   # Low royalty, female, medium status\nqueen = np.array([0.8, 0.8, 0.9])   # High royalty, female, high status\n\n# Test the relationship\nresult = king - man + woman\nprint(f\"Result: {result}\")          # Should be close to queen\nprint(f\"Queen: {queen}\")\nprint(f\"Cosine similarity: {cosine_similarity([result], [queen])[0][0]:.3f}\")\n```\n\n## 2.4 Modality-Specific Representations\n\n### Text Representations\n\n**Challenge:** Convert discrete symbols (words) into continuous vectors\n\n```\nMETHOD 1 - One-hot encoding:\n  Vocabulary: [the, cat, sat, on, mat]\n  \"cat\" → [0, 1, 0, 0, 0]\n  Dimension = vocabulary size\n  Problem: Very high-dimensional, no semantic meaning\n\nMETHOD 2 - Word embeddings:\n  \"cat\" → [0.2, -0.5, 0.8, ..., 0.1]\n  Dimension = 300 (fixed)\n  Benefit: Captures semantic meaning\n```\n\n**Modern approaches:**\n\n```\nWord2Vec (2013):\n  - Learns from word co-occurrence\n  - 300D vectors typically\n  - Captures semantic relationships\n  - Limitation: Single vector per word (polysemy problem)\n\nBERT (2018):\n  - Contextual embeddings\n  - 768D vectors typically\n  - Different vector for same word in different contexts\n  - \"bank\" in \"river bank\" vs \"financial bank\"\n```\n\n### Image Representations\n\n**Challenge:** Convert pixel arrays into semantic features\n\n**Representation hierarchy:**\n\n```\nLEVEL 1 - Pixel level:\n  Image 224×224×3 → 150,528 values\n  Problem: Too high-dimensional, redundant\n\nLEVEL 2 - Low-level features:\n  Edges, corners, textures\n  Extracted by early CNN layers\n\nLEVEL 3 - Mid-level features:\n  Simple shapes, color regions\n  Extracted by middle CNN layers\n\nLEVEL 4 - High-level features:\n  Object parts, semantic concepts\n  Extracted by deep CNN layers\n\nLEVEL 5 - Global representation:\n  Final feature vector (e.g., 2048D)\n  Represents entire image content\n```\n\n**CNN Feature Extraction Process:**\n\n```\nInput: 224×224×3 image (150,528 values)\n        ↓\nConv layers: Extract hierarchical features\n        ↓\nGlobal Average Pool: Summarize spatial information\n        ↓\nOutput: 2048D feature vector **v**_image\n\nProperties:\n- Captures visual content at multiple scales\n- Translation and scale invariant (to some degree)  \n- Trained on millions of images\n- Transfer learning: Pre-trained features work for new tasks\n```\n\n**Key insight for multimodal:**\n```\nImages are not immediately interpretable\nThe 2048 dimensions don't correspond to human-understandable concepts\n(Except through attention visualization)\n\nThis makes image-text alignment challenging\nMust learn mappings between image features and text concepts\n```\n\n## 2.5 Cross-Modal Feature Combination\n\n### The Fundamental Problem\n\nWhen working with multiple modalities, we face the **representation gap**:\n\n```\nImage features: **v**_img ∈ ℝ^2048  (from ResNet)\nText features:  **v**_txt ∈ ℝ^768   (from BERT)\n\nProblems:\n1. Different dimensions (2048 ≠ 768)\n2. Different ranges and scales\n3. Different semantic spaces\n4. No natural correspondence between dimensions\n```\n\n### Critical Issue: The Wrong Way to Combine Features\n\n⚠️ **IMPORTANT WARNING** ⚠️\n\nA common **MISTAKE** is to use separate dimensionality reduction techniques:\n\n```python\n# ❌ WRONG APPROACH - DON'T DO THIS!\nfrom sklearn.decomposition import PCA\n\n# Separate PCA for each modality\npca_img = PCA(n_components=256)\npca_txt = PCA(n_components=256)\n\n# Project to same dimension BUT different coordinate systems\nimg_proj = pca_img.fit_transform(image_features)    # 256D in coordinate system A\ntxt_proj = pca_txt.fit_transform(text_features)     # 256D in coordinate system B\n\n# Compute similarity\nsimilarity = cosine_similarity(img_proj, txt_proj)  # ❌ MEANINGLESS!\n```\n\n**Why this is wrong:**\n\n```\nThe problem: Different coordinate systems!\n\nimg_proj[0] might represent \"redness\" in image space\ntxt_proj[0] might represent \"past_tense\" in text space\n\nThese dimensions are UNRELATED!\nComputing similarity between them gives random results\nThe similarity score will be close to 0 regardless of semantic content\n```\n\n**Analogy:**\n```\nThis is like:\n- Measuring image \"temperature\" in Fahrenheit  \n- Measuring text \"distance\" in meters\n- Then comparing 75°F with 100m\n\nThe numbers are unrelated even though both are valid measurements!\n```\n\n### The Correct Approach: Joint Embedding Space\n\n✅ **CORRECT APPROACH**\n\nLearn projections that map both modalities to a **shared semantic space**:\n\n```python\n# ✅ CORRECT: Joint training for shared space\nimport torch\nimport torch.nn as nn\n\nclass MultimodalEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Learned projections to SHARED space\n        self.img_projection = nn.Linear(2048, 256)  # Image → shared space\n        self.txt_projection = nn.Linear(768, 256)   # Text → shared space\n        \n    def forward(self, img_features, txt_features):\n        # Project to SAME coordinate system\n        img_embedded = self.img_projection(img_features)  # 256D in shared space\n        txt_embedded = self.txt_projection(txt_features)  # 256D in SAME shared space\n        \n        # Normalize for cosine similarity\n        img_embedded = nn.functional.normalize(img_embedded, dim=-1)\n        txt_embedded = nn.functional.normalize(txt_embedded, dim=-1)\n        \n        return img_embedded, txt_embedded\n\n# Training process ensures both modalities map to MEANINGFUL shared space\n# where similar content has similar representations\n```\n\n**Key insight:**\n```\nThe projection matrices are learned jointly through training on paired data:\n- Image of cat + text \"cat\" → should have high similarity\n- Image of cat + text \"dog\" → should have low similarity\n\nThe learning process ensures:\n- img_embedded[i] and txt_embedded[i] measure similar semantic properties\n- Similarity calculations are meaningful\n- Cross-modal retrieval actually works\n```\n\n### Proper Cross-Modal Similarity Calculation\n\nHere's the complete correct approach:\n\n```python\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel, AutoTokenizer\nfrom torchvision import models\nimport numpy as np\n\nclass CorrectMultimodalModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Pre-trained encoders\n        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n        self.image_encoder = models.resnet50(pretrained=True)\n        self.image_encoder.fc = nn.Identity()  # Remove final classifier\n        \n        # Learned projections to shared 256D space\n        self.text_projection = nn.Linear(768, 256)\n        self.image_projection = nn.Linear(2048, 256)\n        \n    def encode_text(self, text_inputs):\n        # Extract BERT features\n        with torch.no_grad():\n            outputs = self.text_encoder(**text_inputs)\n            text_features = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        \n        # Project to shared space\n        text_embedded = self.text_projection(text_features)\n        return nn.functional.normalize(text_embedded, dim=-1)\n    \n    def encode_image(self, image_inputs):\n        # Extract ResNet features\n        with torch.no_grad():\n            image_features = self.image_encoder(image_inputs)\n        \n        # Project to shared space  \n        image_embedded = self.image_projection(image_features)\n        return nn.functional.normalize(image_embedded, dim=-1)\n    \n    def compute_similarity(self, image_embedded, text_embedded):\n        # Now this is meaningful because both are in same coordinate system!\n        return torch.mm(image_embedded, text_embedded.t())\n\n# Usage example\nmodel = CorrectMultimodalModel()\n\n# For a cat image and \"cat\" text:\nimage_emb = model.encode_image(cat_image)      # Shape: (1, 256)\ntext_emb = model.encode_text(cat_text)         # Shape: (1, 256)\n\nsimilarity = model.compute_similarity(image_emb, text_emb)  # Should be HIGH\nprint(f\"Cat image vs cat text similarity: {similarity.item():.3f}\")\n\n# For a cat image and \"dog\" text:  \ndog_text_emb = model.encode_text(dog_text)\nsimilarity = model.compute_similarity(image_emb, dog_text_emb)  # Should be LOW\nprint(f\"Cat image vs dog text similarity: {similarity.item():.3f}\")\n```\n\n### Training the Joint Embedding\n\nThe key is **contrastive learning** with paired examples:\n\n```python\ndef contrastive_loss(image_emb, text_emb, temperature=0.1):\n    \"\"\"\n    image_emb: (batch_size, 256) - image embeddings\n    text_emb: (batch_size, 256) - text embeddings\n    Assumes corresponding indices are positive pairs\n    \"\"\"\n    \n    # Compute all pairwise similarities\n    logits = torch.mm(image_emb, text_emb.t()) / temperature\n    \n    # Labels: diagonal elements are positive pairs\n    labels = torch.arange(len(image_emb)).to(image_emb.device)\n    \n    # Cross-entropy loss\n    loss_i2t = nn.functional.cross_entropy(logits, labels)\n    loss_t2i = nn.functional.cross_entropy(logits.t(), labels)\n    \n    return (loss_i2t + loss_t2i) / 2\n\n# Training loop\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nfor batch in dataloader:\n    images, texts = batch\n    \n    # Encode both modalities\n    image_emb = model.encode_image(images)\n    text_emb = model.encode_text(texts)\n    \n    # Contrastive loss\n    loss = contrastive_loss(image_emb, text_emb)\n    \n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n## 2.6 Evaluation and Debugging\n\n### Checking Embedding Quality\n\n```python\ndef evaluate_embeddings(model, test_data):\n    \"\"\"Evaluate quality of learned embeddings\"\"\"\n    \n    similarities_pos = []  # Similarities for positive pairs\n    similarities_neg = []  # Similarities for negative pairs\n    \n    for image, pos_text, neg_text in test_data:\n        img_emb = model.encode_image(image)\n        pos_emb = model.encode_text(pos_text)\n        neg_emb = model.encode_text(neg_text)\n        \n        pos_sim = torch.mm(img_emb, pos_emb.t()).item()\n        neg_sim = torch.mm(img_emb, neg_emb.t()).item()\n        \n        similarities_pos.append(pos_sim)\n        similarities_neg.append(neg_sim)\n    \n    print(f\"Average positive similarity: {np.mean(similarities_pos):.3f}\")\n    print(f\"Average negative similarity: {np.mean(similarities_neg):.3f}\")\n    print(f\"Separation: {np.mean(similarities_pos) - np.mean(similarities_neg):.3f}\")\n    \n    # Good embeddings should have:\n    # - High positive similarities (> 0.3)\n    # - Low negative similarities (< 0.1)  \n    # - Large separation (> 0.2)\n```\n\n### Common Issues and Solutions\n\n**Issue 1: All similarities are near zero**\n```\nCause: Embeddings not properly normalized or poorly trained\nSolution: Check normalization, increase learning rate, train longer\n```\n\n**Issue 2: Positive and negative similarities are similar**\n```\nCause: Model hasn't learned to distinguish, need more training\nSolution: Better negative sampling, harder negatives, more data\n```\n\n**Issue 3: Very high similarities for unrelated content**\n```\nCause: Model collapsed to single representation\nSolution: Better regularization, temperature tuning, diverse training data\n```\n\n## Key Takeaways\n\n- **Embeddings** are vector representations that capture semantic meaning through geometry\n- **Cosine similarity** is the preferred metric for comparing embeddings\n- **Normalization** is essential when working with multimodal data\n- **Different modalities** have different properties requiring specialized handling\n- **❌ NEVER use separate PCA/dimensionality reduction** for different modalities\n- **✅ ALWAYS learn joint projections** to shared semantic space through training\n- **Feature extraction** is a pipeline from raw data to interpretable vectors\n- **Cross-modal alignment** requires careful design and joint training\n\n## Symbol and Variable Reference\n\nThroughout this chapter, we use the following notation:\n\n- **Bold lowercase** (**v**, **u**, **e**): Vectors\n- **Regular lowercase** (d, n, i): Scalars and indices  \n- **ℝ^d**: d-dimensional real vector space\n- **||v||**: Vector magnitude/norm\n- **v** · **u**: Dot product between vectors **v** and **u**\n- cos(θ): Cosine of angle θ between vectors\n- **e**_img: Image embedding vector\n- **e**_txt: Text embedding vector\n- **W**: Projection matrix (transforms one space to another)\n- τ (tau): Temperature parameter in contrastive learning\n\n## Further Reading\n\n**Mathematical Foundations:**\n- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapters 2-3.\n- Geron, A. (2019). *Hands-On Machine Learning*. O'Reilly. Chapter 8.\n\n**Embedding Techniques:**\n- Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space. *arXiv:1301.3781*.\n- Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers. *arXiv:1810.04805*.\n\n**Multimodal Alignment:**\n- Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. *ICML 2021*.\n\n---\n\n**Previous**: [Chapter 1: Introduction to Multimodal Learning](chapter-01.md) | **Next**: [Chapter 3: Feature Representation for Each Modality](chapter-03.md) | **Home**: [Table of Contents](index.md)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-02.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}