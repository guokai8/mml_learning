{"title":"Chapter 2: Foundations and Core Concepts","markdown":{"headingText":"Chapter 2: Foundations and Core Concepts","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 1: Introduction to Multimodal Learning](chapter-01.md) | **Next**: [Chapter 3: Feature Representation for Each Modality](chapter-03.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand the mathematical foundations of multimodal learning\n- Explain feature representation and embedding concepts\n- Describe similarity metrics used in multimodal systems\n- Understand modality normalization\n- Apply fundamental concepts to real problems\n\n## 2.1 Mathematical Foundations\n\n### Vectors and Embedding Spaces\n\n**Definition:** An embedding is a representation of data as a vector in a high-dimensional space.\n\n**Mathematical notation:**\n```\nFor data point x, its embedding e is:\ne ∈ ℝ^d\n\nwhere:\nd = dimensionality of embedding space\nℝ^d = d-dimensional real number space\n\nExample:\nImage of cat → e_image = [0.2, -0.5, 0.8, ..., 0.1] ∈ ℝ^2048\nText \"cat\" → e_text = [0.1, 0.3, -0.2, ..., 0.5] ∈ ℝ^768\n```\n\n**Why embeddings work:**\n\nEmbeddings capture semantic meaning through:\n1. **Distance** - Similar items have vectors close together\n2. **Direction** - Related concepts align directionally\n3. **Magnitude** - Can encode confidence or importance\n4. **Relationships** - Vector arithmetic can represent semantic operations\n\n**Example - Word2Vec:**\n\n```\nEmpirically discovered vector relationships:\n\nking - man + woman ≈ queen\n\nThis works because:\n- \"king\" and \"queen\" appear in similar contexts\n- \"man\" and \"woman\" capture gender transformation\n- Vector arithmetic preserves semantic relationships\n```\n\n### Similarity Metrics\n\n**Core concept:** We need ways to measure how similar two embeddings are.\n\n#### Cosine Similarity\n\n**Definition:**\n```\nsimilarity(u, v) = (u · v) / (||u|| × ||v||)\n\nwhere:\nu · v = dot product\n||u|| = L2 norm (magnitude)\n\nResult: Score in [-1, 1]\n```\n\n**Geometric intuition:**\n\n```\nangle between vectors = arc_cos(similarity)\n\nsimilarity = 1  → Same direction (identical)\nsimilarity = 0  → Perpendicular (unrelated)\nsimilarity = -1 → Opposite direction (contradictory)\n```\n\n**Why preferred for embeddings:**\n- Invariant to magnitude (direction matters, not size)\n- Computationally efficient\n- Interpretable as angle between vectors\n- Works well in high-dimensional spaces\n\n**Example calculation:**\n\n```python\nimport numpy as np\n\n# Vectors\nu = np.array([1, 0, 1, 0])\nv = np.array([1, 0, 1, 0])\n\n# Cosine similarity\ndot_product = np.dot(u, v)  # 1*1 + 0*0 + 1*1 + 0*0 = 2\nmagnitude_u = np.linalg.norm(u)  # sqrt(1+0+1+0) = 1.414\nmagnitude_v = np.linalg.norm(v)  # sqrt(1+0+1+0) = 1.414\n\nsimilarity = dot_product / (magnitude_u * magnitude_v)\n# = 2 / (1.414 * 1.414) = 1.0 (identical vectors)\n```\n\n#### Euclidean Distance\n\n**Definition:**\n```\ndistance(u, v) = ||u - v|| = sqrt(Σ(u_i - v_i)^2)\n\nResult: Score in [0, ∞)\n- 0 means identical\n- Larger means more different\n```\n\n**When to use:**\n- When absolute differences matter\n- In clustering algorithms\n- When coordinates have physical meaning\n\n**Drawback for high dimensions:**\n- \"Curse of dimensionality\" - distances become less meaningful\n- Cosine similarity preferred for embeddings\n\n#### Manhattan Distance (L1 Distance)\n\n**Definition:**\n```\ndistance(u, v) = Σ|u_i - v_i|\n\nResult: Sum of absolute differences\n```\n\n**Advantages:**\n- Computationally faster than Euclidean\n- Robust to outliers\n- Encourages sparsity\n\n#### Dot Product Similarity\n\n**Definition:**\n```\nsimilarity(u, v) = u · v = Σ(u_i × v_i)\n\nResult: Score in (-∞, ∞)\n- Unbounded, depends on magnitude\n```\n\n**When to use:**\n- In contrastive learning (with temperature scaling)\n- When magnitude carries meaning\n- With normalized vectors (then equivalent to cosine)\n\n### Normalization and Standardization\n\n**Why normalize?**\n\nDifferent modalities have different value ranges:\n\n```\nImage pixels: [0, 255] or [0, 1] after normalization\nText embeddings: [-1, 1] typically\nAudio: [-1, 1] normalized\n\nWithout normalization:\n- Similarity metrics behave unpredictably\n- Model training becomes unstable\n- Different modalities don't mix well\n```\n\n**Common normalization techniques:**\n\n**1. Min-Max Normalization (Scaling)**\n```\nx_normalized = (x - min(x)) / (max(x) - min(x))\n\nResult: All values in [0, 1]\nPreserves: Relationships and shape of distribution\n```\n\n**2. Z-Score Normalization (Standardization)**\n```\nx_normalized = (x - mean(x)) / std(x)\n\nResult: Mean = 0, Standard deviation = 1\nBenefit: Works well for values with Gaussian distribution\n```\n\n**3. L2 Normalization (Unit Vector)**\n```\nx_normalized = x / ||x||\n\nResult: Vector has magnitude 1\nProperty: Cosine similarity = dot product of L2-normalized vectors\n\nUsed in: CLIP, many embedding models\n```\n\n**Example - Normalizing image and text for comparison:**\n\n```\nRaw image features: [234, 1024, -500, ...]\nRaw text features: [0.023, -0.18, 0.51, ...]\n\nAfter L2 normalization:\nImage: [0.234, 0.298, -0.145, ...] (magnitude = 1)\nText: [0.0412, -0.321, 0.911, ...] (magnitude = 1)\n\nNow: Cosine similarity ≈ dot product\nBoth: Fair comparison despite different scales\n```\n\n## 2.2 Representing Data as Vectors\n\n### Information-to-Vector Mapping\n\n**Challenge:** Convert diverse data types to vectors\n\n```\nText: \"I love cats\"\n      ↓ (encoder)\n      [0.23, -0.51, 0.82, ..., 0.15] (768D)\n\nImage: [Cat photo]\n       ↓ (encoder)\n       [0.45, 0.12, -0.33, ..., 0.67] (2048D)\n\nAudio: [Cat meow sound]\n       ↓ (encoder)\n       [0.11, -0.09, 0.54, ..., -0.22] (768D)\n```\n\n### Desiderata (Desired Properties) for Embeddings\n\nA good embedding should have:\n\n1. **Meaningfulness**\n   - Similar inputs → similar vectors\n   - Related concepts → nearby in space\n\n2. **Efficiency**\n   - Reasonable dimensionality (not too large)\n   - Fast to compute\n   - Fast to compare\n\n3. **Stability**\n   - Small input changes → small embedding changes\n   - Noise in input shouldn't drastically change embedding\n\n4. **Interpretability** (optional but helpful)\n   - Can understand what dimensions represent\n   - Some dimensions → face detection, others → color, etc.\n\n5. **Transferability**\n   - Learned embeddings work across tasks\n   - Generalizes to new data\n\n### Dimensionality Considerations\n\n**Common embedding dimensions:**\n\n```\nTask                          Typical Dimension\n────────────────────────────────────────────\nWord embeddings (Word2Vec)    300\nSentence embeddings (BERT)    768\nContextual text (GPT)         1536-2048\nImage (ResNet50)              2048\nImage (Vision Transformer)    768\nAudio (Wav2Vec2)              768\nMultimodal shared space       256-512\n```\n\n**Dimensionality trade-off:**\n\n```\nToo small (e.g., 32D):\n  ✗ Information loss\n  ✗ Cannot capture complex relationships\n  ✓ Fast computation\n  ✓ Less memory\n  ✓ Less prone to overfitting\n\nToo large (e.g., 8192D):\n  ✓ Can capture fine details\n  ✓ Better expressiveness\n  ✗ Slow computation\n  ✗ High memory usage\n  ✗ Prone to overfitting\n  ✗ Curse of dimensionality (distances become uninformative)\n\nSweet spot: 256-2048D for most applications\n```\n\n## 2.3 Core Concepts Illustrated\n\n### Embedding Space Visualization\n\nWhile we can't visualize high-dimensional spaces, we can use dimensionality reduction:\n\n**t-SNE (t-Distributed Stochastic Neighbor Embedding):**\n\n```\n2048D embedding space\n        ↓ (project to 2D while preserving relationships)\n2D visualization\n\nExample - CLIP embeddings of common objects:\n\n         \"dog\"\n        /    \\\n      dog   cat -- \"cat\"\n        \\    /\n         \"cat image\"\n\nSimilar items cluster together\nDifferent items spread apart\n```\n\n### Semantic Relationships in Embedding Space\n\nEmbeddings capture semantic meaning:\n\n```\nVector relationships:\n\n┌─────────────────────────────────────────┐\n│ SEMANTIC RELATIONSHIPS IN EMBEDDING     │\n├─────────────────────────────────────────┤\n│                                         │\n│    queen                                │\n│      •                                  │\n│     /│                                  │\n│    / │ (king - man + woman)             │\n│   /  │                                  │\n│  •───•───•                              │\n│ king man woman                          │\n│                                         │\n│ Geometric interpretation:               │\n│ - Parallelogram property               │\n│ - Vector arithmetic = semantic ops      │\n│                                         │\n└─────────────────────────────────────────┘\n```\n\n**Real multimodal example - CLIP space:**\n\n```\nCLIP learns joint image-text space where:\n\nImage of cat ────────────► [embedding]\n    \"A picture of a cat\"──► [similar embedding]\n\nImage of dog ────────────► [distant embedding]\n    \"A picture of a dog\"──► [similar embedding]\n\nStructure:\n  - Cat images cluster with \"cat\" texts\n  - Dog images cluster with \"dog\" texts\n  - Cross-category items are far apart\n```\n\n## 2.4 Understanding Modality-Specific Properties\n\n### Text as Vectors\n\n**Properties:**\n- Discrete symbols (words, subwords)\n- Sequential structure (word order matters)\n- Compositional (words combine into sentences)\n- Abstract (can express concepts beyond physical)\n\n**Representation levels:**\n\n```\nLEVEL 1 - Character level:\n  \"cat\" → [c, a, t]\n  Problem: Loses semantic meaning\n\nLEVEL 2 - Word level:\n  \"The cat sat\" → [[the], [cat], [sat]]\n  Standard approach\n\nLEVEL 3 - Subword level (BPE):\n  \"unbelievable\" → [un, believable]\n  Handles rare words\n\nLEVEL 4 - Contextual:\n  \"The bank by the river\" → [context-aware embeddings]\n  Same word, different representation based on context\n```\n\n**Vector representation methods:**\n\n```\nMETHOD 1 - One-hot encoding:\n  Vocabulary: [the, cat, sat, on, mat]\n  \"cat\" → [0, 1, 0, 0, 0]\n  Dimension = vocabulary size\n  Problem: Very high-dimensional, no semantic meaning\n\nMETHOD 2 - Word embeddings:\n  \"cat\" → [0.2, -0.5, 0.8, ..., 0.1]\n  Dimension = 300 (fixed)\n  Benefit: Captures semantic meaning\n\nMETHOD 3 - Contextual embeddings:\n  \"The cat sat\" →\n  [context_embedding_1, context_embedding_2, context_embedding_3]\n  Dimension = 768 (fixed) per token\n  Benefit: Handles polysemy (multiple meanings)\n```\n\n**Key insight for multimodal:**\n```\nText is interpretable!\nTokens map to meaningful units\nWe can reason about which text parts matter\n\nThis differs from image/audio where interpretation is harder\n```\n\n### Images as Vectors\n\n**Properties:**\n- Continuous values (pixel intensities)\n- 2D spatial structure (nearby pixels correlated)\n- Hierarchical features (edges → shapes → objects)\n- Translational equivariance (object can be anywhere)\n\n**Representation hierarchy:**\n\n```\nLEVEL 1 - Pixel level:\n  Image 224×224×3 → 150,528 values\n  Problem: Too high-dimensional, redundant\n\nLEVEL 2 - Low-level features:\n  Edges, corners, textures\n  Extracted by early CNN layers\n\nLEVEL 3 - Mid-level features:\n  Shapes, parts, patterns\n  From middle CNN layers\n\nLEVEL 4 - High-level features:\n  Objects, scenes, semantic concepts\n  From final CNN layers\n\nLEVEL 5 - Global representation:\n  Single vector representing entire image\n  From average pooling or final layer\n```\n\n**CNN feature hierarchy visualization:**\n\n```\nInput image (224×224×3)\n        ↓\nLayer 1: Edges [112×112×64]\n        ↓\nLayer 2: Textures [56×56×128]\n        ↓\nLayer 3: Shapes [28×28×256]\n        ↓\nLayer 4: Parts [14×14×512]\n        ↓\nLayer 5: Objects [7×7×512]\n        ↓\nAverage Pool: [2048D vector]\n\nEach layer extracts higher-level patterns\n```\n\n**Key insight for multimodal:**\n```\nImages are not immediately interpretable\nThe 2048 dimensions don't correspond to human-understandable concepts\n(Except through attention visualization)\n\nThis makes image-text alignment challenging\nMust learn mappings between image features and text concepts\n```\n\n### Audio as Vectors\n\n**Properties:**\n- 1D signal over time\n- Varying frequency content\n- Temporal structure (order matters)\n- Perceptually motivated (frequency relevance to humans)\n\n**Feature extraction process:**\n\n```\nRaw waveform (16kHz, 1 second) → 16,000 samples\n            ↓\nSplit into frames (25ms each) → 40 frames\n            ↓\nExtract spectrogram → 40×513 (time × frequency)\n            ↓\nApply Mel-scale + log → 40×128 (more human-like)\n            ↓\nFinal MFCCs or spectrogram features\n```\n\n**Representation methods:**\n\n```\nMETHOD 1 - MFCC (Mel-Frequency Cepstral Coefficients):\n  Frame → Spectrum → Mel-scale → Cepstral → [39D per frame]\n  Mimics human hearing\n  Traditional approach\n\nMETHOD 2 - Spectrogram:\n  Frame → FFT → Power spectrum → [513D per frame]\n  All frequency information\n  Used in deep learning\n\nMETHOD 3 - Learned features (Wav2Vec):\n  Raw waveform → CNN → Quantized codes\n  → Transformer → [768D learned representation]\n  Modern approach\n  Learns task-relevant features\n```\n\n**Key insight for multimodal:**\n```\nAudio is temporal but can be converted to spectral view\nFrequency information is similar to visual features\n(Both are \"spectral\" representations)\n\nThis can facilitate audio-visual alignment\n(e.g., beat synchronization in music videos)\n```\n\n## 2.5 The Feature Extraction Pipeline\n\n### General Pipeline Structure\n\n```\nRaw Data\n    ↓\n[Preprocessing]\n  - Normalization\n  - Augmentation\n  - Format conversion\n    ↓\n[Feature Extraction]\n  - Shallow features (SIFT, MFCC)\n  - Or deep features (CNN, Transformer)\n    ↓\n[Post-processing]\n  - Normalization (L2)\n  - Dimensionality reduction\n  - Feature selection\n    ↓\n[Embedding Vector]\n  Ready for comparison or fusion\n```\n\n### Practical Considerations\n\n**1. Batch processing efficiency:**\n```\nDon't extract features one at a time\nProcess batches for GPU efficiency\n\nBatch size trade-offs:\n  Larger batch: Better GPU utilization\n  Smaller batch: Less memory, more iterations needed\n  Typical: 32-256 depending on data type\n```\n\n**2. Feature caching:**\n```\nFor large-scale retrieval systems:\n\nOnline phase (expensive):\n  Extract features once, cache them\n\nRetrieval phase (cheap):\n  Query by similarity to cached features\n  No need to re-extract\n\nExample:\n  E-commerce with 10M products\n  Extract features once (hours of computation)\n  Serve queries (milliseconds)\n```\n\n**3. Approximate similarity:**\n```\nExact nearest neighbor search is slow for large datasets\nUse approximate methods:\n\nHashing: Map similar embeddings to same hash bucket\nLSH: Locality-Sensitive Hashing\nFAISS: Facebook AI Similarity Search\nScaNN: Scalable Nearest Neighbors\n\nTrade-off: Accuracy vs speed\n```\n\n## 2.6 Practical Example: Building Feature Extractors\n\n### Image Feature Extractor\n\n```python\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\n\n# Load pre-trained ResNet50\nmodel = models.resnet50(pretrained=True)\n# Remove classification head\nmodel = torch.nn.Sequential(*list(model.children())[:-1])\nmodel.eval()\n\n# Image preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Extract features\ndef extract_image_features(image_path):\n    image = Image.open(image_path)\n    image = preprocess(image)\n    image = image.unsqueeze(0)  # Add batch dimension\n\n    with torch.no_grad():\n        features = model(image)\n\n    # Flatten and L2-normalize\n    features = features.flatten()\n    features = features / torch.norm(features)\n\n    return features.numpy()\n```\n\n### Text Feature Extractor\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Load BERT\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\nmodel.eval()\n\ndef extract_text_features(text):\n    # Tokenize\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n        padding=True\n    )\n\n    # Extract embeddings\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Use [CLS] token representation\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n\n    # L2-normalize\n    cls_embedding = cls_embedding / torch.norm(cls_embedding)\n\n    return cls_embedding.numpy()\n```\n\n### Computing Similarity\n\n```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Extract features\nimage_feat = extract_image_features(\"cat.jpg\")  # Shape: (2048,)\ntext_feat = extract_text_features(\"A cute cat\")  # Shape: (768,)\n\n# Problem: Different dimensions!\n# Solution: Project to shared space\n\n# Simple approach: L2 normalization and dimension reduction\nfrom sklearn.decomposition import PCA\n\n# Project both to 256D\npca_img = PCA(n_components=256)\npca_txt = PCA(n_components=256)\n\nimg_proj = pca_img.fit_transform(image_feat.reshape(1, -1))\ntxt_proj = pca_txt.fit_transform(text_feat.reshape(1, -1))\n\n# Compute similarity\nsimilarity = cosine_similarity(img_proj, txt_proj)\nprint(f\"Similarity: {similarity[0][0]:.3f}\")\n```\n\n## Key Takeaways\n\n- **Embeddings** are vector representations that capture semantic meaning through geometry\n- **Cosine similarity** is the preferred metric for comparing embeddings\n- **Normalization** is essential when working with multimodal data\n- **Different modalities** have different properties requiring specialized handling\n- **Feature extraction** is a pipeline from raw data to interpretable vectors\n- **Dimensionality** is a critical design choice balancing expressiveness and efficiency\n\n## Exercises\n\n**⭐ Beginner:**\n1. Calculate cosine similarity between three vectors by hand\n2. Explain why L2 normalization makes cosine similarity equal to dot product\n3. Describe the properties of text, image, and audio modalities\n\n**⭐⭐ Intermediate:**\n4. Implement a similarity search system for 1000 pre-extracted embeddings\n5. Compare cosine, Euclidean, and dot product similarity on sample data\n6. Visualize embeddings using t-SNE and interpret clusters\n\n**⭐⭐⭐ Advanced:**\n7. Design a hybrid normalization scheme for multimodal data\n8. Implement approximate nearest neighbor search with locality-sensitive hashing\n9. Analyze how normalization affects downstream multimodal fusion\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-02.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}