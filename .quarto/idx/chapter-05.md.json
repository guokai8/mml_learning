{"title":"Chapter 5: Fusion Strategies","markdown":{"headingText":"Chapter 5: Fusion Strategies","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 4: Feature Alignment and Bridging Modalities](chapter-04.md) | **Next**: [Chapter 6: Attention Mechanisms in Multimodal Systems](chapter-06.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand three levels of fusion (early, mid, late)\n- Implement each fusion strategy\n- Know when to use each approach\n- Combine multiple fusion methods\n- Handle missing modalities\n\n## 5.1 Three Fusion Architectures\n\n### Level 1: Early Fusion (Raw Data Level)\n\n**Concept:**\nCombine raw or minimally processed data before any feature extraction\n\n**Process:**\n\n```\nRaw modality 1: Image pixels (224×224×3 = 150,528 values)\nRaw modality 2: Text words (50 tokens × 300D = 15,000 values)\n                ↓\n         Concatenation\n                ↓\n    Combined vector (165,528D)\n                ↓\n         Joint model (CNN/Transformer)\n                ↓\n            Prediction\n```\n\n**Example architecture:**\n\n```python\nclass EarlyFusionModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        # Combined input: 165,528D\n        self.fc1 = nn.Linear(165528, 4096)\n        self.fc2 = nn.Linear(4096, 1024)\n        self.fc3 = nn.Linear(1024, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, image_pixels, text_embeddings):\n        # Flatten and concatenate\n        image_flat = image_pixels.reshape(image_pixels.shape[0], -1)\n        text_flat = text_embeddings.reshape(text_embeddings.shape[0], -1)\n        combined = torch.cat([image_flat, text_flat], dim=1)\n\n        # Process through network\n        x = self.relu(self.fc1(combined))\n        x = self.relu(self.fc2(x))\n        out = self.fc3(x)\n\n        return out\n```\n\n**Analysis:**\n\n**Advantages:**\n✓ Model can learn all interactions (nothing hidden)\n✓ Simple to understand\n✓ In theory, powerful (can learn anything)\n\n**Disadvantages:**\n✗ Extremely high dimensionality (165K features!)\n✗ Serious overfitting risk with limited data\n✗ Ignores modality-specific structure\n  - CNN architectures don't help\n  - Text structure not leveraged\n  - Image spatial correlations ignored\n✗ Model must learn modality-specific patterns from scratch\n✗ One noisy modality ruins everything\n✗ No transfer learning possible (no pre-trained models)\n\n**When to use:**\n- Tiny datasets (where all information essential)\n- Unlimited computational resources\n- Modalities are tightly coupled (rare)\n- As baseline only (not recommended for practice)\n\n**Real-world likelihood:**\n❌ Almost never used in practice\n❌ Only for academic comparisons\n\n---\n\n### Level 2: Mid-Fusion (Feature Level) - Most Common\n\n**Concept:**\nProcess each modality separately, then fuse features\n\n**Process:**\n\n```\nImage (224×224×3)\n    ├─ Image encoder (ResNet50)\n    └─ Image features (2048D)\n                ├─ Projection to shared space (256D)\n                │\nText (50 tokens)        │\n    ├─ Text encoder (BERT)     │\n    └─ Text features (768D)    │\n                ├─ Projection to shared space (256D)\n                │\n                └─ Fusion module\n                        ├─ Concatenation\n                        ├─ Addition\n                        ├─ Multiplication\n                        ├─ Bilinear\n                        └─ Attention\n                        ↓\n                    Fused features\n                        ↓\n                    Classifier\n                        ↓\n                    Prediction\n```\n\n**Multiple fusion options:**\n\n**Option 1: Simple concatenation**\n\n```\nimage_proj: [0.1, 0.2, ..., 0.5] (256D)\ntext_proj: [0.3, -0.1, ..., 0.8] (256D)\n\nConcatenated: [0.1, 0.2, ..., 0.5, 0.3, -0.1, ..., 0.8] (512D)\n\nCode:\n  fused = torch.cat([img_proj, txt_proj], dim=1)  # (batch, 512)\n```\n\n**Option 2: Element-wise addition**\n\n```\nBoth projected to same dimension (256D)\n\nimage_proj: [0.1, 0.2, 0.3, ...]\ntext_proj:  [0.3, -0.1, 0.2, ...]\n            ───────────────────\nSum:        [0.4, 0.1, 0.5, ...]  (256D)\n\nCode:\n  fused = img_proj + txt_proj\n\nInterpretation:\n  Dimensions with high values in both → amplified\n  Dimensions with opposite signs → cancel out\n  Result: Finds agreement between modalities\n```\n\n**Option 3: Element-wise multiplication (Hadamard product)**\n\n```\nimage_proj: [0.5, 0.2, 0.8, ...]\ntext_proj:  [0.9, 0.1, 0.3, ...]\n            ─────────────────────\nProduct:    [0.45, 0.02, 0.24, ...]  (256D)\n\nCode:\n  fused = img_proj * text_proj\n\nInterpretation:\n  Emphasizes dimensions where BOTH are large\n  Downplays where one is small\n  Creates AND-like interaction (both must agree)\n\nExample:\n  Image dimension \"red\": 0.9 (strong red feature)\n  Text dimension \"red\": 0.8 (word \"red\" present)\n  Product: 0.72 (strong agreement on red)\n\n  Image dimension \"square\": 0.1 (weak square feature)\n  Text dimension \"square\": 0.05 (weak word mention)\n  Product: 0.005 (both weak, product weaker)\n```\n\n**Option 4: Bilinear pooling**\n\n```\nCaptures pairwise interactions\n\nfused = img_proj^T @ W @ txt_proj\n\nwhere W ∈ ℝ^(256 × 256) is learnable matrix\n\nResult: Single scalar (interaction strength)\n\nCode:\n  W = nn.Parameter(torch.randn(256, 256))\n  interaction = torch.einsum('bi,ij,bj->b', img_proj, W, txt_proj)\n\nInterpretation:\n  All-pairs interaction between dimensions\n  More expressive than element-wise operations\n  But higher computational cost\n```\n\n**Option 5: Concatenation + attention**\n\n```\nKeep both representations separate\nUse attention to combine\n\nQuery: image features (256D)\nKey/Value: text features (256D)\n\nattention_weights = softmax(Query @ Key^T / sqrt(d))\ntext_attended = attention_weights @ Value\n\nCombined: [image_features, text_attended]  (512D)\n\nCode:\n  attention_scores = img_proj @ txt_proj.t()\n  attention_weights = softmax(attention_scores / sqrt(256))\n  txt_attended = attention_weights @ txt_proj\n  combined = torch.cat([img_proj, txt_attended], dim=1)\n```\n\n**Example - Sentiment analysis with mid-fusion:**\n\n```\nTask: Predict sentiment from image + text\n\nInput:\n  Image: [Happy face]\n  Text: \"I love this!\"\n\nProcessing:\n\n① Feature extraction\n   Image → ResNet50 → 2048D features\n   Text → BERT → 768D features\n\n② Dimensionality reduction (optional)\n   Image → Linear(2048→256) → 256D\n   Text → Linear(768→256) → 256D\n\n③ Fusion options:\n\n   Option A - Addition:\n     fused = img + text = [0.5, 0.3, ..., 0.2] (256D)\n     Interpretation: Aggregate all information\n\n   Option B - Multiplication:\n     fused = img * text = [0.45, 0.06, ..., 0.04] (256D)\n     Interpretation: Emphasize agreement\n\n   Option C - Concatenation:\n     fused = [img; text] = [512D]\n     Interpretation: Keep all information separate\n\n④ Classification\n   Linear layer: 256D/512D → 3 (pos/neutral/neg)\n\n⑤ Prediction\n   Output: Positive sentiment (0.92 confidence)\n```\n\n**Advantages of mid-fusion:**\n✓ Each modality processed with appropriate encoder\n✓ Transfer learning from pre-trained models\n✓ Reasonable dimensionality (512D vs 165K)\n✓ Flexible fusion options\n✓ Each modality can be fine-tuned independently\n✓ Good balance of modeling power and efficiency\n\n**Disadvantages:**\n✗ Some cross-modal interactions missed (due to independent encoding)\n✗ Requires projecting to common space\n✗ Hyperparameter choices (dimension, fusion method)\n\n**When to use:**\n✓ Most standard applications\n✓ When each modality has good encoder\n✓ Balanced importance across modalities\n✓ **Most recommended approach for practice**\n\n---\n\n### Level 3: Late Fusion (Decision Level)\n\n**Concept:**\nEach modality makes independent prediction, then combine decisions\n\n**Process:**\n\n```\nImage\n    ├─ Image encoder\n    ├─ Image classifier\n    └─ Image prediction: [0.7, 0.2, 0.1]  (3 class probs)\n                ├─ Combine predictions\n                ├─ Voting\nText            ├─ Averaging\n    ├─ Text encoder    ├─ Weighted sum\n    ├─ Text classifier ├─ Bayesian fusion\n    └─ Text prediction: [0.3, 0.5, 0.2]\n                    ↓\n                Final prediction\n                    ↓\n                Output class\n```\n\n**Multiple combination strategies:**\n\n**Strategy 1: Voting (Majority)**\n\n```\nImage prediction: Class 0 (highest prob 0.7)\nText prediction: Class 1 (highest prob 0.5)\n\nVote:\n  Class 0: 1 vote\n  Class 1: 1 vote\n\nResult: Tie!\nTiebreaker needed: Pick randomly or use confidence\n\nCode:\n  img_pred = torch.argmax(img_logits)\n  txt_pred = torch.argmax(txt_logits)\n\n  if img_pred == txt_pred:\n    final_pred = img_pred\n  else:\n    # Use highest confidence\n    img_conf = torch.max(img_logits)\n    txt_conf = torch.max(txt_logits)\n    final_pred = img_pred if img_conf > txt_conf else txt_pred\n```\n\n**Strategy 2: Averaging probabilities**\n\n```\nImage probs:     [0.7, 0.2, 0.1]\nText probs:      [0.3, 0.5, 0.2]\n                 ─────────────────\nAverage:         [0.5, 0.35, 0.15]\n\nFinal prediction: Class 0 (0.5 probability)\n\nCode:\n  avg_probs = (img_probs + txt_probs) / 2\n  final_pred = torch.argmax(avg_probs)\n```\n\n**Strategy 3: Weighted averaging**\n\n```\nWeight image more (assume it's more reliable):\n  w_img = 0.7\n  w_txt = 0.3\n\nWeighted combination:\n  [0.7*0.7 + 0.3*0.3, 0.7*0.2 + 0.3*0.5, 0.7*0.1 + 0.3*0.2]\n= [0.49+0.09, 0.14+0.15, 0.07+0.06]\n= [0.58, 0.29, 0.13]\n\nFinal prediction: Class 0\n\nCode:\n  weighted_probs = w_img * img_probs + w_txt * txt_probs\n  final_pred = torch.argmax(weighted_probs)\n```\n\n**Strategy 4: Product of probabilities (Bayesian)**\n\n```\nIdea: Multiply probabilities across modalities\nAssumption: Modalities independent given true class\n\nImage probs:     [0.7, 0.2, 0.1]\nText probs:      [0.3, 0.5, 0.2]\n                 ──────────────────\nProduct:         [0.21, 0.10, 0.02]\nNormalized:      [0.66, 0.31, 0.03]\n\nFinal: Class 0\n\nCode:\n  combined = img_probs * txt_probs\n  combined = combined / torch.sum(combined)  # Normalize\n  final_pred = torch.argmax(combined)\n```\n\n**Strategy 5: Maximum (Optimistic)**\n\n```\nTake maximum probability for each class\n\nClass 0: max(0.7, 0.3) = 0.7\nClass 1: max(0.2, 0.5) = 0.5\nClass 2: max(0.1, 0.2) = 0.2\n\nResult: [0.7, 0.5, 0.2]\nBut: Doesn't sum to 1! (Renormalize: [0.538, 0.385, 0.154])\n\nCode:\n  combined = torch.max(img_probs, txt_probs)\n  combined = combined / torch.sum(combined)\n```\n\n**Example - Medical diagnosis:**\n\n```\nPatient data: CT scan + blood tests + symptoms\n\nModel 1 (Image-based):\n  Analyzes CT scan\n  Prediction: \"Likely cancer\" (0.85)\n                \"Uncertain\" (0.12)\n                \"Unlikely\" (0.03)\n\nModel 2 (Lab-based):\n  Analyzes blood markers\n  Prediction: \"Likely cancer\" (0.62)\n                \"Uncertain\" (0.25)\n                \"Unlikely\" (0.13)\n\nCombination strategies:\n\n① Averaging:\n   Result: [0.735, 0.185, 0.08]\n   → \"Likely cancer\" (73.5%)\n\n② Weighted (trust image more):\n   0.7 × Image + 0.3 × Lab\n   → \"Likely cancer\" (76.9%)\n\n③ Product (Bayesian):\n   0.85*0.62 / Z = [0.781, 0.069, 0.15]\n   → \"Likely cancer\" (78.1%)\n\nDifferent strategies give similar but slightly different results\n```\n\n**Advantages of late fusion:**\n✓ Highest modularity\n✓ Each modality completely independent\n✓ Easy to add/remove modalities\n✓ Handles missing modalities gracefully (just skip that classifier)\n✓ Easy to debug (know which modality failed)\n✓ Can use completely different model types per modality\n\n**Disadvantages:**\n✗ Lost fine-grained cross-modal interactions\n✗ Each modality must predict well independently\n✗ Weaker modality can't be helped by stronger one\n✗ Higher computational cost (multiple full pipelines)\n✗ Information not shared during training\n\n**When to use:**\n✓ Modalities strongly independent\n✓ Need robustness to missing modalities\n✓ Interpretability important\n✓ Modalities have very different characteristics\n✓ Each modality has mature specialized model\n\n**Real-world example - Autonomous driving:**\n\n```\nSensor fusion in self-driving car:\n\nCamera → Object detection → Predictions\nLIDAR → Distance measurement → Predictions\nRadar → Velocity detection → Predictions\n\nLate fusion:\n  Each sensor makes independent decision\n  \"Object at x=100m, y=50m\"\n\n  Decisions combined:\n  Average positions across sensors\n  Confidence = agreement between sensors\n\nResult: Robust detection even if one sensor fails\n```\n\n---\n\n## 5.2 Comparison of Fusion Levels\n\n**Summary table:**\n\n```\n                Early          Mid            Late\n              Fusion         Fusion         Fusion\n─────────────────────────────────────────────────\nInput         Raw data       Features       Predictions\n              (pixels)       (2048D, 768D)  (probabilities)\n\nComputation   Slow           Medium         Fast (per fusion)\nMemory        Very high      Medium         Low\nOverfitting   High risk      Moderate       Low risk\nrisk\n\nCross-modal   Very strong    Strong         None\ninteraction\n\nInterpretab   Low            Medium         High\n-ility\n\nTransfer      Impossible     Excellent      Good\nlearning      (no pre-trains)\n\nRobustness    Poor           Good           Excellent\nto noise\n\nModularity    Low            Medium         High\n\nWhen to use   Rare           Most tasks     Special cases\n```\n\n**Decision flowchart:**\n\n```\nStart\n  │\n  ├─ Are modalities completely independent?\n  │   YES ─→ Consider LATE fusion (modularity)\n  │   NO ──→ Continue\n  │\n  ├─ Must handle missing modalities?\n  │   YES ─→ LATE fusion preferred\n  │   NO ──→ Continue\n  │\n  ├─ Have pre-trained encoders?\n  │   YES ─→ MID fusion (use them!)\n  │   NO ──→ Continue\n  │\n  ├─ Very small dataset?\n  │   YES ─→ MID fusion (leverage pre-training)\n  │   NO ──→ Continue\n  │\n  ├─ Importance of cross-modal interaction?\n  │   HIGH ─→ EARLY fusion (but risky!)\n  │   LOW ──→ MID or LATE fusion\n  │\n  └─→ DEFAULT: MID FUSION (best balance)\n```\n\n## 5.3 Advanced Fusion: Multimodal Transformers\n\n**Modern approach:** Use transformer architecture for fusion\n\n**Key insight:**\n```\nTransformers naturally handle multi-modal inputs\nJust treat different modalities as different token types\n```\n\n**Architecture:**\n\n```\nImage patches: [patch_1, patch_2, ..., patch_196]  (from ViT)\nText tokens:   [word_1, word_2, ..., word_N]       (from BERT tokenizer)\n\nUnified input: [[IMG:patch_1], [IMG:patch_2], ...,\n                [TXT:word_1], [TXT:word_2], ...]\n\nModality markers: [IMG, IMG, ..., TXT, TXT, ...]\nPosition encoding: [0, 1, ..., 196, 197, 198, ...]\n\nCombined tokens + markers + positions\n        ↓\nTransformer encoder (12 layers)\n        ↓\nSelf-attention between all tokens\n(image patches attend to text, vice versa)\n        ↓\nOutput: Multimodal representations\n```\n\n**Why this works:**\n\n```\nTransformer doesn't care about modality type\nPure attention-based fusion\nEach position (patch or token) can attend to all others\nLearns how to combine automatically\n\nExample attention pattern:\n\nWord \"red\" attends to:\n  - Red-colored image patches (high weight)\n  - Other color-related words (medium weight)\n  - Unrelated patches/words (low weight)\n\nImage patch attends to:\n  - Corresponding text description (high weight)\n  - Related patches (medium weight)\n  - Unrelated text (low weight)\n\nAll learned without explicit rules!\n```\n\n**Example architecture:**\n\n```python\nclass MultimodalTransformer(nn.Module):\n    def __init__(self, vocab_size, hidden_dim=768, num_layers=12):\n        super().__init__()\n\n        # Embeddings\n        self.img_embed = nn.Linear(2048, hidden_dim)  # Project image patches\n        self.txt_embed = nn.Embedding(vocab_size, hidden_dim)\n\n        # Modality tokens\n        self.img_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n        self.txt_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n\n        # Position encoding\n        self.pos_embed = nn.Embedding(1000, hidden_dim)\n\n        # Transformer\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_dim,\n                nhead=12,\n                dim_feedforward=3072,\n                batch_first=True\n            ),\n            num_layers=num_layers\n        )\n\n        self.classifier = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, images, text_ids):\n        batch_size = images.shape[0]\n\n        # Embed images (196 patches per image)\n        img_emb = self.img_embed(images)  # (batch, 196, 768)\n        img_emb = img_emb + self.img_token\n\n        # Embed text\n        txt_emb = self.txt_embed(text_ids)  # (batch, seq_len, 768)\n        txt_emb = txt_emb + self.txt_token\n\n        # Concatenate\n        combined = torch.cat([img_emb, txt_emb], dim=1)\n\n        # Add positional encoding\n        seq_len = combined.shape[1]\n        pos_ids = torch.arange(seq_len, device=combined.device)\n        pos_enc = self.pos_embed(pos_ids).unsqueeze(0)\n        combined = combined + pos_enc\n\n        # Transformer\n        out = self.transformer(combined)\n\n        # Use first token (like BERT [CLS]) for classification\n        cls_out = out[:, 0, :]\n        logits = self.classifier(cls_out)\n\n        return logits\n```\n\n**Advantages:**\n✓ Unified architecture\n✓ Automatic cross-modal fusion\n✓ Scales well\n✓ Flexible (add any modality)\n✓ State-of-the-art performance\n\n**Disadvantages:**\n✗ More complex\n✗ Slower inference\n✗ Needs careful tuning\n\n## 5.4 Handling Missing Modalities\n\n**Real-world challenge:**\n\n```\nTraining: All modalities present\nDeployment:\n  Sometimes only image available\n  Sometimes only text available\n  Rarely all modalities together\n\nExample scenarios:\n\nE-commerce system:\n  Training: 1M products with image + description + reviews\n  At test time:\n    Product A: Image only (video unavailable)\n    Product B: Text only (image not loading)\n    Product C: All modalities\n\nMedical system:\n  Training: Patients with CT + MRI + blood tests\n  At test time:\n    Patient A: Only CT scan (MRI machine broken)\n    Patient B: CT + blood (MRI not done)\n    Patient C: All three\n```\n\n### Solution 1: Independent Modality Pipelines\n\n**Approach:**\nTrain separate models for each modality and combinations\n\n```python\nclass MultimodalClassifier:\n    def __init__(self):\n        self.img_only_model = train_image_classifier()\n        self.txt_only_model = train_text_classifier()\n        self.fusion_model = train_fusion_model()\n\n    def predict(self, image=None, text=None):\n        if image is not None and text is not None:\n            # Both available: use fusion\n            img_features = extract_image_features(image)\n            txt_features = extract_text_features(text)\n            return self.fusion_model.predict([img_features, txt_features])\n\n        elif image is not None:\n            # Only image\n            return self.img_only_model.predict(image)\n\n        elif text is not None:\n            # Only text\n            return self.txt_only_model.predict(text)\n```\n\n**Advantages:**\n✓ Simple and modular\n✓ Good performance per modality\n✓ Easy to add modalities\n\n**Disadvantages:**\n✗ Requires training multiple models\n✗ Duplication of effort\n✗ Inconsistent predictions (models disagree)\n\n### Solution 2: Adaptive Fusion with Modality Weighting\n\n**Approach:**\nLearn which modalities to trust based on availability\n\n```python\nclass AdaptiveFusionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Feature extractors\n        self.img_extractor = ImageEncoder()\n        self.txt_extractor = TextEncoder()\n\n        # Modality gates (learn importance)\n        self.gate_img = nn.Sequential(\n            nn.Linear(2048, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n        self.gate_txt = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n        # Fusion and classification\n        self.fusion = nn.Linear(2048 + 768, 256)\n        self.classifier = nn.Linear(256, num_classes)\n\n    def forward(self, image=None, text=None):\n        features = []\n\n        if image is not None:\n            img_feat = self.img_extractor(image)\n            w_img = self.gate_img(img_feat)\n            img_feat = img_feat * w_img\n            features.append(img_feat)\n\n        if text is not None:\n            txt_feat = self.txt_extractor(text)\n            w_txt = self.gate_txt(txt_feat)\n            txt_feat = txt_feat * w_txt\n            features.append(txt_feat)\n\n        # Concatenate available features\n        combined = torch.cat(features, dim=1)\n\n        # Pad if missing modalities\n        if image is None:\n            combined = torch.cat([torch.zeros(batch, 2048), combined])\n        if text is None:\n            combined = torch.cat([combined, torch.zeros(batch, 768)])\n\n        fused = self.fusion(combined)\n        logits = self.classifier(fused)\n\n        return logits\n```\n\n**How it works:**\n- Gate networks learn importance of each modality\n- During training: All modalities penalize equally (gates = 1)\n- Some modalities learned as less important (gates < 1)\n- At test time: Missing modalities handled gracefully (gate = 0)\n\n**Advantages:**\n✓ Single model\n✓ Learns to trust modalities\n✓ Handles missing data\n\n**Disadvantages:**\n✗ More complex training\n✗ Potential numerical issues (zeros in features)\n\n### Solution 3: Modality Embedding/Imputation\n\n**Approach:**\nPredict missing modalities from available ones\n\n```python\nclass ImputingFusionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.img_encoder = ImageEncoder()\n        self.txt_encoder = TextEncoder()\n\n        # Decoders for imputation\n        self.img_to_txt_decoder = nn.Linear(2048, 768)\n        self.txt_to_img_decoder = nn.Linear(768, 2048)\n\n        # Classification\n        self.classifier = nn.Linear(2048 + 768, num_classes)\n\n    def forward(self, image=None, text=None):\n        if image is not None:\n            img_feat = self.img_encoder(image)\n        else:\n            txt_feat = self.txt_encoder(text)\n            img_feat = self.txt_to_img_decoder(txt_feat)\n\n        if text is not None:\n            txt_feat = self.txt_encoder(text)\n        else:\n            img_feat = self.img_encoder(image)\n            txt_feat = self.img_to_txt_decoder(img_feat)\n\n        combined = torch.cat([img_feat, txt_feat], dim=1)\n        logits = self.classifier(combined)\n\n        return logits\n```\n\n**How it works:**\n- If text missing: Predict from image\n- If image missing: Predict from text\n- Use predictions as if real\n\n**Advantages:**\n✓ Single model\n✓ Predictions fill in gaps\n✓ Cross-modal knowledge transfer\n\n**Disadvantages:**\n✗ Predictions may be inaccurate\n✗ Error propagation\n✗ Requires training decoder networks\n\n## 5.5 Practical Fusion Examples\n\n### Example 1: Image-Text Sentiment Analysis\n\n**Problem:**\nDetermine sentiment from image + text (social media post)\n\n```python\nclass SentimentFusionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Encoders\n        self.img_encoder = models.resnet50(pretrained=True)\n        self.img_encoder = nn.Sequential(*list(self.img_encoder.children())[:-1])\n\n        self.txt_encoder = AutoModel.from_pretrained('bert-base-uncased')\n\n        # Projections to common space (256D)\n        self.img_proj = nn.Linear(2048, 256)\n        self.txt_proj = nn.Linear(768, 256)\n\n        # Fusion\n        self.fusion_options = {\n            'concat': FusionConcat(512, 256),\n            'add': FusionAdd(),\n            'mult': FusionMult(),\n            'attention': FusionAttention(256)\n        }\n\n        # Classification\n        self.classifier = nn.Linear(256, 3)  # 3 sentiments\n\n    def forward(self, image, text, fusion_type='concat'):\n        # Extract features\n        img_feat = self.img_encoder(image).squeeze(-1).squeeze(-1)\n\n        txt_inputs = self.tokenizer(\n            text, return_tensors='pt', padding=True, truncation=True\n        )\n        txt_out = self.txt_encoder(**txt_inputs)\n        txt_feat = txt_out.last_hidden_state[:, 0, :]\n\n        # Project to common space\n        img_proj = self.img_proj(img_feat)\n        txt_proj = self.txt_proj(txt_feat)\n\n        # Normalize\n        img_proj = F.normalize(img_proj, p=2, dim=1)\n        txt_proj = F.normalize(txt_proj, p=2, dim=1)\n\n        # Fuse\n        fused = self.fusion_options[fusion_type](img_proj, txt_proj)\n\n        # Classify\n        logits = self.classifier(fused)\n\n        return logits\n\n# Fusion modules\nclass FusionConcat(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, img, txt):\n        combined = torch.cat([img, txt], dim=1)\n        return self.fc(combined)\n\nclass FusionAdd(nn.Module):\n    def forward(self, img, txt):\n        return img + txt\n\nclass FusionMult(nn.Module):\n    def forward(self, img, txt):\n        return img * txt\n\nclass FusionAttention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(dim, num_heads=8)\n\n    def forward(self, img, txt):\n        # img, txt: (batch, dim)\n        # Reshape for attention: (seq_len=1, batch, dim)\n        img_seq = img.unsqueeze(0)\n        txt_seq = txt.unsqueeze(0)\n\n        # txt attends to img\n        attended, _ = self.attention(txt_seq, img_seq, img_seq)\n        return attended.squeeze(0)\n```\n\n**Training:**\n\n```python\nmodel = SentimentFusionModel()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(num_epochs):\n    for images, texts, labels in train_loader:\n        # Forward pass with different fusion strategies\n        logits = model(images, texts, fusion_type='attention')\n\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Evaluate\n    model.eval()\n    accuracy = evaluate(model, val_loader)\n    print(f\"Epoch {epoch}: Accuracy = {accuracy:.3f}\")\n```\n\n---\n\n### Example 2: Video Understanding with Audio-Visual Fusion\n\n**Problem:**\nClassify video action considering both visual frames and audio\n\n```python\nclass AudioVisualFusionModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        # Visual encoder (3D CNN for video)\n        self.visual_encoder = Video3DCNN(output_dim=512)\n\n        # Audio encoder\n        self.audio_encoder = AudioCNN(output_dim=256)\n\n        # Temporal models\n        self.visual_lstm = nn.LSTM(512, 256, batch_first=True)\n        self.audio_lstm = nn.LSTM(256, 256, batch_first=True)\n\n        # Fusion\n        self.fusion = nn.Linear(512, 256)\n\n        # Classification\n        self.classifier = nn.Linear(256, num_classes)\n\n    def forward(self, video, audio):\n        batch_size = video.shape[0]\n        num_frames = video.shape[1]\n\n        # Process video frames\n        visual_features = []\n        for t in range(num_frames):\n            frame_feat = self.visual_encoder(video[:, t])  # (batch, 512)\n            visual_features.append(frame_feat)\n        visual_seq = torch.stack(visual_features, dim=1)  # (batch, num_frames, 512)\n\n        # LSTM over frames\n        visual_out, _ = self.visual_lstm(visual_seq)\n        visual_final = visual_out[:, -1, :]  # (batch, 256) - last frame\n\n        # Process audio\n        audio_feat = self.audio_encoder(audio)  # (batch, seq_len, 256)\n        audio_out, _ = self.audio_lstm(audio_feat)\n        audio_final = audio_out[:, -1, :]  # (batch, 256) - last time step\n\n        # Fuse\n        combined = torch.cat([visual_final, audio_final], dim=1)  # (batch, 512)\n        fused = self.fusion(combined)\n\n        # Classify\n        logits = self.classifier(fused)\n\n        return logits\n```\n\n**Key considerations:**\n- Video: Multiple frames, visual information\n- Audio: Temporal signal, semantic content\n- Synchronization: Both should be aligned in time\n- Late fusion: Aggregate final representations\n\n---\n\n## Key Takeaways\n\n- **Early fusion:** Raw data level, high dimensionality, rarely used\n- **Mid fusion:** Feature level, standard approach, recommended\n- **Late fusion:** Decision level, modular, handles missing data well\n- **Transformers:** Modern approach, automatic fusion\n- **Missing modalities:** Solutions include independent models, adaptive weighting, imputation\n- **Choose based on:** Data characteristics, modality importance, missing data handling\n\n## Exercises\n\n**⭐ Beginner:**\n1. Implement early, mid, late fusion for simple dataset\n2. Compare fusion strategies on evaluation metrics\n3. Visualize combined feature space\n\n**⭐⭐ Intermediate:**\n4. Build adaptive fusion with modality gates\n5. Handle missing modalities with multiple strategies\n6. Compare computational costs of different approaches\n\n**⭐⭐⭐ Advanced:**\n7. Implement multimodal transformer from scratch\n8. Design adaptive weighting scheme for heterogeneous data\n9. Build system handling variable numbers of modalities\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-05.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}