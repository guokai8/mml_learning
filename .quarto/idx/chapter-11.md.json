{"title":"Chapter 11: Practical Implementation Guide","markdown":{"headingText":"Chapter 11: Practical Implementation Guide","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 10: Seminal Models and Architectures](chapter-10.md) | **Next**: [Chapter 12: Advanced Topics and Future Directions](chapter-12.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Collect and preprocess multimodal datasets\n- Build production-ready training pipelines\n- Handle edge cases and failures\n- Deploy models efficiently\n- Monitor and maintain systems\n- Implement best practices for MLOps\n\n## 11.1 Data Collection and Preprocessing\n\n### Building Multimodal Datasets\n\n**Data sources:**\n\n```\nWeb-scale data:\n  LAION (5.8B images + captions)\n  Conceptual Captions (3.3M pairs)\n  Wikipedia + images\n  News articles + images\n  Social media posts + images/video\n\nCurated datasets:\n  COCO (image captioning)\n  Flickr30K (image-text)\n  Visual Genome (regions + descriptions)\n  ActivityNet (video + captions)\n\nSynthetic/Generated:\n  Text descriptions from writers\n  AI-generated descriptions\n  Rule-based generation\n```\n\n**Data quality considerations:**\n\n```\nIssue 1: Image-text mismatch\n  Problem: Caption doesn't describe image\n  Solution: Filter with CLIP-based similarity\n\nIssue 2: Duplicate or near-duplicate pairs\n  Problem: Same image with different captions\n  Solution: Hash-based deduplication\n\nIssue 3: Offensive or sensitive content\n  Problem: Dataset contains harmful content\n  Solution: Content moderation filters\n\nIssue 4: Biases in distribution\n  Problem: Skewed toward certain domains\n  Solution: Stratified sampling, data augmentation\n\nIssue 5: Missing or corrupted files\n  Problem: Broken image links, corrupted videos\n  Solution: Validation pipeline\n```\n\n### Preprocessing Pipeline\n\n**Step 1: Image preprocessing**\n\n```python\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch\n\nclass ImagePreprocessor:\n    def __init__(self, input_size=224):\n        self.input_size = input_size\n\n        # Training transforms (with augmentation)\n        self.train_transforms = transforms.Compose([\n            transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.RandomRotation(15),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n        # Validation transforms (no augmentation)\n        self.val_transforms = transforms.Compose([\n            transforms.Resize((input_size, input_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    def preprocess_image(self, image_path, is_train=True):\n        \"\"\"Load and preprocess image\"\"\"\n        try:\n            # Load image\n            image = Image.open(image_path).convert('RGB')\n\n            # Apply transforms\n            if is_train:\n                image = self.train_transforms(image)\n            else:\n                image = self.val_transforms(image)\n\n            return image\n\n        except Exception as e:\n            print(f\"Error processing {image_path}: {e}\")\n            return None\n\n    def preprocess_batch(self, image_paths, is_train=True):\n        \"\"\"Preprocess batch of images\"\"\"\n        images = []\n        valid_paths = []\n\n        for path in image_paths:\n            img = self.preprocess_image(path, is_train)\n            if img is not None:\n                images.append(img)\n                valid_paths.append(path)\n\n        if images:\n            images = torch.stack(images)\n            return images, valid_paths\n        else:\n            return None, []\n\n# Example usage\npreprocessor = ImagePreprocessor(input_size=224)\nimage_batch, valid_paths = preprocessor.preprocess_batch(\n    image_paths=['img1.jpg', 'img2.jpg', 'img3.jpg'],\n    is_train=True\n)\n```\n\n**Step 2: Text preprocessing**\n\n```python\nfrom transformers import AutoTokenizer\n\nclass TextPreprocessor:\n    def __init__(self, model_name='bert-base-uncased', max_length=77):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.max_length = max_length\n\n    def clean_text(self, text):\n        \"\"\"Clean text\"\"\"\n        # Remove extra whitespace\n        text = ' '.join(text.split())\n\n        # Remove special characters (keep basic punctuation)\n        import re\n        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\']', '', text)\n\n        # Lowercase\n        text = text.lower()\n\n        return text\n\n    def tokenize(self, text):\n        \"\"\"Tokenize single text\"\"\"\n        cleaned = self.clean_text(text)\n\n        tokens = self.tokenizer(\n            cleaned,\n            return_tensors='pt',\n            padding='max_length',\n            max_length=self.max_length,\n            truncation=True,\n            add_special_tokens=True\n        )\n\n        return tokens\n\n    def tokenize_batch(self, texts):\n        \"\"\"Tokenize batch of texts\"\"\"\n        cleaned = [self.clean_text(text) for text in texts]\n\n        tokens = self.tokenizer(\n            cleaned,\n            return_tensors='pt',\n            padding='max_length',\n            max_length=self.max_length,\n            truncation=True,\n            batch_first=True\n        )\n\n        return tokens\n\n# Example\ntext_proc = TextPreprocessor()\ntokens = text_proc.tokenize_batch([\n    \"A red cat on a wooden chair\",\n    \"Two dogs playing in the park\"\n])\nprint(tokens['input_ids'].shape)  # (2, 77)\n```\n\n**Step 3: Video preprocessing**\n\n```python\nimport cv2\nimport numpy as np\n\nclass VideoPreprocessor:\n    def __init__(self, fps=1, frame_count=8, frame_size=224):\n        self.fps = fps\n        self.frame_count = frame_count\n        self.frame_size = frame_size\n\n    def extract_frames(self, video_path):\n        \"\"\"Extract frames from video\"\"\"\n        try:\n            cap = cv2.VideoCapture(video_path)\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n            # Sample frames evenly\n            frame_indices = np.linspace(\n                0, total_frames - 1,\n                self.frame_count,\n                dtype=int\n            )\n\n            frames = []\n            for idx in frame_indices:\n                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n                ret, frame = cap.read()\n\n                if ret:\n                    # Resize\n                    frame = cv2.resize(\n                        frame,\n                        (self.frame_size, self.frame_size)\n                    )\n                    # Convert BGR to RGB\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    frames.append(frame)\n\n            cap.release()\n\n            if frames:\n                return np.stack(frames)  # (frame_count, h, w, 3)\n            else:\n                return None\n\n        except Exception as e:\n            print(f\"Error processing video {video_path}: {e}\")\n            return None\n\n# Example\nvideo_proc = VideoPreprocessor(frame_count=8)\nframes = video_proc.extract_frames('video.mp4')\nprint(frames.shape)  # (8, 224, 224, 3)\n```\n\n### Complete Preprocessing Pipeline\n\n```python\nclass MultimodalDataPreprocessor:\n    \"\"\"Complete preprocessing for image-text-video data\"\"\"\n\n    def __init__(self, image_size=224, max_text_length=77,\n                 video_frames=8):\n        self.image_preprocessor = ImagePreprocessor(image_size)\n        self.text_preprocessor = TextPreprocessor(max_text_length)\n        self.video_preprocessor = VideoPreprocessor(frame_count=video_frames)\n\n    def process_sample(self, sample):\n        \"\"\"Process single multimodal sample\"\"\"\n        processed = {}\n\n        # Image\n        if 'image_path' in sample:\n            img = self.image_preprocessor.preprocess_image(\n                sample['image_path'],\n                is_train=sample.get('is_train', True)\n            )\n            if img is not None:\n                processed['image'] = img\n\n        # Text\n        if 'text' in sample:\n            tokens = self.text_preprocessor.tokenize(sample['text'])\n            processed['text_ids'] = tokens['input_ids'].squeeze()\n            processed['text_mask'] = tokens['attention_mask'].squeeze()\n\n        # Video\n        if 'video_path' in sample:\n            frames = self.video_preprocessor.extract_frames(\n                sample['video_path']\n            )\n            if frames is not None:\n                processed['video'] = torch.from_numpy(frames).float()\n\n        # Label (if available)\n        if 'label' in sample:\n            processed['label'] = torch.tensor(sample['label'])\n\n        return processed\n\n    def validate_sample(self, sample):\n        \"\"\"Check if sample is valid\"\"\"\n        required_keys = sample.get('required_modalities', ['image', 'text'])\n\n        for key in required_keys:\n            if key not in sample:\n                return False\n\n        return True\n\n# Usage\npreprocessor = MultimodalDataPreprocessor()\n\nsample = {\n    'image_path': 'cat.jpg',\n    'text': 'A cute cat on a sofa',\n    'label': 0,\n    'is_train': True,\n    'required_modalities': ['image', 'text']\n}\n\nif preprocessor.validate_sample(sample):\n    processed = preprocessor.process_sample(sample)\n    print(f\"Image shape: {processed['image'].shape}\")\n    print(f\"Text IDs shape: {processed['text_ids'].shape}\")\n```\n\n## 11.2 Building Training Pipelines\n\n### Data Loading with Multiprocessing\n\n```python\nfrom torch.utils.data import Dataset, DataLoader\nimport multiprocessing as mp\n\nclass MultimodalDataset(Dataset):\n    \"\"\"Efficient multimodal dataset\"\"\"\n\n    def __init__(self, samples, preprocessor, cache_size=1000):\n        self.samples = samples\n        self.preprocessor = preprocessor\n        self.cache = {}\n        self.cache_size = cache_size\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # Check cache first\n        if idx in self.cache:\n            return self.cache[idx]\n\n        # Load and preprocess\n        sample = self.samples[idx]\n        processed = self.preprocessor.process_sample(sample)\n\n        # Cache if space available\n        if len(self.cache) < self.cache_size:\n            self.cache[idx] = processed\n\n        return processed\n\ndef create_dataloaders(train_samples, val_samples, batch_size=256,\n                      num_workers=8):\n    \"\"\"Create train and validation dataloaders\"\"\"\n\n    preprocessor = MultimodalDataPreprocessor()\n\n    train_dataset = MultimodalDataset(train_samples, preprocessor)\n    val_dataset = MultimodalDataset(val_samples, preprocessor)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=False\n    )\n\n    return train_loader, val_loader\n\n# Usage\ntrain_loader, val_loader = create_dataloaders(\n    train_samples=train_data,\n    val_samples=val_data,\n    batch_size=256,\n    num_workers=8\n)\n```\n\n### Training Loop with Best Practices\n\n```python\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nimport wandb\n\nclass MultimodalTrainer:\n    \"\"\"Production-ready trainer\"\"\"\n\n    def __init__(self, model, device='cuda', use_wandb=True):\n        self.model = model\n        self.device = device\n        self.use_wandb = use_wandb\n\n        if use_wandb:\n            wandb.init(project='multimodal-learning')\n\n    def train_epoch(self, train_loader, optimizer, scheduler,\n                   criterion, scaler=None):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        num_batches = 0\n\n        pbar = tqdm(train_loader, desc='Training')\n\n        for batch_idx, batch in enumerate(pbar):\n            # Move to device\n            images = batch['image'].to(self.device)\n            text_ids = batch['text_ids'].to(self.device)\n            text_mask = batch['text_mask'].to(self.device)\n\n            # Forward pass with mixed precision\n            if scaler is not None:\n                with torch.cuda.amp.autocast():\n                    logits = self.model(images, text_ids, text_mask)\n                    loss = criterion(logits, batch['label'].to(self.device))\n\n                # Backward pass\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n            else:\n                logits = self.model(images, text_ids, text_mask)\n                loss = criterion(logits, batch['label'].to(self.device))\n                loss.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(\n                self.model.parameters(),\n                max_norm=1.0\n            )\n\n            # Optimization step\n            if scaler is not None:\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.step()\n\n            optimizer.zero_grad()\n            scheduler.step()\n\n            total_loss += loss.item()\n            num_batches += 1\n\n            # Update progress bar\n            pbar.set_postfix({'loss': total_loss / num_batches})\n\n            # Log to wandb\n            if self.use_wandb and batch_idx % 100 == 0:\n                wandb.log({\n                    'train_loss': loss.item(),\n                    'learning_rate': scheduler.get_last_lr()[0]\n                })\n\n        return total_loss / num_batches\n\n    @torch.no_grad()\n    def evaluate(self, val_loader, criterion):\n        \"\"\"Evaluate on validation set\"\"\"\n        self.model.eval()\n        total_loss = 0\n        total_acc = 0\n        num_batches = 0\n\n        pbar = tqdm(val_loader, desc='Validating')\n\n        for batch in pbar:\n            images = batch['image'].to(self.device)\n            text_ids = batch['text_ids'].to(self.device)\n            text_mask = batch['text_mask'].to(self.device)\n            labels = batch['label'].to(self.device)\n\n            logits = self.model(images, text_ids, text_mask)\n            loss = criterion(logits, labels)\n\n            # Accuracy\n            preds = logits.argmax(dim=1)\n            acc = (preds == labels).float().mean()\n\n            total_loss += loss.item()\n            total_acc += acc.item()\n            num_batches += 1\n\n            pbar.set_postfix({\n                'loss': total_loss / num_batches,\n                'acc': total_acc / num_batches\n            })\n\n        return total_loss / num_batches, total_acc / num_batches\n\n    def train(self, train_loader, val_loader, num_epochs=10,\n             lr=1e-4, warmup_steps=1000):\n        \"\"\"Full training loop\"\"\"\n\n        # Optimizer\n        optimizer = AdamW(self.model.parameters(), lr=lr)\n\n        # Scheduler with warmup\n        total_steps = len(train_loader) * num_epochs\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=lr,\n            total_steps=total_steps,\n            pct_start=warmup_steps / total_steps\n        )\n\n        # Mixed precision\n        scaler = torch.cuda.amp.GradScaler()\n\n        # Loss\n        criterion = torch.nn.CrossEntropyLoss()\n\n        # Training loop\n        best_val_loss = float('inf')\n        patience = 5\n        patience_counter = 0\n\n        for epoch in range(num_epochs):\n            print(f\"\\n{'='*50}\")\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n            print(f\"{'='*50}\")\n\n            # Train\n            train_loss = self.train_epoch(\n                train_loader, optimizer, scheduler,\n                criterion, scaler\n            )\n\n            # Validate\n            val_loss, val_acc = self.evaluate(val_loader, criterion)\n\n            print(f\"Train Loss: {train_loss:.4f}\")\n            print(f\"Val Loss: {val_loss:.4f}\")\n            print(f\"Val Acc: {val_acc:.4f}\")\n\n            # Log to wandb\n            if self.use_wandb:\n                wandb.log({\n                    'epoch': epoch,\n                    'train_loss': train_loss,\n                    'val_loss': val_loss,\n                    'val_acc': val_acc\n                })\n\n            # Early stopping\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n\n                # Save checkpoint\n                self.save_checkpoint(f'best_model_epoch{epoch}.pt')\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping after {epoch+1} epochs\")\n                    break\n\n        if self.use_wandb:\n            wandb.finish()\n\n    def save_checkpoint(self, path):\n        \"\"\"Save model checkpoint\"\"\"\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'model_config': self.model.config if hasattr(self.model, 'config') else None\n        }, path)\n        print(f\"Saved checkpoint to {path}\")\n\n# Usage\nmodel = MultimodalModel()\ntrainer = MultimodalTrainer(model)\n\ntrainer.train(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=30,\n    lr=1e-4\n)\n```\n\n## 11.3 Handling Edge Cases and Failures\n\n### Error Handling in Data Loading\n\n```python\nclass RobustDataLoader:\n    \"\"\"Data loader with error handling\"\"\"\n\n    def __init__(self, dataset, batch_size=32, num_workers=4):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.failed_indices = []\n\n    def load_with_retry(self, idx, max_retries=3):\n        \"\"\"Load sample with retry logic\"\"\"\n        for attempt in range(max_retries):\n            try:\n                return self.dataset[idx]\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    print(f\"Failed to load sample {idx} after {max_retries} attempts: {e}\")\n                    self.failed_indices.append(idx)\n                    return None\n\n    def get_valid_batch(self, indices):\n        \"\"\"Get batch skipping failed samples\"\"\"\n        batch = []\n        valid_indices = []\n\n        for idx in indices:\n            sample = self.load_with_retry(idx)\n            if sample is not None:\n                batch.append(sample)\n                valid_indices.append(idx)\n\n        if not batch:\n            return None, []\n\n        # Stack samples\n        try:\n            stacked = {}\n            for key in batch[0].keys():\n                stacked[key] = torch.stack([s[key] for s in batch])\n            return stacked, valid_indices\n        except Exception as e:\n            print(f\"Error stacking batch: {e}\")\n            return None, []\n\n# Usage\nrobust_loader = RobustDataLoader(dataset, batch_size=32)\n```\n\n### Validation and Sanity Checks\n\n```python\nclass DataValidator:\n    \"\"\"Validate data quality\"\"\"\n\n    @staticmethod\n    def check_image_quality(image_tensor, min_entropy=0.5):\n        \"\"\"Check if image has meaningful content\"\"\"\n        # Calculate entropy\n        import torch.nn.functional as F\n\n        # Flatten and normalize to [0, 1]\n        flat = image_tensor.flatten()\n        flat = (flat - flat.min()) / (flat.max() - flat.min() + 1e-8)\n\n        # Histogram-based entropy\n        hist = torch.histc(flat, bins=256)\n        hist = hist / hist.sum()\n        entropy = -(hist * torch.log(hist + 1e-8)).sum()\n\n        return entropy > min_entropy\n\n    @staticmethod\n    def check_text_quality(text, min_length=5, max_length=1000):\n        \"\"\"Check if text is valid\"\"\"\n        if text is None or not isinstance(text, str):\n            return False\n\n        text = text.strip()\n\n        if len(text) < min_length or len(text) > max_length:\n            return False\n\n        # Check for too many special characters\n        special_chars = sum(1 for c in text if not c.isalnum() and c != ' ')\n        if special_chars / len(text) > 0.5:\n            return False\n\n        return True\n\n    @staticmethod\n    def check_alignment(image_tensor, text, similarity_fn):\n        \"\"\"Check if image and text are aligned\"\"\"\n        # Encode both\n        img_feat = image_encoder(image_tensor.unsqueeze(0))\n        txt_feat = text_encoder(text)\n\n        # Compute similarity\n        sim = similarity_fn(img_feat, txt_feat)\n\n        # Threshold (depends on model)\n        return sim > 0.3\n\n# Usage\nvalidator = DataValidator()\n\n# Check a sample\nif validator.check_image_quality(image) and \\\n   validator.check_text_quality(text) and \\\n   validator.check_alignment(image, text, similarity_fn):\n    print(\"Sample is valid!\")\n```\n\n## 11.4 Optimization for Production\n\n### Model Quantization\n\n```python\nclass ModelQuantizer:\n    \"\"\"Quantize model for faster inference\"\"\"\n\n    @staticmethod\n    def quantize_int8(model, sample_input):\n        \"\"\"Convert to INT8 quantization\"\"\"\n        model.eval()\n\n        # Dynamic quantization (easiest)\n        quantized = torch.quantization.quantize_dynamic(\n            model,\n            {torch.nn.Linear},\n            dtype=torch.qint8\n        )\n\n        return quantized\n\n    @staticmethod\n    def quantize_with_calibration(model, calibration_loader):\n        \"\"\"Quantization with calibration data\"\"\"\n        model.eval()\n        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n        # Insert observers\n        torch.quantization.prepare(model, inplace=True)\n\n        # Calibrate with sample data\n        with torch.no_grad():\n            for batch in calibration_loader:\n                _ = model(batch)\n\n        # Convert to quantized model\n        torch.quantization.convert(model, inplace=True)\n\n        return model\n\n# Usage\nquantizer = ModelQuantizer()\n\n# Simple quantization\nq_model = quantizer.quantize_int8(model, sample_input)\n\n# Memory savings\nprint(f\"Original model size: {get_model_size(model):.2f} MB\")\nprint(f\"Quantized model size: {get_model_size(q_model):.2f} MB\")\n```\n\n### Knowledge Distillation\n\n```python\nclass KnowledgeDistiller:\n    \"\"\"Distill large model to small student\"\"\"\n\n    def __init__(self, teacher_model, student_model, temperature=3.0):\n        self.teacher = teacher_model\n        self.student = student_model\n        self.temperature = temperature\n\n    def distillation_loss(self, student_logits, teacher_logits, labels,\n                         alpha=0.7):\n        \"\"\"Combined distillation + task loss\"\"\"\n        # KL divergence for distillation\n        kd_loss = torch.nn.functional.kl_div(\n            torch.nn.functional.log_softmax(\n                student_logits / self.temperature,\n                dim=1\n            ),\n            torch.nn.functional.softmax(\n                teacher_logits / self.temperature,\n                dim=1\n            ),\n            reduction='batchmean'\n        ) * (self.temperature ** 2)\n\n        # Task loss\n        task_loss = torch.nn.functional.cross_entropy(\n            student_logits,\n            labels\n        )\n\n        # Combined\n        return alpha * kd_loss + (1 - alpha) * task_loss\n\n    def train_student(self, train_loader, optimizer, num_epochs):\n        \"\"\"Train student model\"\"\"\n        self.teacher.eval()\n\n        for epoch in range(num_epochs):\n            total_loss = 0\n\n            for batch in train_loader:\n                # Teacher predictions (no gradients)\n                with torch.no_grad():\n                    teacher_logits = self.teacher(batch)\n\n                # Student predictions\n                student_logits = self.student(batch)\n\n                # Loss\n                loss = self.distillation_loss(\n                    student_logits,\n                    teacher_logits,\n                    batch['label']\n                )\n\n                # Backprop\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")\n```\n\n### Model Serving with TorchServe\n\n```yaml\n# config.yaml\nmodel_store: ./model_store\nncs: false\n\n# Handler (my_handler.py)\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\nclass MultimodalHandler:\n    def __init__(self):\n        self.image_model = AutoModel.from_pretrained('model_name')\n        self.text_model = AutoModel.from_pretrained('model_name')\n        self.tokenizer = AutoTokenizer.from_pretrained('model_name')\n\n    def preprocess(self, data):\n        image = data['image']\n        text = data['text']\n\n        tokens = self.tokenizer(text, return_tensors='pt')\n\n        return image, tokens\n\n    def inference(self, image, tokens):\n        img_feat = self.image_model(image)\n        txt_feat = self.text_model(tokens['input_ids'])\n\n        # Compute similarity\n        similarity = torch.cosine_similarity(img_feat, txt_feat)\n\n        return similarity\n\n    def postprocess(self, output):\n        return {'similarity': float(output)}\n\n# Deployment\n# torch-model-archiver --model-name multimodal \\\n#     --version 1.0 \\\n#     --model-file model.py \\\n#     --serialized-file model.pt \\\n#     --handler my_handler.py \\\n#     --export-path model_store\n#\n# torchserve --start --model-store model_store \\\n#     --ncs --models multimodal=multimodal.mar\n```\n\n## 11.5 Monitoring and Maintenance\n\n### Model Performance Monitoring\n\n```python\nimport logging\nfrom datetime import datetime\n\nclass ModelMonitor:\n    \"\"\"Monitor model performance in production\"\"\"\n\n    def __init__(self, log_file='model_performance.log'):\n        self.log_file = log_file\n        self.setup_logging()\n\n    def setup_logging(self):\n        logging.basicConfig(\n            filename=self.log_file,\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n\n    def check_drift(self, current_batch, reference_data):\n        \"\"\"Check for data drift\"\"\"\n        # Compare statistics\n        current_mean = current_batch.mean()\n        reference_mean = reference_data.mean()\n\n        # Z-test\n        drift_score = abs(current_mean - reference_mean) / reference_data.std()\n\n        if drift_score > 3.0:  # Threshold\n            logging.warning(f\"Data drift detected: {drift_score:.2f}\")\n            return True\n\n        return False\n\n    def log_prediction(self, input_id, prediction, confidence, latency):\n        \"\"\"Log prediction for audit trail\"\"\"\n        log_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'input_id': input_id,\n            'prediction': prediction,\n            'confidence': float(confidence),\n            'latency_ms': latency\n        }\n\n        logging.info(str(log_entry))\n\n    def detect_anomalies(self, predictions, threshold=2.0):\n        \"\"\"Detect anomalous predictions\"\"\"\n        confidences = [p['confidence'] for p in predictions]\n        mean_conf = np.mean(confidences)\n        std_conf = np.std(confidences)\n\n        anomalies = []\n        for i, pred in enumerate(predictions):\n            z_score = abs(pred['confidence'] - mean_conf) / (std_conf + 1e-6)\n            if z_score > threshold:\n                anomalies.append(i)\n\n        return anomalies\n\n# Usage\nmonitor = ModelMonitor()\n\n# During inference\nfor batch in inference_batches:\n    predictions = model(batch)\n\n    for i, pred in enumerate(predictions):\n        monitor.log_prediction(\n            input_id=batch['id'][i],\n            prediction=pred['class'],\n            confidence=pred['confidence'],\n            latency=pred['latency_ms']\n        )\n\n    # Check for issues\n    if monitor.check_drift(batch, reference_batch):\n        print(\"Model may need retraining!\")\n\n    anomalies = monitor.detect_anomalies(predictions)\n    if anomalies:\n        print(f\"Anomalous predictions at indices: {anomalies}\")\n```\n\n### A/B Testing\n\n```python\nclass ABTester:\n    \"\"\"A/B testing for model updates\"\"\"\n\n    def __init__(self, model_a, model_b, split_ratio=0.5):\n        self.model_a = model_a\n        self.model_b = model_b\n        self.split_ratio = split_ratio\n        self.results = {'a': [], 'b': []}\n\n    def predict(self, input_data, user_id=None):\n        \"\"\"Route to model A or B\"\"\"\n        # Consistent routing per user\n        if user_id is not None:\n            use_a = hash(user_id) % 100 < (self.split_ratio * 100)\n        else:\n            use_a = np.random.rand() < self.split_ratio\n\n        if use_a:\n            prediction = self.model_a(input_data)\n            self.results['a'].append(prediction)\n            return prediction, 'a'\n        else:\n            prediction = self.model_b(input_data)\n            self.results['b'].append(prediction)\n            return prediction, 'b'\n\n    def get_statistics(self):\n        \"\"\"Compare model performance\"\"\"\n        def compute_stats(results):\n            accs = [r['accuracy'] for r in results]\n            return {\n                'mean_accuracy': np.mean(accs),\n                'std_accuracy': np.std(accs),\n                'count': len(accs)\n            }\n\n        stats_a = compute_stats(self.results['a'])\n        stats_b = compute_stats(self.results['b'])\n\n        # Statistical test\n        from scipy import stats\n        t_stat, p_value = stats.ttest_ind(\n            [r['accuracy'] for r in self.results['a']],\n            [r['accuracy'] for r in self.results['b']]\n        )\n\n        return {\n            'model_a': stats_a,\n            'model_b': stats_b,\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'winner': 'b' if stats_b['mean_accuracy'] > stats_a['mean_accuracy'] else 'a'\n        }\n\n# Usage\nab_tester = ABTester(model_v1, model_v2, split_ratio=0.5)\n\n# In production\nfor request in requests:\n    prediction, model_used = ab_tester.predict(request, user_id=request['user_id'])\n\n# After collecting data\nstats = ab_tester.get_statistics()\nprint(f\"Winner: Model {stats['winner']}\")\nprint(f\"P-value: {stats['p_value']}\")\n```\n\n## Key Takeaways\n\n- **Preprocessing is critical** - garbage in, garbage out\n- **Robust error handling** prevents cascading failures\n- **Monitoring catches issues early** - drift, anomalies, degradation\n- **Optimization techniques** make models production-ready\n- **A/B testing validates improvements** before full rollout\n- **MLOps practices** enable reliable systems\n\n## Exercises\n\n**⭐ Beginner:**\n1. Build image preprocessing pipeline\n2. Create text tokenization pipeline\n3. Implement basic data validation\n\n**⭐⭐ Intermediate:**\n4. Build multimodal dataset loader\n5. Implement training loop with early stopping\n6. Add logging and monitoring\n\n**⭐⭐⭐ Advanced:**\n7. Implement model quantization\n8. Set up knowledge distillation\n9. Deploy model with monitoring\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-11.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}