{"title":"Chapter 3: Feature Representation for Each Modality","markdown":{"headingText":"Chapter 3: Feature Representation for Each Modality","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 2: Foundations and Core Concepts](chapter-02.md) | **Next**: [Chapter 4: Feature Alignment and Bridging Modalities](chapter-04.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Extract features from text using various methods\n- Understand CNN architectures for image processing  \n- Process audio signals for machine learning\n- Choose appropriate feature extraction for different modalities\n- Debug common issues in feature extraction pipelines\n\n## 3.1 Text Representation\n\n### Bag of Words (BoW) - The Foundation\n\n**Basic concept:** Represent text as word frequency counts\n\n**Example:**\n```\nText: \"The cat sat on the mat\"\nVocabulary: [the, cat, sat, on, mat, dog, run, ...]\n\nBoW representation:\n[2, 1, 1, 1, 1, 0, 0, ...]\n↑  ↑  ↑  ↑  ↑  ↑  ↑\n\"the\" appears 2 times\n\"cat\" appears 1 time  \n\"sat\" appears 1 time\n\"on\" appears 1 time\n\"mat\" appears 1 time\n\"dog\" appears 0 times\n\"run\" appears 0 times\n```\n\n**Advantages:**\n✓ Simple to understand and implement\n✓ Fast computation\n✓ Good baseline for many tasks\n✓ Interpretable (can see which words matter)\n\n**Disadvantages:**\n✗ Loses word order (\"dog bit man\" = \"man bit dog\")\n✗ No semantic relationships (\"happy\" vs \"joyful\" treated as completely different)\n✗ All words equally important (doesn't distinguish important from common words)\n✗ Very high dimensionality\n\n**When to use:**\n- Spam detection\n- Topic modeling\n- Simple text classification\n- Document similarity (basic)\n\n### TF-IDF (Term Frequency-Inverse Document Frequency)\n\n**Improvement over BoW:** Weight words by importance\n\n**Mathematical formulation:**\n```\nFor term t in document d within corpus D:\n\nTF(t,d) = count(t,d) / |d|\nwhere:\n  count(t,d) = number of times term t appears in document d\n  |d| = total number of terms in document d\n\nIDF(t,D) = log(|D| / |{d ∈ D : t ∈ d}|)\nwhere:\n  |D| = total number of documents in corpus\n  |{d ∈ D : t ∈ d}| = number of documents containing term t\n\nTF-IDF(t,d,D) = TF(t,d) × IDF(t,D)\n```\n\n**Intuition:**\n```\nTF (Term Frequency):\n  Higher if word appears more in this document\n  \"This document is about cats. Cats are amazing. I love cats.\"\n  → \"cats\" gets high TF score\n\nIDF (Inverse Document Frequency):  \n  Higher if word appears in fewer documents overall\n  Common words like \"the\", \"is\", \"a\" appear everywhere → low IDF\n  Specific words like \"photosynthesis\" appear rarely → high IDF\n\nCombined TF-IDF:\n  High score = word is frequent in this document AND rare overall\n  → Indicates this word is important for characterizing this document\n```\n\n**Example calculation:**\n```\nCorpus: 1000 documents\nDocument: \"The cat sat on the mat. The cat was happy.\"\n\nFor word \"cat\":\n  TF = 2/8 = 0.25 (appears 2 times out of 8 total words)\n  IDF = log(1000/50) = log(20) ≈ 3.0 (assuming \"cat\" appears in 50 documents)\n  TF-IDF = 0.25 × 3.0 = 0.75\n\nFor word \"the\":\n  TF = 3/8 = 0.375 (appears 3 times)  \n  IDF = log(1000/900) = log(1.11) ≈ 0.1 (appears in most documents)\n  TF-IDF = 0.375 × 0.1 = 0.0375\n\nResult: \"cat\" gets much higher score than \"the\"\n```\n\n### Word Embeddings - Semantic Vectors\n\n**Key insight:** Words with similar meanings should have similar representations\n\n#### Word2Vec (2013)\n\n**Core idea:** Learn embeddings from word co-occurrence patterns\n\n**Two algorithms:**\n1. **Skip-gram:** Given center word, predict context words\n2. **CBOW (Continuous Bag of Words):** Given context words, predict center word\n\n**Training example (Skip-gram):**\n```\nSentence: \"The cat sat on the mat\"\nWindow size: 2\n\nTraining pairs:\nInput → Output\n\"cat\" → \"The\"     (context word 2 positions left)\n\"cat\" → \"sat\"     (context word 1 position right)  \n\"sat\" → \"cat\"     (context word 1 position left)\n\"sat\" → \"on\"      (context word 1 position right)\n...\n\nModel learns: words appearing in similar contexts get similar embeddings\n```\n\n**Remarkable property - Semantic arithmetic:**\n```\n**king** - **man** + **woman** ≈ **queen**\n\nExplanation:\n- \"king\" and \"queen\" appear in similar contexts (monarchy)\n- \"man\" and \"woman\" capture gender dimension\n- Vector subtraction removes gender from \"king\"\n- Vector addition applies gender to result\n- Result: \"queen\"\n\nThis algebraic structure wasn't hand-designed!\nIt emerged from learning co-occurrence patterns.\n```\n\n**Typical dimensions:** 300D\n**Training corpus:** Billions of words from Wikipedia, news, web text\n\n#### BERT (2018) - Contextual Embeddings\n\n**Key improvement:** Same word gets different embeddings in different contexts\n\n**Problem Word2Vec couldn't solve:**\n```\nSentence 1: \"I went to the bank to deposit money\"\nSentence 2: \"I sat by the river bank to watch sunset\"\n\nWord2Vec: \"bank\" gets SAME embedding in both sentences\nBERT: \"bank\" gets DIFFERENT embeddings based on context\n```\n\n**Architecture:** Transformer encoder with bidirectional attention\n\n**Training:** Masked Language Modeling\n```\nInput: \"The cat [MASK] on the mat\"\nTask: Predict \"[MASK]\" = \"sat\"\n\nBERT learns to use context from BOTH sides:\n- Left context: \"The cat\"  \n- Right context: \"on the mat\"\n- Combined context suggests \"sat\" is most likely\n```\n\n**Embedding extraction:**\n```\nInput: Tokenize text into [CLS] + words + [SEP]\nProcess: 12 transformer layers (BERT-base) or 24 layers (BERT-large)  \nOutput: Contextual embedding for each token\n\nCommon approaches:\n1. Use [CLS] token embedding (768D) for sentence representation\n2. Average word embeddings for sentence representation\n3. Use specific word embeddings for word-level tasks\n```\n\n#### Modern Large Language Models (2020+)\n\n**GPT series:**\n```\nGPT-1 (2018): 117M parameters, decoder-only\nGPT-2 (2019): 1.5B parameters, \"too dangerous to release\"\nGPT-3 (2020): 175B parameters, few-shot learning\nGPT-4 (2023): Estimated 1T+ parameters, multimodal\n\nProperties:\n  - 12,288D vectors (very high-dimensional)\n  - Captures vast knowledge\n  - Can be used as semantic features\n  - More interpretable than BERT in some ways\n```\n\n## 3.2 Image Representation\n\n### Classical Approaches (Pre-Deep Learning)\n\n#### SIFT (Scale-Invariant Feature Transform)\n\n**Purpose:** Detect and describe local features in images that are invariant to scale, rotation, and illumination\n\n**Process:**\n```\n1. Find keypoints (interest points)\n   - Corners, edges, distinctive regions\n\n2. Describe neighborhoods around keypoints\n   - Direction and magnitude of gradients\n   - Histogram of edge orientations\n\n3. Result: Keypoint descriptor (128D vector)\n   - Invariant to many transformations\n   - Can match same keypoint across images\n```\n\n**Example application:**\n```\nImage 1: Photo of building from front\nImage 2: Photo of same building from side, different lighting\n\nSIFT can find corresponding points:\n- Corner of window in both images\n- Door handle in both images  \n- Logo on building in both images\n\nUse cases:\n- Image stitching (panoramas)\n- Object recognition\n- 3D reconstruction\n```\n\n**Advantages:**\n✓ Mathematically well-understood\n✓ Invariant to common transformations\n✓ Works without training data\n✓ Interpretable features\n\n**Disadvantages:**\n✗ Hand-crafted (not learned from data)\n✗ Limited to certain types of features\n✗ Not end-to-end optimizable\n✗ Slower than modern CNN features\n\n### Deep Learning Approaches\n\n#### Convolutional Neural Networks (CNNs)\n\n**Key insight:** Learn hierarchical features automatically from data\n\n**Convolution operation:**\n```\nMathematical definition:\n(I * K)[i,j] = ΣΣ I[i+m, j+n] × K[m,n]\n               m n\n\nwhere:\nI = input image/feature map\nK = kernel/filter  \n* = convolution operator\n\nInterpretation:\n- Slide kernel across image\n- Compute dot product at each position\n- Result: Feature map showing kernel responses\n```\n\n**Example convolution:**\n```\nInput (5×5):          Kernel (3×3):\n[1 2 3 4 5]          [1  0 -1]\n[2 3 4 5 6]          [1  0 -1]  \n[3 4 5 6 7]          [1  0 -1]\n[4 5 6 7 8]\n[5 6 7 8 9]\n\nOutput (3×3):\n[0  0  0]     # Each value computed as dot product\n[0  0  0]     # of kernel with corresponding image region\n[0  0  0]\n```\n\n**Feature hierarchy:**\n```\nLayer 1 (early): Edge detectors\n  - Vertical edges: [-1 0 1; -1 0 1; -1 0 1]\n  - Horizontal edges: [-1 -1 -1; 0 0 0; 1 1 1]\n  - Diagonal edges: [1 0 -1; 0 0 0; -1 0 1]\n\nLayer 2: Simple patterns\n  - Corners (combination of edges)\n  - Curves  \n  - Textures\n\nLayer 3: Object parts\n  - Eyes, noses (for faces)\n  - Wheels, windows (for cars)\n  - Leaves, branches (for trees)\n\nLayer 4: Full objects\n  - Complete faces\n  - Full cars\n  - Entire trees\n```\n\n#### ResNet (Residual Networks)\n\n**Motivation:** Very deep networks are hard to train\n\n**The problem:**\n```\nIntuition: Deeper = more parameters = better?\nBut: Very deep networks are hard to train!\n\nCause: Gradient vanishing during backpropagation\nBackprop through L layers:\n\n∂Loss/∂θ₁ = ∂Loss/∂h_L × ∂h_L/∂h_{L-1} × ∂h_{L-1}/∂h_{L-2} × ... × ∂h₂/∂h₁ × ∂h₁/∂θ₁\n\nChain rule multiplication: If each ∂h_i/∂h_{i-1} ≈ g < 1:\nFinal gradient ≈ g^L × (initial gradient)\n\nExample with L=100 layers and g=0.9:\n0.9¹⁰⁰ ≈ 0.0000027 (essentially zero!)\n\nResult: Early layers receive almost no gradient signal\n```\n\n**Solution: Skip connections (residual connections)**\n\n**Architecture change:**\n```\nTraditional layer: h_{i+1} = f(h_i)\nResidual layer: h_{i+1} = h_i + f(h_i)\n\nwhere f(h_i) is typically:\nConv → BatchNorm → ReLU → Conv → BatchNorm\n```\n\n**Why this helps:**\n```\nBenefit:\nEven if f(h_i) learns nothing (f(h_i)=0),\nh_{i+1} = h_i still flows information through\n\nGradient paths (using chain rule correctly):\nWithout skip connections:\n  ∂h_{i+1}/∂h_i = f'(h_i)\n\nWith skip connections:  \n  ∂h_{i+1}/∂h_i = ∂(h_i + f(h_i))/∂h_i = 1 + f'(h_i)\n\nThe \"+1\" term provides direct gradient pathway!\n\nThrough L layers:\nWithout skip: gradient ∝ ∏ᵢ f'(h_i) (product of derivatives < 1)\nWith skip: gradient includes terms with ∏ᵢ (1 + f'(h_i)) (always ≥ 1)\n\nThe identity mappings prevent gradient vanishing!\n```\n\n**ResNet architecture example (ResNet-50):**\n```\nInput: Image (224×224×3)\n  ↓\nConv 7×7, stride 2\n→ (112×112×64)\n  ↓\nMaxPool 3×3, stride 2  \n→ (56×56×64)\n  ↓\nStage 1: 3 residual blocks\n→ (56×56×256)\n  ↓\nStage 2: 4 residual blocks  \n→ (28×28×512)\n  ↓\nStage 3: 6 residual blocks\n→ (14×14×1024)\n  ↓\nStage 4: 3 residual blocks\n→ (7×7×2048)\n  ↓\nGlobal Average Pool\n→ (2048,)\n  ↓\nFully Connected\n→ (num_classes,)\n```\n\n**Properties:**\n```\nResNet-50 output:\n- 2048-dimensional feature vector\n- Captures high-level semantic content\n- Pre-trained on ImageNet (1.2M images, 1000 classes)\n- Transfer learning: Works well for new tasks\n```\n\n**Advantages:**\n✓ Much deeper networks possible (50, 101, 152 layers)\n✓ Better performance than shallow networks\n✓ Stable training (deep networks possible)\n✓ Interpretable to some extent (visualize activations)\n✓ Efficient inference\n\n**Disadvantages:**\n✗ Black-box decisions (what does each dimension mean?)\n✗ Requires large labeled datasets to train from scratch\n✗ Inherits biases from ImageNet\n\n**When to use:**\n- Most modern computer vision tasks\n- Transfer learning base\n- Feature extraction for multimodal systems\n\n### Vision Transformer (ViT) - Modern Alternative\n\n**Key idea:** Treat image patches as sequence tokens, apply transformer\n\n**Process:**\n```\n1. Split image into patches (e.g., 16×16 patches)\n   224×224 image → 14×14 = 196 patches\n\n2. Linear projection of each patch\n   16×16×3 = 768D → Linear layer → 768D embedding\n\n3. Add positional embeddings\n   Patch embeddings + position info\n\n4. Transformer encoder  \n   Self-attention across patches\n\n5. Classification token [CLS]\n   Final representation for whole image\n```\n\n**Comparison with CNNs:**\n```\nCNNs:                    ViTs:\n- Inductive bias         - Less inductive bias\n- Local connectivity     - Global attention\n- Translation equivariance - Learned spatial relationships  \n- Smaller datasets OK    - Needs large datasets\n- More efficient         - More computation\n```\n\n## 3.3 Audio Representation\n\n### Traditional Signal Processing\n\n#### Mel-frequency Cepstral Coefficients (MFCC)\n\n**Purpose:** Extract perceptually meaningful features from audio\n\n**Process:**\n```\n1. Pre-emphasis filter\n   Boost high frequencies\n   \n2. Windowing  \n   Split audio into overlapping frames (25ms windows, 10ms step)\n   \n3. FFT (Fast Fourier Transform)\n   Time domain → Frequency domain\n   \n4. Mel filter bank\n   Human auditory perception-based frequency spacing\n   \n5. Logarithm\n   Compress dynamic range\n   \n6. DCT (Discrete Cosine Transform)\n   Decorrelate features\n   \nOutput: Typically 13 MFCC coefficients per frame\n```\n\n**Visual representation:**\n```\nAudio waveform:\nTime: 0----1----2----3----4----5 seconds\nAmplitude: ∼∼∼∼∼∼∼∼∼∼∼∼∼∼∼∼∼∼∼∼\n        ↓\nMFCC features (13 × num_frames):\nFrame: 1    2    3    4    5   ...\nc₁:   [2.1  1.8  2.3  1.9  2.0]\nc₂:   [0.5  0.3  0.7  0.4  0.6]  \nc₃:   [-0.2 0.1 -0.3  0.0 -0.1]\n...\nc₁₃:  [0.8  0.9  0.7  1.0  0.8]\n        ↓\nFinal representation per utterance:\nStatistical summary (mean, std) → 26D vector\nOr sequence of frame vectors for RNN processing\n```\n\n**Example use case:**\n```\nInput: Audio \"Hello\"\n        ↓\nMFCC extraction\n        ↓\nOutput: Text \"Hello\"\n```\n\n**Properties:**\n- Fixed dimensionality (39D total: 13 MFCC + 13 Δ + 13 ΔΔ)\n- Perceptually meaningful\n- Low computational cost\n- Standard for speech tasks\n\n**Advantages:**\n✓ Fast to compute\n✓ Well-understood (40+ years research)\n✓ Works well for speech (the main audio task)\n✓ Low dimensionality\n✓ Perceptually meaningful\n\n**Disadvantages:**\n✗ Not learnable (fixed formula)\n✗ May discard useful information\n✗ Designed specifically for speech\n✗ Not optimal for music or environmental sounds\n\n#### Spectrograms\n\n**Purpose:** Visualize frequency content over time\n\n**Types:**\n```\n1. Linear spectrogram:\n   FFT magnitudes plotted over time\n   Y-axis: Frequency (0 to Nyquist)\n   X-axis: Time\n   Color: Magnitude\n\n2. Log spectrogram:\n   Log-scale frequency axis\n   Better for human perception\n\n3. Mel spectrogram:\n   Mel-scale frequency axis\n   Even better perceptual modeling\n```\n\n**Advantages:**\n✓ Complete frequency information preserved\n✓ Raw frequency content visible\n✓ Can apply deep learning directly\n✓ Works for any audio (not just speech)\n\n**Disadvantages:**\n✗ High dimensionality (harder to process)\n✗ Not perceptually normalized\n✗ Less standard for speech\n\n**When to use:**\n- Music processing and generation\n- Environmental sound classification\n- Any audio task where full frequency content matters\n\n### Modern Deep Learning Approaches\n\n#### Wav2Vec 2.0\n\n**Purpose:** Learn audio representations from raw waveforms\n\n**Architecture:**\n```\nRaw audio waveform\n        ↓\nCNN encoder (6 layers)\n        ↓  \nQuantization module\n        ↓\nTransformer (12 layers)\n        ↓\nContextualized representations (768D per timestep)\n```\n\n**Training:** Self-supervised contrastive learning\n```\n1. Mask portions of audio  \n2. Learn to predict masked regions\n3. Use contrastive loss (similar to BERT for text)\n\nResult: Rich audio representations without labeled data\n```\n\n**Advantages:**\n✓ Learned from data (not hand-crafted)\n✓ Works across different audio domains\n✓ State-of-the-art for many audio tasks\n✓ Can fine-tune for specific tasks\n\n**Disadvantages:**\n✗ Requires large amounts of training data\n✗ Computationally expensive\n✗ Black-box (hard to interpret)\n\n## 3.4 Practical Implementation Examples\n\n### Text Feature Extraction\n\n```python\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\n\nclass TextFeatureExtractor:\n    def __init__(self):\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.model = BertModel.from_pretrained('bert-base-uncased')\n        self.model.eval()\n    \n    def extract_features(self, text):\n        \"\"\"Extract BERT features from text\"\"\"\n        # Tokenize\n        inputs = self.tokenizer(text, return_tensors='pt', \n                              padding=True, truncation=True, max_length=512)\n        \n        # Extract features\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            \n        # Use [CLS] token embedding as sentence representation\n        sentence_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (1, 768)\n        \n        return sentence_embedding.numpy()\n\n# Usage\nextractor = TextFeatureExtractor()\nfeatures = extractor.extract_features(\"A cute cat sitting on a mat\")\nprint(f\"Text features shape: {features.shape}\")  # (1, 768)\n```\n\n### Image Feature Extraction\n\n```python\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nclass ImageFeatureExtractor:\n    def __init__(self):\n        # Load pre-trained ResNet\n        self.model = models.resnet50(pretrained=True)\n        self.model.fc = torch.nn.Identity()  # Remove final classifier\n        self.model.eval()\n        \n        # Standard ImageNet preprocessing\n        self.preprocess = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225]),\n        ])\n    \n    def extract_features(self, image_path):\n        \"\"\"Extract ResNet features from image\"\"\"\n        # Load and preprocess image\n        image = Image.open(image_path).convert('RGB')\n        image_tensor = self.preprocess(image).unsqueeze(0)  # Add batch dimension\n        \n        # Extract features\n        with torch.no_grad():\n            features = self.model(image_tensor)  # Shape: (1, 2048)\n            \n        return features.numpy()\n\n# Usage  \nextractor = ImageFeatureExtractor()\nfeatures = extractor.extract_features(\"cat.jpg\")\nprint(f\"Image features shape: {features.shape}\")  # (1, 2048)\n```\n\n### Audio Feature Extraction\n\n```python\nimport librosa\nimport numpy as np\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\n\nclass AudioFeatureExtractor:\n    def __init__(self):\n        # Traditional MFCC\n        self.sample_rate = 16000\n        \n        # Modern Wav2Vec2\n        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n        self.model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n        self.model.eval()\n    \n    def extract_mfcc(self, audio_path):\n        \"\"\"Extract MFCC features\"\"\"\n        # Load audio\n        audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n        \n        # Extract MFCC (13 coefficients)\n        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n        \n        # Take statistical summary\n        mfcc_mean = np.mean(mfcc, axis=1)  # (13,)\n        mfcc_std = np.std(mfcc, axis=1)    # (13,)\n        \n        return np.concatenate([mfcc_mean, mfcc_std])  # (26,)\n    \n    def extract_wav2vec(self, audio_path):\n        \"\"\"Extract Wav2Vec2 features\"\"\"\n        # Load audio\n        audio, sr = librosa.load(audio_path, sr=16000)\n        \n        # Process audio\n        inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n        \n        # Extract features\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            \n        # Average over time dimension\n        features = outputs.last_hidden_state.mean(dim=1)  # Shape: (1, 768)\n        \n        return features.numpy()\n\n# Usage\nextractor = AudioFeatureExtractor()\nmfcc_features = extractor.extract_mfcc(\"hello.wav\")\nwav2vec_features = extractor.extract_wav2vec(\"hello.wav\")\n\nprint(f\"MFCC features shape: {mfcc_features.shape}\")      # (26,)\nprint(f\"Wav2Vec features shape: {wav2vec_features.shape}\") # (1, 768)\n```\n\n## 3.5 Debugging Feature Extraction\n\n### Common Issues and Solutions\n\n**Issue 1: Features are all zeros or very small**\n```python\ndef debug_features(features, name=\"features\"):\n    print(f\"{name} statistics:\")\n    print(f\"  Shape: {features.shape}\")\n    print(f\"  Min: {features.min():.6f}\")\n    print(f\"  Max: {features.max():.6f}\")\n    print(f\"  Mean: {features.mean():.6f}\")\n    print(f\"  Std: {features.std():.6f}\")\n    print(f\"  Zeros: {(features == 0).sum()} / {features.size}\")\n    \n    if features.std() < 1e-6:\n        print(\"  WARNING: Very low variance - check preprocessing!\")\n    if np.isnan(features).any():\n        print(\"  WARNING: NaN values detected!\")\n    if np.isinf(features).any():\n        print(\"  WARNING: Infinite values detected!\")\n```\n\n**Issue 2: Inconsistent feature scales across modalities**\n```python\ndef normalize_features(features, method='l2'):\n    \"\"\"Normalize features for consistent scale\"\"\"\n    if method == 'l2':\n        # L2 normalization (unit length)\n        norm = np.linalg.norm(features, axis=-1, keepdims=True)\n        return features / (norm + 1e-8)\n    elif method == 'zscore':\n        # Z-score normalization  \n        mean = features.mean(axis=-1, keepdims=True)\n        std = features.std(axis=-1, keepdims=True)\n        return (features - mean) / (std + 1e-8)\n    elif method == 'minmax':\n        # Min-max normalization\n        min_val = features.min(axis=-1, keepdims=True)\n        max_val = features.max(axis=-1, keepdims=True)\n        return (features - min_val) / (max_val - min_val + 1e-8)\n```\n\n**Issue 3: Memory issues with large feature matrices**\n```python\ndef batch_feature_extraction(file_paths, extractor, batch_size=32):\n    \"\"\"Process files in batches to avoid memory issues\"\"\"\n    features = []\n    \n    for i in range(0, len(file_paths), batch_size):\n        batch_paths = file_paths[i:i+batch_size]\n        batch_features = []\n        \n        for path in batch_paths:\n            feat = extractor.extract_features(path)\n            batch_features.append(feat)\n            \n        # Stack batch and free memory\n        batch_features = np.vstack(batch_features)\n        features.append(batch_features)\n        \n        # Progress indicator\n        print(f\"Processed {min(i+batch_size, len(file_paths))}/{len(file_paths)} files\")\n    \n    return np.vstack(features)\n```\n\n## 3.6 Exercises and Projects\n\n**⭐ Beginner:**\n1. Implement BoW and TF-IDF from scratch\n2. Extract MFCC features from audio files\n3. Visualize CNN filter responses on images\n4. Compare different text representations on sentiment analysis\n\n**⭐⭐ Intermediate:**\n5. Fine-tune BERT on domain-specific text\n6. Extract ResNet features and cluster images\n\n**⭐⭐⭐ Advanced:**\n7. Implement self-attention for images (simplified ViT)\n8. Build Wav2Vec2 from scratch (simplified)\n9. Compare different dimensionality reduction techniques\n\n---\n\n## Key Takeaways\n\n- **Text representations** evolved from simple BoW to contextual embeddings (BERT, GPT)\n- **Image features** benefit from hierarchical processing (CNNs) and skip connections (ResNet)\n- **Audio processing** uses both traditional signal processing (MFCC) and modern deep learning (Wav2Vec2)\n- **Feature quality** is crucial for downstream multimodal tasks\n- **Normalization** is essential when combining features from different modalities\n- **Debugging tools** help identify and fix common feature extraction issues\n\n## Further Reading\n\n**Text Representations:**\n- Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space. *arXiv:1301.3781*\n- Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers. *arXiv:1810.04805*\n\n**Computer Vision:**\n- He, K., et al. (2016). Deep Residual Learning for Image Recognition. *CVPR 2016*\n- Dosovitskiy, A., et al. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *ICLR 2021*\n\n**Audio Processing:**\n- Baevski, A., et al. (2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. *NeurIPS 2020*\n\n---\n\n**Previous**: [Chapter 2: Foundations and Core Concepts](chapter-02.md) | **Next**: [Chapter 4: Feature Alignment and Bridging Modalities](chapter-04.md) | **Home**: [Table of Contents](index.md)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-03.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}