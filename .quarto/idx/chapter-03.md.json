{"title":"Chapter 3: Feature Representation for Each Modality","markdown":{"headingText":"Chapter 3: Feature Representation for Each Modality","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 2: Foundations and Core Concepts](chapter-02.md) | **Next**: [Chapter 4: Feature Alignment and Bridging Modalities](chapter-04.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand text representation methods from BoW to BERT\n- Explain CNNs and Vision Transformers for images\n- Describe MFCC and self-supervised learning for audio\n- Compare different modality representations\n- Choose appropriate representations for specific tasks\n\n## 3.1 Text Representation: Evolution and Methods\n\n### Historical Evolution\n\n```\nTimeline of text representation:\n\n1950s-1990s:    Manual feature engineering\n  ↓\n1990s-2000s:    Bag-of-Words, TF-IDF\n  ↓\n2000s-2010s:    Word embeddings (Word2Vec, GloVe)\n  ↓\n2013-2018:      RNN, LSTM, GRU with embeddings\n  ↓\n2017+:          Transformer-based (BERT, GPT)\n  ↓\n2022+:          Large language models (GPT-3, LLaMA)\n  ↓\n2024+:          Multimodal LLMs\n```\n\n### Method 1: Bag-of-Words (BoW)\n\n**Concept:**\nTreat text as unordered collection of words, ignoring sequence and grammar.\n\n**Process:**\n\n```\nInput:     \"The cat sat on the mat\"\n             ↓\nTokenize:  [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n             ↓\nCount:     {\"the\": 2, \"cat\": 1, \"sat\": 1, \"on\": 1, \"mat\": 1}\n             ↓\nVectorize: [2, 1, 1, 1, 1]  (in vocabulary order)\n```\n\n**Formal definition:**\n\n```\nFor vocabulary V = {w_1, w_2, ..., w_N}\nText represented as: x = [c_1, c_2, ..., c_N]\nwhere c_i = count of word w_i in text\n\nDimension = vocabulary size (can be 10,000-50,000)\n```\n\n**Example - Classification:**\n\n```\nTraining data:\n  Text 1: \"I love this movie\" → Label: Positive\n  Text 2: \"This movie is bad\" → Label: Negative\n\nBoW vectors:\n  Text 1: {love: 1, movie: 1, positive words}\n  Text 2: {bad: 1, movie: 1, negative words}\n\nClassifier learns:\n  \"love\" → +positive contribution\n  \"bad\" → +negative contribution\n```\n\n**Advantages:**\n✓ Simple and fast\n✓ Interpretable\n✓ Works surprisingly well for many tasks\n\n**Disadvantages:**\n✗ Loses word order (\"dog bit man\" = \"man bit dog\")\n✗ No semantic relationships (\"happy\" vs \"joyful\" treated as completely different)\n✗ All words equally important (doesn't distinguish important from common words)\n✗ Very high dimensionality\n\n**When to use:**\n- Spam detection\n- Topic modeling\n- Simple text classification\n- When simplicity and speed are priorities\n\n### Method 2: TF-IDF (Term Frequency-Inverse Document Frequency)\n\n**Motivation:**\nBoW treats all words equally. But some words are more informative than others.\n\n**Concept:**\n```\nImportance = (word frequency in document) × (rarity across corpus)\n\nWords appearing everywhere (\"the\", \"is\") get low weight\nWords appearing rarely but specifically (\"CEO\", \"algorithm\") get high weight\n```\n\n**Formal definition:**\n\n```\nTF (Term Frequency):\n  TF(t,d) = count(t in d) / total_words(d)\n  Normalized frequency of term t in document d\n\nIDF (Inverse Document Frequency):\n  IDF(t) = log(total_documents / documents_containing_t)\n  How rare is this term across all documents?\n\nTF-IDF:\n  TF-IDF(t,d) = TF(t,d) × IDF(t)\n```\n\n**Example calculation:**\n\n```\nCorpus: 1,000 documents\nTerm \"cat\": appears in 100 documents, 5 times in document D\n\nTF = 5 / total_words_in_D = 0.05\nIDF = log(1000/100) = log(10) = 1.0\nTF-IDF = 0.05 × 1.0 = 0.05\n\nCompare to:\nTerm \"the\": appears in 900 documents, 50 times in document D\n\nTF = 50 / total_words_in_D = 0.50\nIDF = log(1000/900) = log(1.11) ≈ 0.1\nTF-IDF = 0.50 × 0.1 = 0.05\n\nWait, same score! That's the point - importance normalized.\n```\n\n**Benefits over BoW:**\n✓ Handles different document lengths better\n✓ Downweights common words\n✓ Emphasizes distinctive terms\n\n**Disadvantages:**\n✗ Still ignores word order\n✗ No semantic understanding\n✗ Requires corpus statistics\n✗ Doesn't handle synonyms\n\n**When to use:**\n- Information retrieval and search\n- TF-IDF is foundation of many search engines\n- Document classification\n- When you have many documents and limited compute\n\n### Method 3: Word2Vec - Learning Word Meaning\n\n**Revolutionary idea (Mikolov et al., 2013):**\n\"Words with similar contexts have similar meanings\"\n\n**Learning through prediction:**\n\n```\nIdea: If we can predict context words from a word,\n      we've learned what that word means.\n\nProcess:\n\nText: \"The dog barked loudly at the mailman\"\n              ↓\nFocus on \"barked\", predict context:\n  Context: {dog, loudly, at, the}\n  Prediction task: Given \"barked\", predict these\n\nLoss: How well did we predict?\n  If good prediction → \"barked\" representation is good\n  If poor → Update \"barked\" vector\n\nAfter training on millions of sentences:\n  \"barked\" vector captures:\n  - Associated with actions\n  - Related to animals\n  - Past tense\n  - Physical events\n```\n\n**Key discovery:**\n\n```\nVector arithmetic works!\n\nking - man + woman ≈ queen\n\nExplanation:\n- \"king\" and \"queen\" appear in similar contexts (monarchy)\n- \"man\" and \"woman\" capture gender dimension\n- Vector subtraction removes gender from \"king\"\n- Vector addition applies gender to result\n- Result: \"queen\"\n\nThis algebraic structure wasn't hand-designed!\nIt emerged from learning word contexts.\n```\n\n**Technical details - Two approaches:**\n\n**Skip-gram:**\n```\nInput: Target word \"barked\"\nTask: Predict context words {dog, loudly, at, the}\n\nModel: Two embedding matrices\n  Input embedding: What is \"barked\"?\n  Output embedding: What patterns lead to context?\n\nOptimization:\n  Maximize: P(context | barked)\n  Network learns useful representations\n```\n\n**CBOW (Continuous Bag of Words):**\n```\nInput: Context words {the, dog, barked, loudly}\nTask: Predict center word\n\nReverse of skip-gram\nCan be faster to train\n```\n\n**Properties:**\n- Fixed embedding per word (doesn't handle polysemy)\n- 300D vectors typical\n- Can be trained on unlabeled data\n- Transferable to downstream tasks\n\n**Example - Semantic relationships:**\n\n```\ncos_sim(king, queen) ≈ 0.7   (high, related)\ncos_sim(king, man) ≈ 0.65     (high, overlapping)\ncos_sim(queen, woman) ≈ 0.68  (high, overlapping)\ncos_sim(king, dog) ≈ 0.2      (low, unrelated)\n\nStructure emerges in embedding space!\n```\n\n**Limitations:**\n✗ One vector per word (ignores context and polysemy)\n✗ \"Bank\" (financial) and \"bank\" (river) have identical vectors\n✗ Same word might mean different things in different contexts\n✗ Doesn't capture longer-range dependencies\n\n**When to use:**\n- Quick baseline for text tasks\n- When you need interpretable word relationships\n- Transfer learning where only word similarity needed\n- When computational resources are limited\n\n### Method 4: BERT - Context-Aware Embeddings\n\n**Motivation:**\n\nWord2Vec limitation - context blindness:\n\n```\nSentence 1: \"I went to the bank to deposit money\"\nSentence 2: \"I sat on the bank of the river\"\n\nWord2Vec:\n  \"bank\" in both sentences → IDENTICAL vector\n  Problem: Different meanings!\n\nWhat we need:\n  Context-aware \"bank\" for finance sentence\n  Different context-aware \"bank\" for river sentence\n```\n\n**BERT Innovation (Devlin et al., 2018):**\n\"Use entire sentence context to generate embeddings\"\n\n**Architecture overview:**\n\n```\nInput text: \"The cat sat on the mat\"\n             ↓\nTokenization (using WordPiece):\n  [CLS] The cat sat on the mat [SEP]\n             ↓\nEmbedding:\n  - Token embedding (which word)\n  - Position embedding (where in sequence)\n  - Segment embedding (which sentence)\n             ↓\nTransformer encoder (12 layers):\n  Each layer:\n    - Self-attention (how relevant is each token to others)\n    - Feed-forward network\n    - Normalization\n             ↓\nOutput: 12 vectors of 768D each\n  Each token has representation influenced by entire sequence\n```\n\n**Key innovation - Bidirectional context:**\n\n```\nTraditional RNN: Left-to-right only\n  Input: \"The cat sat...\"\n         Process: The → cat → sat\n         When processing \"sat\", don't know what comes after\n\nBERT: Bidirectional\n  Input: \"The cat sat on the mat\"\n         Process: Entire sequence simultaneously\n         All positions see all other positions\n         Through self-attention in first layer\n```\n\n**Training procedure - Masked Language Modeling:**\n\n```\nGoal: Learn good representations for any language task\n\nMethod: Predict masked words\n\nOriginal:      \"The [MASK] sat on the mat\"\nTask:          Predict the masked word\nExpected:      \"cat\"\n\nTraining:\n  ① Randomly mask 15% of tokens\n  ② Model predicts masked tokens\n  ③ Loss = cross-entropy between predicted and actual\n  ④ Update all parameters\n\nResult:\n  Model learns representations that contain\n  information about what words should appear\n  = learns semantic and syntactic patterns\n```\n\n**Using BERT embeddings:**\n\n```\nFor sentence classification:\n  ① Process sentence through BERT\n  ② Extract [CLS] token (special classification token)\n  ③ [CLS] vector = sentence representation (768D)\n  ④ Add linear classifier on top\n  ⑤ Train classifier on downstream task\n\nFor token classification (e.g., NER):\n  ① Process sentence through BERT\n  ② Extract all token vectors (each is 768D)\n  ③ Each token has context-aware representation\n  ④ Add classifier for each token\n  ⑤ Predict label for each token\n\nBenefit:\n  - No task-specific feature engineering needed\n  - Transfer learning from massive pre-training\n  - Strong performance on small datasets\n```\n\n**Concrete example - Polysemy handling:**\n\n```\nSentence 1: \"I went to the bank to deposit money\"\n  \"bank\" → BERT embedding with finance context\n\nSentence 2: \"I sat on the bank of the river\"\n  \"bank\" → BERT embedding with geography context\n\nDifferent embeddings!\nBERT captures context from surrounding words\n```\n\n**Properties:**\n- Context-dependent embeddings\n- 768D vectors (BERT-base)\n- Larger versions available (BERT-large: 1024D)\n- Pre-trained on 3.3B words\n- Extremely effective for transfer learning\n\n**Advantages over Word2Vec:**\n✓ Handles polysemy (same word, different contexts)\n✓ Bidirectional context\n✓ Pre-trained on massive corpus\n✓ Strong transfer learning\n✓ Achieves SOTA on many tasks\n\n**Disadvantages:**\n✗ Computationally expensive\n✗ Slower inference than Word2Vec\n✗ Requires more compute resources\n✗ Less interpretable (768D vectors hard to understand)\n\n**When to use:**\n- Text classification (sentiment, topic)\n- Named entity recognition\n- Question answering\n- Semantic similarity\n- When accuracy more important than speed\n- When GPU resources available\n\n### Method 5: Large Language Models (LLMs)\n\n**Further evolution - GPT family:**\n\n```\nBERT (2018):        Encoder-only, bidirectional\nGPT (2018):         Decoder-only, left-to-right\nGPT-2 (2019):       1.5B parameters\nGPT-3 (2020):       175B parameters - in-context learning\nGPT-4 (2023):       ~1.76T parameters - multimodal\n```\n\n**LLM representations:**\n\n```\nGPT-3 embeddings:\n  Layer 1:    Basic patterns\n  Layer 16:   Mid-level concepts\n  Layer 32:   High-level semantics\n  Layer 48 (final): Task-specific representations\n\nProperties:\n  - 12,288D vectors (very high-dimensional)\n  - Captures vast knowledge\n  - Can be used as semantic features\n  - More interpretable than BERT in some ways\n```\n\n**Using LLM embeddings for multimodal tasks:**\n\n```\nInstead of using fixed word embeddings,\nuse representations from large language models\n\nBenefit:\n  - Captures world knowledge from pre-training\n  - Understands complex semantics\n  - Better for rare/unusual concepts\n  - Can be adapted to specific domains\n\nCost:\n  - Expensive API calls (if using services like OpenAI)\n  - Privacy concerns (data sent to external servers)\n  - Latency (requires API round-trip)\n```\n\n**Comparison of text representations:**\n\n```\nMethod          Dimension   Context-aware   Speed   Pre-training\n────────────────────────────────────────────────────────────────\nBoW             10K-50K     No              Fast    None needed\nTF-IDF          10K-50K     No              Fast    Corpus stats\nWord2Vec        300         No              Fast    Large corpus\nGloVe           300         No              Fast    Large corpus\nFastText        300         No              Fast    Large corpus\nELMo            1024        Yes             Slow    Large corpus\nBERT            768         Yes             Medium  Huge corpus\nRoBERTa         768         Yes             Medium  Huge corpus\nGPT-2           1600        Yes             Slow    Huge corpus\nGPT-3           12288       Yes             Very slow API\n```\n\n## 3.2 Image Representation: From Pixels to Concepts\n\n### Historical Evolution\n\n```\nTimeline:\n\n1980s-1990s:    Edge detection (Canny, Sobel)\n  ↓\n1990s-2000s:    Hand-crafted features (SIFT, HOG)\n  ↓\n2012:           AlexNet - Deep learning breakthrough\n  ↓\n2014:           VGGNet, GoogleNet\n  ↓\n2015:           ResNet - Skip connections, very deep networks\n  ↓\n2020:           Vision Transformer - Attention-based vision\n  ↓\n2024:           Large multimodal models processing images\n```\n\n### Method 1: Hand-Crafted Features\n\n**SIFT (Scale-Invariant Feature Transform)**\n\n```\nProblem solved:\n  \"Find the same building in photos taken at different times,\n   different angles, different zoom levels\"\n\nSIFT features are invariant to:\n  - Translation (where object is in image)\n  - Scaling (zoom level)\n  - Rotation (camera angle)\n  - Illumination (lighting changes)\n\nProcess:\n  1. Find keypoints (interest points)\n     - Corners, edges, distinctive regions\n\n  2. Describe neighborhoods around keypoints\n     - Direction and magnitude of gradients\n     - Histogram of edge orientations\n\n  3. Result: Keypoint descriptor (128D vector)\n     - Invariant to many transformations\n     - Can match same keypoint across images\n\nExample:\n  Building in Photo 1 (summer, noon, straight angle)\n  Same building in Photo 2 (winter, sunset, aerial view)\n\n  SIFT can find matching keypoints!\n  Enables: Panorama stitching, 3D reconstruction\n```\n\n**HOG (Histogram of Oriented Gradients)**\n\n```\nKey insight:\n  Human shape recognition relies on edge directions\n  (Horizontal edges on top = head, vertical on sides = body)\n\nProcess:\n  1. Divide image into cells (8×8 pixels)\n\n  2. For each cell:\n     - Compute edge direction at each pixel\n     - Create histogram of edge directions\n\n  3. Result: Concatenate all histograms\n     - Captures shape and edge structure\n     - Dimension: ~3,780 for 64×128 image\n\nApplication:\n  Pedestrian detection\n  - HOG captures distinctive human silhouette\n  - Works well because human shape is distinctive\n  - Fast computation (no deep learning needed)\n\n  Limitation:\n  - Only works for rigid objects (humans, faces)\n  - Fails for abstract categories\n```\n\n**Bag-of-Visual-Words**\n\n```\nIdea: Apply Bag-of-Words concept to images\n\nProcess:\n  1. Extract SIFT features from image\n     → Get 100-1000 keypoint descriptors per image\n\n  2. Cluster descriptors (k-means)\n     → Create \"visual vocabulary\" (e.g., 1000 clusters)\n     → Each cluster = one \"visual word\"\n\n  3. Histogram of visual words\n     → Count which words appear in image\n     → Result: Bag-of-words vector\n\n  4. Classify or compare based on histogram\n\nExample:\n  Image 1 has: {30 \"corner edges\", 20 \"smooth curves\", ...}\n  Image 2 has: {5 \"corner edges\", 45 \"smooth curves\", ...}\n\n  More curve words → Perhaps a cat\n  More corner words → Perhaps a building\n```\n\n**Advantages of hand-crafted features:**\n✓ Interpretable (understand what they measure)\n✓ Fast computation\n✓ Works with small datasets\n✓ Explicit mathematical basis\n\n**Disadvantages:**\n✗ Requires domain expertise to design\n✗ Limited to specific feature types\n✗ Poor generalization to new domains\n✗ Cannot capture complex semantic patterns\n✗ Manually chosen → not optimized for task\n\n**When to use:**\n- When you understand the specific patterns to detect\n- Limited computational resources\n- Small datasets\n- Tasks where hand-crafted features are well-suited (e.g., pedestrian detection)\n\n### Method 2: CNNs - Automatic Feature Learning\n\n**The Breakthrough (AlexNet, 2012):**\n\n```\nRevolutionary insight:\n  \"Stop hand-crafting features!\n   Let neural networks learn what's important.\"\n\nResults:\n  ImageNet competition:\n  - 2011 (hand-crafted): 25.8% error\n  - 2012 (AlexNet): 15.3% error  ← 38% error reduction!\n  - 2015 (ResNet): 3.6% error   ← Human-level performance\n```\n\n**Hierarchical Feature Learning:**\n\n```\nRaw image (224×224×3 pixels)\n        ↓\nLayer 1-2: Low-level features\n  - Edge detection\n  - Simple curves\n  - Corners\n  └─→ What: Detects local patterns\n      Why: Edges are building blocks\n      Output: 64 feature maps (32×32)\n\nLayer 3-4: Mid-level features\n  - Textures\n  - Shapes\n  - Parts\n  └─→ What: Combines local patterns\n      Why: Shapes emerge from edges\n      Output: 256 feature maps (16×16)\n\nLayer 5: High-level features\n  - Objects\n  - Semantic concepts\n  - Scene context\n  └─→ What: Object detectors\n      Why: Objects are concepts\n      Output: 512 feature maps (8×8)\n\nGlobal pooling & Dense layers:\n  - Aggregate spatial info\n  - Predict class probabilities\n  └─→ Output: Class predictions\n```\n\n**Why CNNs work:**\n\n```\n1. Inductive bias toward images\n   - Local connectivity: Nearby pixels related\n   - Shared weights: Same pattern recognized anywhere\n   - Translation invariance: \"Cat is a cat\" whether left/right\n\n2. Hierarchical composition\n   - Edges → Shapes → Objects\n   - Matches how we see\n\n3. Parameter sharing\n   - Filters reused across space\n   - Reduces parameters vs fully connected\n   - Enables learning on larger images\n```\n\n**Key architecture - ResNet (Residual Networks):**\n\n```\nProblem with deep networks:\n  Deeper = more parameters = better?\n  But: Very deep networks are hard to train!\n\n  Cause: Gradient vanishing\n    Backprop through 100 layers:\n    gradient = g₁ × g₂ × g₃ × ... × g₁₀₀\n\n    If each gᵢ = 0.9:\n    0.9¹⁰⁰ ≈ 0.0000027  (essentially zero!)\n\n    Can't learn early layers\n\nSolution: Skip connections (residual connections)\n\nNormal layer: y = f(x)\nResidual layer: y = x + f(x)\n\nBenefit:\n  Even if f(x) learns nothing (f(x)=0),\n  y = x still flows information through\n\n  Gradient paths:\n  Without skip: gradient = ∂f/∂x × ∂f/∂x × ...\n  With skip: gradient = ... + 1 + 1 + ...\n\n  The \"+1\" terms prevent vanishing!\n```\n\n**ResNet architecture example (ResNet-50):**\n\n```\nInput: Image (224×224×3)\n  ↓\nConv 7×7, stride 2\n→ (112×112×64)\n  ↓\nMaxPool 3×3, stride 2\n→ (56×56×64)\n  ↓\nResidual Block 1: [16 conv blocks]\n→ (56×56×256)\n  ↓\nResidual Block 2: [33 conv blocks]\n→ (28×28×512)\n  ↓\nResidual Block 3: [36 conv blocks]\n→ (14×14×1024)\n  ↓\nResidual Block 4: [3 conv blocks]\n→ (7×7×2048)\n  ↓\nAverage Pool\n→ (2048,)\n  ↓\nLinear layer (1000 classes)\n→ Predictions\n\nTotal parameters: 25.5M\nDepth: 50 layers\nPerformance: 76% ImageNet top-1 accuracy\n```\n\n**Properties:**\n- 2048D global feature vector (before classification)\n- Pre-trained on ImageNet (1.4M images)\n- Can fine-tune on downstream tasks\n- Very stable training (skip connections)\n\n**Advantages:**\n✓ Learns task-relevant features\n✓ Transfers well to other tasks\n✓ Stable training (deep networks possible)\n✓ Interpretable to some extent (visualize activations)\n✓ Efficient inference\n\n**Disadvantages:**\n✗ Black-box decisions (what does each dimension mean?)\n✗ Requires large labeled datasets to train from scratch\n✗ Inherits biases from ImageNet\n\n**When to use:**\n- Most modern computer vision tasks\n- Transfer learning (fine-tune on new task)\n- When you want strong off-the-shelf features\n- Production systems (mature, optimized, proven)\n\n### Method 3: Vision Transformers (ViT)\n\n**Paradigm shift (Dosovitskiy et al., 2020):**\n\n```\nTraditional thinking:\n  \"Images need CNNs!\"\n  Reason: Spatial structure, translational equivariance\n\nViT question:\n  \"What if we just use Transformers like NLP?\"\n  Insight: Pure attention can learn spatial patterns\n\nResult:\n  Vision Transformer outperforms ResNet\n  When trained on large datasets!\n```\n\n**Architecture:**\n\n```\nInput image (224×224×3)\n        ↓\nDivide into patches (16×16)\n        ↓\n14×14 = 196 patches\n        ↓\nEach patch: 16×16×3 = 768D\n        ↓\nLinear projection\n        ↓\n196 vectors of 768D\n        ↓\nAdd positional encoding\n(so model knows spatial position)\n        ↓\nAdd [CLS] token\n(like BERT for images)\n        ↓\nTransformer encoder (12 layers)\n        ↓\nExtract [CLS] token\n        ↓\n768D image representation\n```\n\n**How it works:**\n\n```\nKey insight: Patches are like words\n\nIn NLP:\n  Word tokens → Transformer → Semantic relationships\n\nIn ViT:\n  Image patches → Transformer → Spatial relationships\n\nLayer 1:\n  Each patch attends to all other patches\n  Learns: Which patches are related?\n\nLayer 2-12:\n  Progressively integrate information\n  Layer 6: Coarse spatial understanding\n  Layer 12: Fine-grained semantic understanding\n```\n\n**Why this works:**\n\n1. **Global receptive field from Layer 1**\n\n   CNN needs many layers to see globally\n   ViT sees all patches from first layer\n   Enables faster learning of global patterns\n\n2. **Flexible to patches**\n\n   Can use any patch size\n   Trade-off:\n   - Larger patches (32×32): Fewer tokens, less detail\n   - Smaller patches (8×8): More tokens, finer detail\n\n3. **Scales with data**\n\n   CNNs strong with small data (inductive biases)\n   ViT weak with small data, strong with large\n\n   Modern datasets massive\n   → ViT wins\n\n**Example - ViT-Base vs ResNet-50:**\n\n```\n                ViT-Base       ResNet-50\n────────────────────────────────────\nParameters      86M            25.5M\nImageNet acc    77.9%          76%\nTraining data   1.4M+JFT      1.4M\nPre-training    224×224        1000×1000\nFine-tuning     Excellent      Good\n\nInterpretation:\n  ViT needs more data to train\n  But then performs better\n  Especially when transferring to new tasks\n```\n\n**Advantages:**\n✓ Better scaling properties\n✓ Transfers better to downstream tasks\n✓ Simpler architecture (no CNN-specific tricks needed)\n✓ More interpretable (attention patterns show what matters)\n✓ Unified with NLP (same architecture for both)\n\n**Disadvantages:**\n✗ Worse with small datasets\n✗ Requires more computation than CNN equivalents\n✗ Training unstable (needs careful tuning)\n✗ Slower inference in some hardware\n\n**When to use:**\n- Large-scale applications\n- Transfer learning to new visual tasks\n- When computational resources abundant\n- When interpretability matters (attention visualization)\n- New research (faster progress with transformers)\n\n**Attention visualization:**\n\n```\nFor each query patch, show which patches it attends to\n\nExample - Query at cat's head position:\n\nAttention heatmap:\n[   0    0    0  ]\n[   0   0.9   0.8]  (high attention to nearby patches)\n[   0    0.6   0  ]\n\nShows:\n- Model focuses on cat head region\n- Attends to surrounding patches (context)\n- Ignores background regions\n```\n\n## 3.3 Audio Representation: From Waveforms to Features\n\n### Method 1: MFCC (Mel-Frequency Cepstral Coefficients)\n\n**Principle:**\n\"Extract features that match human hearing, not physics\"\n\n**Why needed:**\n\n```\nRaw audio at 16kHz:\n  1 second = 16,000 samples\n  10 seconds = 160,000 samples\n\nProblem:\n  Too many numbers to process\n  Not perceptually relevant (e.g., 16kHz vs 16.1kHz)\n\nSolution:\n  Extract ~39 MFCCs per frame (25ms)\n  Much more compact and perceptually meaningful\n```\n\n**Extraction process step-by-step:**\n\n```\n① Raw waveform\n   Sample audio: 16kHz, mono\n   Duration: 10 seconds\n\n② Pre-emphasis\n   Amplify high frequencies\n   Reason: High frequencies carry important information\n   Filter: y[n] = x[n] - 0.95*x[n-1]\n\n③ Frame division\n   Split into overlapping frames\n   Frame length: 25ms = 400 samples\n   Hop size: 10ms\n   Result: ~980 frames for 10-second audio\n\n④ Window each frame\n   Apply Hamming window: reduces edge artifacts\n\n⑤ Fourier Transform (FFT)\n   Convert time domain → frequency domain\n   For each frame: 400 samples → 200 frequency bins\n\n⑥ Mel-scale warping\n   Map frequency to Mel scale (human perception)\n\n   Linear frequency: 125Hz, 250Hz, 500Hz, 1000Hz, 2000Hz\n   Mel frequency:     0Mel,   250Mel, 500Mel, 1000Mel, 1700Mel\n\n   Why?\n   Humans more sensitive to low frequencies\n   High frequencies sound similar to each other\n   (1000Hz difference matters less at 10,000Hz)\n\n⑦ Logarithm\n   Human loudness perception is logarithmic\n   log(power) more perceptually uniform than power\n\n⑧ Discrete Cosine Transform (DCT)\n   Decorrelate the Mel-scale powers\n   Result: Typically 13-39 coefficients\n\nResult: MFCC vector\n  Dimensions: 39 (or 13, 26 depending on config)\n  One vector per 10ms\n  Represents spectral shape at that time\n```\n\n**Visualization:**\n\n```\nRaw waveform:          Spectrogram:           MFCCs:\nAmplitude              Frequency vs Time      Features vs Time\n   ↑                      High ▲               ↑\n   │ ~~~~               ▓▓▓▓▓│▓▓▓          ▓▓▓│▓▓▓\n   │~  ~  ~  ~~       ▓▓▓  │▓▓▓          ▓▓ │▓▓\n   │ ~ ~~  ~ ~       ▓▓   │▓           ▓  │▓\n   └──────────→      ▓▓    │            ▓  │\n   Time (s)         Low ▼  └─────────→ Coeff│\n                         Time (s)         └─→\n                                        Dim 1-39\n```\n\n**Example - Speech recognition:**\n\n```\nAudio: \"Hello\"\n        ↓\nMFCC extraction (39D per frame)\n        ↓\n10 frames of audio (each 10ms):\n  Frame 1: [0.2, -0.1, 0.5, ..., 0.3] (39D)\n  Frame 2: [0.21, -0.08, 0.52, ..., 0.31] (39D)\n  ...\n  Frame 10: [0.15, -0.12, 0.45, ..., 0.25] (39D)\n        ↓\nSequence of MFCCs: 10×39 matrix\n        ↓\nFeed to speech recognition model\n        ↓\nOutput: Text \"Hello\"\n```\n\n**Properties:**\n- Fixed dimensionality (39D)\n- Perceptually meaningful\n- Low computational cost\n- Standard for speech tasks\n\n**Advantages:**\n✓ Fast to compute\n✓ Well-understood (40+ years research)\n✓ Works well for speech (the main audio task)\n✓ Low dimensionality\n✓ Perceptually meaningful\n\n**Disadvantages:**\n✗ Not learnable (fixed formula)\n✗ May discard useful information\n✗ Optimized for speech, not music\n✗ Doesn't handle music well\n\n**When to use:**\n- Speech recognition\n- Speaker identification\n- Emotion recognition from speech\n- Music genre classification (acceptable)\n- Limited compute resources\n\n### Method 2: Spectrogram\n\n**Alternative to MFCC:**\nKeep all frequency information, don't apply Mel-scale or DCT.\n\n**Process:**\n\n```\n① Raw audio\n② Frame division\n③ FFT\n④ Magnitude spectrum\n⑤ Spectrogram: stacked magnitude spectra over time\n\nResult: 2D matrix\n  Dimensions: Time × Frequency\n  Values: Power at each time-frequency bin\n\nExample: 10-second audio at 16kHz\n  Time: 980 frames\n  Frequency: 513 bins\n  Size: 980×513\n```\n\n**Visualization:**\n\n```\nSpectrogram of \"Hello\":\n\nFrequency\n(Hz)    |▓▓ ▓▓▓▓    ▓▓    |\n        |▓▓▓▓▓▓▓  ▓▓▓▓▓▓ | High freq\n        |  ▓▓▓▓▓▓▓▓▓▓▓▓  |\n  8000  |─────────────────|\n        | ▓▓▓▓ ▓▓▓▓▓  ▓▓  |\n        |▓▓▓▓ ▓▓▓▓▓▓▓▓▓   |\n        |▓▓ ▓ ▓▓▓▓▓ ▓▓    | Low freq\n    0   |___________________|\n        0    2    4    6    8    10\n              Time (seconds)\n\nDarker = higher power\nDifferent time positions → different audio\n```\n\n**Advantages over MFCC:**\n✓ More information preserved\n✓ Raw frequency content visible\n✓ Can apply deep learning directly\n✓ Works for any audio (not just speech)\n\n**Disadvantages:**\n✗ High dimensionality (harder to process)\n✗ Not perceptually normalized\n✗ Less standard for speech\n\n**When to use:**\n- Music processing and generation\n- Sound event detection\n- When using deep learning (CNN/Transformer)\n- When frequency content important\n\n### Method 3: Wav2Vec2 - Self-Supervised Learning\n\n**Modern approach (Meta AI, 2020):**\n\n```\nProblem:\n  Need thousands of hours transcribed audio for ASR\n  Transcription is expensive\n\nSolution:\n  Learn from UNLABELED audio\n  Use self-supervised learning\n```\n\n**Training mechanism:**\n\n```\nPhase 1: Pretraining (on unlabeled data)\n\n  ① Feature extraction (CNN)\n     Raw waveform → discrete codes\n\n     Intuition: Compress speech to meaningful units\n\n  ② Contrastive loss\n     Predict masked codes from context\n     Similar to BERT for speech\n\n  Result: Model learns speech patterns\n          Without any transcriptions!\n\nPhase 2: Fine-tuning (with small labeled dataset)\n\n  ① Load pretrained model\n  ② Add task-specific head (classification)\n  ③ Train on labeled examples\n\n  Benefit: Needs much less labeled data!\n```\n\n**Quantization step:**\n\n```\nWhy quantize speech?\n\nRaw features: Continuous values\nProblem: Too flexible, model can memorize\n\nQuantized features: Discrete codes (e.g., 1-512)\nBenefit:\n  - Reduces search space\n  - Forces learning of essential patterns\n  - Similar to VQ-VAE for images\n\nExample:\n  Raw feature: [0.234, -0.512, 0.891, ...]\n  ↓ (vector quantization)\n  Nearest code ID: 147\n\n  Code vector: Learned codebook entry 147\n```\n\n**Architecture:**\n\n```\nRaw waveform (16kHz)\n        ↓\nCNN feature extraction\n        ↓\nQuantization to codes\n        ↓\nTransformer encoder (contextual understanding)\n        ↓\n768D representation per frame\n```\n\n**Training details:**\n\n```\nObjective:\n  Predict masked codes from surrounding codes\n\n  Input: [code_1, [MASK], code_3, [MASK], code_5]\n  Task: Predict masked codes\n\n  Loss: Contrastive - predict correct code among negatives\n\nResult:\n  Encoder learns to represent speech meaningfully\n  Ready for downstream tasks\n```\n\n**Fine-tuning for tasks:**\n\n```\nTask 1: Speech Recognition (ASR)\n  Add: Linear layer for character/phoneme classification\n  Train: On (audio, transcription) pairs\n\n  Data needed: 10-100 hours labeled\n  Without pretraining: 10,000+ hours needed!\n\nTask 2: Speaker Identification\n  Add: Linear layer for speaker classification\n  Train: On (audio, speaker_id) pairs\n\nTask 3: Emotion Recognition\n  Add: Linear layer for emotion classification\n  Train: On (audio, emotion) pairs\n```\n\n**Empirical results:**\n\n```\nWithout Wav2Vec2 pretraining:\n  ASR with 100 hours data: 25% WER (Word Error Rate)\n\nWith Wav2Vec2 pretraining:\n  ASR with 100 hours data: 10% WER\n  ASR with 10 hours data: 12% WER\n\nImprovement:\n  50% error reduction with same data\n  Or 10× less labeled data for same performance\n```\n\n**Properties:**\n- 768D representation per frame\n- Learned from unlabeled data\n- Transferable across tasks\n- Works for any audio\n\n**Advantages:**\n✓ Leverages massive unlabeled data\n✓ Strong transfer learning\n✓ Handles diverse audio types\n✓ Better than MFCC for complex tasks\n\n**Disadvantages:**\n✗ Complex training procedure\n✗ Requires large unlabeled dataset for pretraining\n✗ Longer inference than MFCC\n\n**When to use:**\n- Speech recognition (SOTA approach)\n- Multi-speaker systems\n- Low-resource languages\n- When accuracy is critical\n\n## 3.4 Comparison and Selection Guide\n\n### Dimension and Computational Cost\n\n```\n                Dimension   Speed       Training Data\n────────────────────────────────────────────────────\nMFCC            39          Very fast   Hundreds hours\nSpectrogram     513         Fast        Thousands hours\nWav2Vec2        768         Slow        Millions hours unlabeled\n\nHand-crafted    1000-5000   Fast        Medium\nSIFT            128/keypoint Fast       Medium\nHOG             3780        Fast        Medium\n\nResNet50        2048        Medium      1.4M images\nViT-Base        768         Medium      14M images\nBERT            768         Medium      3.3B words\nGPT-3           12288       Slow        Huge\n```\n\n### Modality Comparison Summary\n\n```\n                Text            Image           Audio\n────────────────────────────────────────────────────\nModern rep.     BERT/GPT        ResNet/ViT      Wav2Vec2\nDimension       768             2048/768        768\nInterpretable   Somewhat        Little          Very little\nSpeed           Medium          Fast            Medium\nPre-training    Easy (text web) Requires labels Can be unsupervised\nTransfer        Excellent       Good            Good\nMultimodal fit  Good            Excellent       Good\n```\n\n### Choosing Representation\n\n**Decision flowchart:**\n\n```\nIs computational budget limited?\n  YES → Use hand-crafted or MFCC\n  NO → Continue\n       ↓\nIs this a production system?\n  YES → Use proven methods (ResNet, BERT)\n  NO → Continue\n       ↓\nDo you have massive labeled data?\n  YES → Consider training from scratch\n  NO → Use pre-trained features\n       ↓\nDo you have unlabeled data?\n  YES → Consider self-supervised (Wav2Vec2)\n  NO → Use supervised pre-trained models\n```\n\n## Key Takeaways\n\n- **Text:** Evolution from BoW to BERT shows power of context\n- **Images:** CNNs dominate but ViT shows promising future\n- **Audio:** MFCC traditional, Wav2Vec2 is modern frontier\n- **Pre-training is key:** Leveraging unlabeled data essential\n- **Different modalities need different approaches**\n- **Trade-offs exist:** accuracy vs speed, interpretability vs performance\n\n## Exercises\n\n**⭐ Beginner:**\n1. Implement TF-IDF from scratch\n2. Extract MFCC features from an audio file\n3. Visualize a spectrogram\n\n**⭐⭐ Intermediate:**\n4. Compare MFCC vs spectrogram representations\n5. Fine-tune BERT on text classification\n6. Extract ResNet features and cluster images\n\n**⭐⭐⭐ Advanced:**\n7. Implement self-attention for images (simplified ViT)\n8. Build Wav2Vec2 from scratch (simplified)\n9. Compare different dimensionality reduction techniques\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-03.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}