{"title":"Chapter 4: Feature Alignment and Bridging Modalities","markdown":{"headingText":"Chapter 4: Feature Alignment and Bridging Modalities","containsRefs":false,"markdown":"\n---\n\n**Previous**: [Chapter 3: Feature Representation for Each Modality](chapter-03.md) | **Next**: [Chapter 5: Fusion Strategies](chapter-05.md) | **Home**: [Table of Contents](index.md)\n\n---\n\n## Learning Objectives\n\nAfter reading this chapter, you should be able to:\n- Understand why alignment is necessary\n- Implement shared embedding spaces\n- Use cross-attention for fine-grained alignment\n- Handle bidirectional alignment\n- Solve alignment in practice\n\n## 4.1 The Alignment Problem\n\n### Why Alignment Matters\n\n**The Core Challenge:**\n\n```\nImage features: 2048-dimensional vector (from ResNet)\nText features: 768-dimensional vector (from BERT)\n\nQuestion: How similar are they?\n\nProblem:\n  ✗ Different dimensions (can't directly compare)\n  ✗ Different scales (0-1 for text, -infinity to infinity for images)\n  ✗ Different semantics (what does dimension 500 mean in each?)\n  ✗ No natural similarity metric\n\nWe need: ALIGNMENT\nGoal: Make image and text features \"understand\" each other\n```\n\n**Real-world consequence:**\n\n```\nApplication: Image-text search\n  User searches: \"red cat\"\n  System has: 10 million images + descriptions\n\nWithout alignment:\n  Can't compare image and text vectors\n  Search impossible\n\nWith alignment:\n  Image vectors and text vectors in same space\n  Similarity computed easily\n  Search works!\n```\n\n### Levels of Alignment\n\n**Level 1: Coarse-grained (Document-level)**\n\n```\nEntire image ↔ Entire text description\n\nExample:\n  Image: [Full photo of cat on chair]\n  Text: \"A tabby cat relaxing on a wooden chair\"\n\n  Alignment: Image matches entire text\n\nUse cases:\n  - Image-text retrieval\n  - Image classification with descriptions\n  - Document understanding (image + caption)\n\nChallenge: Image might have multiple objects\n           Text mentions most important ones\n```\n\n**Level 2: Fine-grained (Region-level)**\n\n```\nImage regions ↔ Text phrases\n\nExample:\n  Image regions:\n    Region 1: [Cat's head area]\n    Region 2: [Chair seat area]\n    Region 3: [Background]\n\n  Text phrases:\n    \"tabby cat\" ↔ Region 1\n    \"wooden chair\" ↔ Region 2\n    \"cozy room\" ↔ Region 3\n\nUse cases:\n  - Visual question answering (where are things?)\n  - Dense image captioning\n  - Object detection with descriptions\n  - Grounding language in images\n\nChallenge: Multiple valid region boundaries\n           Phrases don't perfectly correspond to regions\n```\n\n**Level 3: Very Fine-grained (Pixel/Token-level)**\n\n```\nImage pixels ↔ Text tokens\n\nExample:\n  Video frame:\n    [Pixels 100-200]: Red fur\n    [Pixels 500-600]: Cat's eye\n    [Pixels 800-900]: Chair texture\n\n  Text tokens:\n    \"red\" ↔ Red fur pixels\n    \"cat\" ↔ Cat structure pixels\n    \"chair\" ↔ Chair pixels\n\nUse cases:\n  - Semantic segmentation with text\n  - Dense video captioning with timestamps\n  - Pixel-level understanding with descriptions\n\nChallenge: Extremely fine-grained\n           Requires pixel-level annotations\n           Computationally expensive\n```\n\n### Why Alignment is Hard\n\n**Reason 1: One-to-many mappings**\n\n```\nSingle image can have many valid descriptions:\n\nImage: [Cat on bed]\n\nValid descriptions:\n  ① \"A cat is sleeping on a bed\"\n  ② \"A cat on a bed\"\n  ③ \"Feline on furniture\"\n  ④ \"A cozy cat\"\n  ⑤ \"Kitty resting\"\n\nAll correct!\nNo single \"ground truth\" alignment\n\nChallenge: How to learn from multiple valid targets?\nSolution: Use soft targets or ranking-based losses\n```\n\n**Reason 2: Implicit pairing in training data**\n\n```\nWeb data structure:\n\n[Article with title: \"Beautiful pets\"]\n│\n├─ [Image 1]\n├─ [Image 2]\n├─ [Large paragraph mentioning pets]\n├─ [Image 3]\n└─ [Image 4]\n\nChallenge:\n  Which image goes with which sentence?\n  Are all images described equally?\n\nSolutions:\n  - Assume images near text match it\n  - Learn implicit pairings\n  - Use weak supervision signals\n```\n\n**Reason 3: Semantic gaps**\n\n```\nImage and text express different aspects:\n\nImage: \"Tabby cat, orange color, on blue chair, sunny room\"\nText: \"A cat resting\"\n\nText is abstract summary\nImage is concrete visual\n\nHow to align?\n  Need to map concrete visual features\n  to abstract semantic concepts\n\nThis requires:\n  ① Understanding visual features\n  ② Understanding text semantics\n  ③ Bridging the gap\n```\n\n**Reason 4: Missing or corrupted data**\n\n```\nData quality issues:\n\nSituation 1: Image and text don't match\n  Image: [Car]\n  Text: \"Beautiful sunset\"\n\n  Alignment should recognize mismatch\n\nSituation 2: Image is corrupted\n  Image: [Blank/noise]\n  Text: \"A dog running\"\n\n  Should still align based on text\n\nSituation 3: Text is poorly written\n  Image: [Cat photo]\n  Text: \"teh kat iz vry smrt\"\n\n  Should understand despite bad spelling\n```\n\n## 4.2 Shared Embedding Space - The Standard Solution\n\n### Core Concept\n\n**Idea:**\n```\nProject both modalities to common space\nwhere similarity can be computed\n\nImage (2048D) --┐\n               ├─→ Shared Space (256D)\nText (768D) ───┘\n\nNow both in same space!\nCan compute cosine similarity directly\n```\n\n### Implementation\n\n**Step 1: Learn projection matrices**\n\n```\nFor images:\n  W_img ∈ ℝ^(2048 × 256)\n  img_proj = W_img @ img_features\n\nFor text:\n  W_txt ∈ ℝ^(768 × 256)\n  txt_proj = W_txt @ txt_features\n\nBoth outputs: 256-dimensional vectors\n```\n\n**Step 2: Normalize in shared space**\n\n```\n# L2 normalize to unit length\nimg_proj = img_proj / ||img_proj||\ntxt_proj = txt_proj / ||txt_proj||\n\nResult:\n  Both vectors have magnitude 1\n  Can use cosine similarity = dot product\n  Similarity ∈ [-1, 1]\n```\n\n**Step 3: Compute similarity**\n\n```\nsimilarity = img_proj · txt_proj\n\n= Σ(img_proj_i × txt_proj_i)\n\nResult interpretation:\n  > 0.8:   Very similar (matched pair)\n  0.5-0.8: Similar\n  0.3-0.5: Somewhat related\n  < 0.3:   Different (unrelated pair)\n```\n\n### Learning the Projections\n\n**Training objective:**\n\n```\nGoal: Maximize similarity of matched pairs\n      Minimize similarity of unmatched pairs\n\nDataset: Pairs (image_i, text_i) where i means matched\n\nLoss function (InfoNCE / Contrastive):\n\nL = -log[ exp(sim(img_i, txt_i) / τ) /\n          (exp(sim(img_i, txt_i) / τ) + Σ_j≠i exp(sim(img_i, txt_j) / τ)) ]\n\nIntuition:\n  Numerator: Similarity of correct pair (should be high)\n  Denominator: All pairs including incorrect ones\n  Loss: Make correct pair stand out from all others\n```\n\n**Batching strategy:**\n\n```\nBatch of 32 samples:\n\n[img_1] ────────┐\n[img_2] ────────┼─ All project to shared space\n[img_3] ────────┤\n...             │\n[img_32] ───────┘\n\n[txt_1] ────────┐\n[txt_2] ────────┼─ All project to shared space\n[txt_3] ────────┤\n...             │\n[txt_32] ───────┘\n\nSimilarity matrix (32×32):\n  sim(img_1, txt_1) = 0.95  ← Matched\n  sim(img_1, txt_2) = 0.2   ← Unmatched\n  sim(img_2, txt_2) = 0.94  ← Matched\n  ...\n\nLoss: Make diagonal elements high\n      Make off-diagonal elements low\n```\n\n**Dimension selection:**\n\n```\nChoice of shared space dimension:\n\nSmall (64D):\n  ✓ Fast computation\n  ✓ Less memory\n  ✗ Information loss\n  ✗ Can't capture fine details\n\nMedium (256D):\n  ✓ Good balance\n  ✓ Standard choice\n  ✓ Preserves information\n\nLarge (1024D):\n  ✓ Maximum information\n  ✗ Slow computation\n  ✗ More memory\n  ✗ Risk of overfitting\n\nTypical sweet spot: 256-512D\n```\n\n### Practical Example\n\n**Image-Text Retrieval System:**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ImageTextAligner(nn.Module):\n    def __init__(self, img_dim=2048, txt_dim=768, shared_dim=256):\n        super().__init__()\n\n        # Projection layers\n        self.img_projection = nn.Linear(img_dim, shared_dim)\n        self.txt_projection = nn.Linear(txt_dim, shared_dim)\n\n    def forward(self, img_features, txt_features):\n        # Project to shared space\n        img_proj = self.img_projection(img_features)  # (batch, 256)\n        txt_proj = self.txt_projection(txt_features)  # (batch, 256)\n\n        # L2 normalize\n        img_proj = F.normalize(img_proj, p=2, dim=1)\n        txt_proj = F.normalize(txt_proj, p=2, dim=1)\n\n        return img_proj, txt_proj\n\n    def compute_similarity(self, img_proj, txt_proj):\n        # Cosine similarity = dot product of normalized vectors\n        similarity = torch.mm(img_proj, txt_proj.t())  # (batch, batch)\n        return similarity\n\n# Training\nmodel = ImageTextAligner()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nfor batch in data_loader:\n    images, texts = batch\n\n    img_features = image_encoder(images)  # (batch, 2048)\n    txt_features = text_encoder(texts)    # (batch, 768)\n\n    # Align\n    img_proj, txt_proj = model(img_features, txt_features)\n\n    # Compute similarities\n    similarities = model.compute_similarity(img_proj, txt_proj)\n\n    # Contrastive loss\n    batch_size = img_proj.shape[0]\n    labels = torch.arange(batch_size).to(device)\n\n    loss_img = F.cross_entropy(similarities / temperature, labels)\n    loss_txt = F.cross_entropy(similarities.t() / temperature, labels)\n    loss = (loss_img + loss_txt) / 2\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n## 4.3 Cross-Attention for Fine-Grained Alignment\n\n### Motivation\n\n**Problem with shared space:**\n\n```\nShared embedding gives global similarity\nBut doesn't tell us WHAT matched\n\nImage representation: Single 256D vector\n  Represents entire image\n  Loses spatial structure\n\nText representation: Single 256D vector\n  Represents entire sentence\n  Loses word-level information\n\nResult: Good for retrieval\n        Bad for understanding fine-grained relationships\n\nSolution: Cross-attention!\n```\n\n### Cross-Attention Mechanism\n\n**Core idea:**\n\n```\nLet each part of one modality\n\"look at\" relevant parts of another modality\n\nText looks at image:\n  Word \"red\" ← attends to → Red pixels in image\n  Word \"cat\" ← attends to → Cat-shaped pixels in image\n\nImage looks at text:\n  Cat region ← attends to → \"cat\" and \"tabby\" words\n  Background ← attends to → \"room\" word\n```\n\n**Mathematical formulation:**\n\n```\nCross-attention (Text queries, Image keys/values):\n\nQuery = Text embeddings (sequence length = # words)\nKey = Image patches from CNN (# patches = 196 for ViT)\nValue = Image patches\n\nAttention = softmax(Query @ Key^T / √d_k) @ Value\n\nResult:\n  Each word gets weighted combination of image patches\n  Weights reflect relevance (attention)\n```\n\n**Concrete example:**\n\n```\nText: \"The [red] cat [sits]\"\n       Word 1  Word 2  Word 3\n\nImage: Divided into 9 patches (3×3 grid)\n\n┌─────┬─────┬─────┐\n│ Sky │ Sky │ Sky │\n├─────┼─────┼─────┤\n│ Cat │ Cat │Chair│\n├─────┼─────┼─────┤\n│Grass│ Cat │Chair│\n└─────┴─────┴─────┘\n\nCross-attention: Word \"red\" attends to image patches\n\nAttention scores:\n  Sky: 0.1\n  Sky: 0.1\n  Sky: 0.1\n  Cat(red): 0.5  ← High! Red cat\n  Cat: 0.3\n  Chair: 0.0\n  Grass: 0.0\n  Cat: 0.3\n  Chair: 0.0\n\nAttention output: Weighted combination of patches\n  0.5 × Cat_patch_red + 0.3 × Cat_patch + 0.3 × Cat_patch\n  ≈ Feature vector emphasizing red cat region\n```\n\n### Multi-head Cross-Attention\n\n**Why multiple heads?**\n\n```\nDifferent heads can attend to different aspects\n\nHead 1: Color-sensitive\n  Attends to: Patches with matching colors\n\nHead 2: Shape-sensitive\n  Attends to: Patches with matching shapes\n\nHead 3: Texture-sensitive\n  Attends to: Patches with matching textures\n\nAll heads run in parallel\nResults concatenated\n```\n\n**Implementation:**\n\n```\n# Pseudo-code for single attention head\n\nQuery = W_q @ text_embedding      # (seq_len, d_k)\nKey = W_k @ image_patches         # (num_patches, d_k)\nValue = W_v @ image_patches       # (num_patches, d_v)\n\n# Compute attention weights\nscores = Query @ Key^T            # (seq_len, num_patches)\nweights = softmax(scores / √d_k)  # (seq_len, num_patches)\n\n# Apply to values\noutput = weights @ Value          # (seq_len, d_v)\n\n# For multiple heads: repeat with different W_q, W_k, W_v\n# Then concatenate outputs\n```\n\n### Bidirectional Alignment\n\n**Problem:** Cross-attention only goes one way\n\n```\nText attends to image: ✓ Good\n  Words understand image\n\nImage attends to text: ✗ Missing\n  Image regions don't understand text\n  Asymmetric!\n```\n\n**Solution: Bidirectional attention**\n\n```\nStep 1: Text cross-attends to image\n  Query = Text\n  Key/Value = Image\n  Result: Text-aware-of-image\n\nStep 2: Image cross-attends to text\n  Query = Image patches\n  Key/Value = Text\n  Result: Image-aware-of-text\n\nStep 3: Combine both representations\n  Use refined representations for tasks\n```\n\n**Architecture with bidirectional alignment:**\n\n```\nInitial representations:\n  Image patches: 196 vectors (from ViT)\n  Text tokens: 10 vectors (from BERT)\n\nLayer 1:\n  ├─ Text cross-attends to Image\n  │  └─ Output: Refined text (10 vectors)\n  │\n  └─ Image self-attends within itself\n     └─ Output: Refined image (196 vectors)\n\nLayer 2:\n  ├─ Image cross-attends to Text\n  │  └─ Output: Refined image (196 vectors)\n  │\n  └─ Text self-attends within itself\n     └─ Output: Refined text (10 vectors)\n\nLayer 3-6: Repeat above\n\nResult:\n  Both modalities refined with knowledge of other\n  Bidirectional influence\n```\n\n**Example - VQA (Visual Question Answering):**\n\n```\nImage: [Photo of cat on chair]\nQuestion: \"What's on the chair?\"\n\nProcessing with bidirectional alignment:\n\n① Initial encoding:\n   Image patches: 196 ViT features\n   Question: \"What's\", \"on\", \"the\", \"chair\", \"?\" (5 tokens)\n\n② Text understands image context:\n   \"chair\" attends to chair-region patches\n   \"on\" understands preposition in spatial context\n\n   Result: Question tokens now image-aware\n\n③ Image understands question:\n   Chair region attends to \"chair\" token\n   Surrounding region attends to \"on\" (preposition)\n\n   Result: Image patches now question-aware\n\n④ Predict answer:\n   Question tokens generate: \"A cat\"\n\nBenefits:\n  - Question focuses on relevant image parts\n  - Image highlights relevant content for question\n  - Mutual refinement through layers\n  - Better understanding than independent processing\n```\n\n## 4.4 Practical Alignment Challenges and Solutions\n\n### Challenge 1: Handling Multiple Valid Alignments\n\n**Problem:**\n\n```\nImage: [Multi-object scene: cat, dog, table]\nText options:\n  ① \"Pets on table\"\n  ② \"Table with animals\"\n  ③ \"Room with furniture\"\n  ④ \"A table with a cat and dog\"\n\nAll valid descriptions!\nWhich should model learn?\n```\n\n**Solutions:**\n\n**Solution 1: All positives training**\n\n```\nTreat all valid descriptions as positive examples\n\nLoss = -log[exp(sim(img, txt1)) + exp(sim(img, txt2)) + exp(sim(img, txt3))]\n       / [exp(sim(img, txt1)) + exp(sim(img, txt2)) + exp(sim(img, txt3)) +\n          exp(sim(img, neg1)) + exp(sim(img, neg2)) + ...]\n\nCode:\n  positives = [text1, text2, text3]  # All valid\n  negatives = [text4, text5, ...]    # Invalid\n\n  for pos in positives:\n    loss += InfoNCE_loss(img, pos, negatives)\n```\n\n**Solution 2: Soft targets**\n\n```\nAssign soft probability to each description\n\nSimilarity scores: [0.9, 0.85, 0.7, 0.3, 0.1]\nProbabilities: [0.4, 0.4, 0.15, 0.04, 0.01]\n\nDistribution rather than hard binary labels\nModel learns to match range of good descriptions\n```\n\n**Solution 3: Ranking-based loss**\n\n```\nInstead of absolute similarity,\noptimize relative ranking\n\nConstraint: sim(img, good_text) > sim(img, bad_text) + margin\n\nLoss = max(0, margin + sim(img, bad) - sim(img, good))\n\nModel ensures good descriptions rank higher\nNot concerned with absolute values\n```\n\n### Challenge 2: Incomplete or Corrupted Data\n\n**Problem 1: Image-text mismatch**\n\n```\nWeb data contains misaligned pairs:\n\nWebsite page:\n[Image1: Beautiful sunset]\n[Article about technology and computers]\n[Image2: Laptop]\n\nProblem:\n  Image1 doesn't match article\n  Article matches Image2 only\n\nSolution: Robustness to noise\n  - Training with some wrong pairs is okay\n  - Model learns that MOST pairs are correct\n  - Wrong pairs become negatives\n  - Loss still works\n\n  Empirically: Works even with 20-30% misaligned data\n```\n\n**Problem 2: Low-quality images or text**\n\n```\nImages:\n  - Blurry photos\n  - Extreme lighting\n  - Occlusions\n  - Irrelevant backgrounds\n\nText:\n  - Spelling errors\n  - Grammar mistakes\n  - Abbreviations\n  - Emotional/subjective language\n\nSolution: Robust feature extraction\n  - Use pre-trained encoders (already robust)\n  - Encoders trained on diverse data\n  - Can handle degraded inputs\n  - Alignment robust if base features good\n```\n\n**Problem 3: Context-dependent meaning**\n\n```\nText: \"A record player\"\nImage: [Phonograph]\n\nChallenge:\n  \"Record\" = LP vinyl record vs historical record\n  \"Player\" = music player vs sports player\n  Multiple interpretations!\n\nSolution: Context through attention\n  - Image patches clarify which \"record\"\n  - Text confirms image interpretation\n  - Cross-attention resolves ambiguity\n```\n\n### Challenge 3: Scaling to Large Datasets\n\n**Problem:**\n\n```\nComputing similarity matrix for large batch:\n\nBatch size: 10,000 images and 10,000 texts\nSimilarity matrix size: 10,000 × 10,000 = 100M elements\n\nComputation:\n  ① Forward pass: 100M multiplies\n  ② Softmax: 100M exponentials\n  ③ Backward pass: 100M gradients\n\nResult: Extremely slow!\nGPU memory: 100M × float32 = 400MB just for similarities\n```\n\n**Solutions:**\n\n**Solution 1: Smaller batches**\n\n```\nBatch size: 256 instead of 10,000\nSimilarity matrix: 256 × 256 = 65K elements\n\nTrade-off:\n  ✓ Faster training\n  ✓ Less memory\n  ✗ Noisier gradients (fewer negatives)\n  ✗ More iterations needed\n```\n\n**Solution 2: Distributed training**\n\n```\nSplit batch across multiple GPUs\n\nGPU 1: 2500 images and texts\nGPU 2: 2500 images and texts\nGPU 3: 2500 images and texts\nGPU 4: 2500 images and texts\n\nGradient computation happens locally\nAll-reduce aggregates gradients\n\nEnables:\n  - Larger effective batch size\n  - Better negatives for learning\n  - Faster training overall\n```\n\n**Solution 3: Hard negative mining**\n\n```\nInstead of all negatives,\nselect hard negatives (easily confused)\n\nFull set: 10,000 possible negatives\nSample: 32 hard negatives (ones model struggles with)\n\nBenefits:\n  - Reduces computation\n  - More efficient learning (focus on hard cases)\n  - Still effective despite smaller negative set\n```\n\n## 4.5 Evaluating Alignment Quality\n\n### Metrics for Alignment\n\n**1. Retrieval Metrics**\n\n```\nSetup: Given 1000 images and 1000 texts (properly paired)\nTask: For each image, rank texts by similarity\n\nMetrics:\n\nRecall@K:\n  Did correct text appear in top K?\n\n  Example (K=1):\n    For each image, check if correct text in top 1\n    Count successes / total images\n\n  Recall@1: 75% (750/1000 correct)\n  Recall@5: 95% (950/1000 correct)\n\n  Interpretation:\n    Recall@1 = Exact match retrieval rate\n    Recall@5 = Reasonable match rate\n\nMean Reciprocal Rank (MRR):\n  Average rank of correct match\n\n  Example:\n    Image 1: Correct text at rank 3 → 1/3\n    Image 2: Correct text at rank 1 → 1/1\n    Image 3: Correct text at rank 10 → 1/10\n    MRR = (1/3 + 1/1 + 1/10) / 3 ≈ 0.44\n\nNormalized DCGA (NDCG):\n  Accounts for relevance scores\n  Perfect ranking = 1.0\n```\n\n**2. Correlation Metrics**\n\n```\nIdea: Good alignment means\n      similar images/texts have high correlation\n\nSpearman Correlation:\n  ① Rank pairs by human similarity judgment\n  ② Rank same pairs by model similarity\n  ③ Compute rank correlation\n\n  Perfect: Correlation = 1.0\n  Random: Correlation ≈ 0.0\n\nPearson Correlation:\n  Linear correlation between human and model scores\n```\n\n**3. Classification Metrics**\n\n```\nBinary classification: Correct or incorrect pairing?\n\nDataset: 1000 correct pairs + 1000 incorrect pairs\n\nMetrics:\n  Accuracy: How many correct predictions?\n  Precision: Of positive predictions, how many correct?\n  Recall: Of correct pairs, how many identified?\n  F1: Harmonic mean of precision and recall\n\nExample results:\n  Accuracy: 95% (1900/2000 correct)\n  Precision: 96% (970/1010 predicted positive)\n  Recall: 97% (970/1000 actually positive)\n  F1: 0.965\n```\n\n### Example Evaluation Script\n\n```python\nfrom sklearn.metrics import recall_score, ndcg_score\nimport numpy as np\n\ndef evaluate_alignment(img_features, txt_features, labels):\n    \"\"\"\n    img_features: (N, 256) aligned image embeddings\n    txt_features: (N, 256) aligned text embeddings\n    labels: (N,) ground truth labels (0=incorrect, 1=correct)\n    \"\"\"\n\n    # Compute similarities\n    similarities = np.dot(img_features, txt_features.T)\n\n    # Recall@K\n    recall_at_1 = compute_recall_at_k(similarities, labels, k=1)\n    recall_at_5 = compute_recall_at_k(similarities, labels, k=5)\n\n    # Classification metrics\n    binary_predictions = (similarities > threshold).astype(int)\n    accuracy = np.mean(binary_predictions == labels)\n    precision = precision_score(labels, binary_predictions)\n    recall = recall_score(labels, binary_predictions)\n    f1 = f1_score(labels, binary_predictions)\n\n    print(f\"Recall@1: {recall_at_1:.3f}\")\n    print(f\"Recall@5: {recall_at_5:.3f}\")\n    print(f\"Accuracy: {accuracy:.3f}\")\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall: {recall:.3f}\")\n    print(f\"F1: {f1:.3f}\")\n\n    return {\n        'recall_at_1': recall_at_1,\n        'recall_at_5': recall_at_5,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\ndef compute_recall_at_k(similarities, labels, k):\n    \"\"\"Compute Recall@K metric\"\"\"\n    n = similarities.shape[0]\n    recall_sum = 0\n\n    for i in range(n):\n        # Get top-K most similar texts for this image\n        top_k_idx = np.argsort(similarities[i])[-k:]\n\n        # Check if any of top-K are correct (label=1)\n        if np.any(labels[top_k_idx] == 1):\n            recall_sum += 1\n\n    return recall_sum / n\n```\n\n## Key Takeaways\n\n- **Alignment is essential** for connecting different modalities\n- **Shared embedding space** is standard, scalable solution\n- **Cross-attention** enables fine-grained alignment\n- **Bidirectional fusion** gives mutual understanding\n- **Practical challenges** require careful handling\n- **Multiple evaluation metrics** give comprehensive picture\n\n## Exercises\n\n**⭐ Beginner:**\n1. Implement cosine similarity between image and text features\n2. Visualize shared embedding space using t-SNE\n3. Compute recall@K for sample retrieval task\n\n**⭐⭐ Intermediate:**\n4. Build shared embedding projection layers\n5. Implement contrastive loss training\n6. Evaluate alignment with multiple metrics\n\n**⭐⭐⭐ Advanced:**\n7. Implement bidirectional cross-attention from scratch\n8. Build hard negative mining strategy\n9. Compare different loss functions (InfoNCE, triplet, etc.)\n\n---\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"chapter-04.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}